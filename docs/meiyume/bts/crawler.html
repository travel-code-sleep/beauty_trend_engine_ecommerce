<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>meiyume.bts.crawler API documentation</title>
<meta name="description" content="The module to crawl Boots website data." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>meiyume.bts.crawler</code></h1>
</header>
<section id="section-intro">
<p>The module to crawl Boots website data.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;The module to crawl Boots website data.&#34;&#34;&#34;
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)

import concurrent.futures
import os
import pdb
import shutil
import sys
import time
import types
import warnings
from datetime import datetime, timedelta
from pathlib import Path
from typing import *

import numpy as np
import pandas as pd
import tldextract
from meiyume.cleaner_plus import Cleaner
from meiyume.utils import (Boots, Browser, Logger, MeiyumeException,
                           accept_alert, chunks, close_popups,
                           convert_ago_to_date, log_exception, ranges)
from selenium import webdriver
from selenium.common.exceptions import (ElementClickInterceptedException,
                                        NoSuchElementException,
                                        StaleElementReferenceException,
                                        TimeoutException)
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.alert import Alert
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.remote.webelement import WebElement
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait

warnings.simplefilter(action=&#39;ignore&#39;, category=FutureWarning)


class Metadata(Boots):
    &#34;&#34;&#34;Metadata extracts product metadata such as product page url, prices and brand from Boots website.

    The Metadata class begins the data crawling process and all other stages depend on
    the product urls extracted by Metadata class.

    Args:
        Boots (Browser): Class that initializes folder paths and selenium webdriver for data scraping.

    &#34;&#34;&#34;
    base_url = &#34;https://www.boots.com&#34;
    info = tldextract.extract(base_url)
    source = info.registered_domain

    @classmethod
    def update_base_url(cls, url: str) -&gt; None:
        &#34;&#34;&#34;update_base_url defines the parent url from where the data scraping process will begin.

        Args:
            url (str): The URL from which the spider will enter the website.

        &#34;&#34;&#34;
        cls.base_url = url
        cls.info = tldextract.extract(cls.base_url)
        cls.source = cls.info.registered_domain

    def __init__(self, log: bool = True, path: Path = Path.cwd()):
        &#34;&#34;&#34;__init__ Metadata class instace initializer.

        This method sets all the folder paths required for Metadata crawler to work.
        If the paths does not exist the paths get automatically created depending on current directory or
        provided directory.

        Args:
            log (bool, optional): Whether to create crawling exception and progess log. Defaults to True.
            path (Path, optional): Folder path where the Metadata will be extracted. Defaults to
                                   current directory(Path.cwd()).

        &#34;&#34;&#34;
        super().__init__(path=path, data_def=&#39;meta&#39;)
        self.path = path
        self.current_progress_path = self.metadata_path/&#39;current_progress&#39;
        self.current_progress_path.mkdir(parents=True, exist_ok=True)

        # move old raw and clean files to old folder
        old_metadata_files = list(self.metadata_path.glob(
            &#39;bts_product_metadata_all*&#39;))
        for f in old_metadata_files:
            shutil.move(str(f), str(self.old_metadata_files_path))

        old_clean_metadata_files = os.listdir(self.metadata_clean_path)
        for f in old_clean_metadata_files:
            shutil.move(str(self.metadata_clean_path/f),
                        str(self.old_metadata_clean_files_path))
        # set logger
        if log:
            self.prod_meta_log = Logger(
                &#34;bts_prod_metadata_extraction&#34;, path=self.crawl_log_path)
            self.logger, _ = self.prod_meta_log.start_log()

    def get_product_type_urls(self, open_headless: bool, open_with_proxy_server: bool) -&gt; pd.DataFrame:
        &#34;&#34;&#34;get_product_type_urls Extract the category/subcategory structure and urls to extract the products of those category/subcategory.

        Extracts the links of pages containing the list of all products structured into
        category/subcategory/product type to effectively stored in relational database.
        Defines the structure of data extraction that helps store unstructured data in a structured manner.

        Args:
            open_headless (bool): Whether to open browser headless.
            open_with_proxy_server (bool): Whether to use proxy server.

        Returns:
            pd.DataFrame: returns pandas dataframe containing urls for getting list of products, category, subcategory etc.
        &#34;&#34;&#34;
        # create webdriver instance
        drv = self.open_browser(
            open_headless=open_headless, open_with_proxy_server=open_with_proxy_server, path=self.metadata_path)

        drv.get(self.base_url)
        time.sleep(15)
        # click and close welcome forms
        accept_alert(drv, 10)
        close_popups(drv)

        try:
            country_popup = WebDriverWait(drv, 3).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, &#39;.estores_overlay_content &gt; a:nth-child(1)&#39;)))
            pop_close_button = drv.find_element_by_css_selector(
                &#39;.estores_overlay_content &gt; a:nth-child(1)&#39;)
            Browser().scroll_to_element(drv, pop_close_button)
            ActionChains(drv).move_to_element(
                pop_close_button).click(pop_close_button).perform()
        except Exception as ex:
            pass

        allowed_categories = [&#39;beauty&#39;, &#39;fragrance&#39;,
                              &#39;toiletries&#39;, &#39;mens&#39;, &#39;wellness&#39;]
        exclude_categories = [&#39;health-pharmacy&#39;, &#39;advice&#39;, &#39;luxury-beauty-premium-beauty-book-an-appointment&#39;,
                              &#39;health-value-packs-and-bundles&#39;, &#39;all-luxury-skincare&#39;, &#39;wellness-supplements&#39;, &#39;travel-essentials&#39;,
                              &#39;opticians&#39;, &#39;feminine-hygiene&#39;, &#39;travel-health&#39;, &#39;sunglasses&#39;, &#39;inspiration&#39;, &#39;food-and-drink&#39;,
                              &#39;nutrition&#39;, &#39;vitaminsandsupplements&#39;, &#39;condoms-sexual-health&#39;, &#39;mens-health-information&#39;,
                              &#39;weightloss&#39;, &#39;menshealth&#39;, &#39;recommended&#39;, &#39;offers&#39;, &#39;all-&#39;, &#39;male-incontinence&#39;, &#39;sustainable-living&#39;,
                              &#39;bootsdental&#39;, &#39;back-to-school-and-nursery&#39;, &#39;luxury-beauty-skincare&#39;, &#39;mens-gift-sets&#39;,
                              &#39;all-face&#39;, &#39;all-eyes&#39;, &#39;all-lips&#39;, &#39;gift/him/mens-aftershave&#39;, &#39;gift/her/luxury-beauty-gift&#39;,
                              &#39;christmas/christmas-3-for-2&#39;, &#39;christmas/gifts-for-her&#39;, &#39;christmas/gifts-for-him&#39;,
                              &#39;christmas/advent-calendars&#39;, &#39;beauty-expert-skincare-expert-skincare-shop-all&#39;,
                              &#39;black-afro-and-textured-hair-straight-hair&#39;, &#39;black-afro-and-textured-hair-wavy&#39;,
                              &#39;black-afro-and-textured-hair-straight-hair&#39;,
                              ]
        # Extracting the category-sub category structure
        cat_urls = []
        for i in drv.find_elements_by_css_selector(&#39;a[id*=&#34;subcategoryLink&#34;]&#39;):
            cat_urls.append(i.get_attribute(&#39;href&#39;))

        cat_urls = [u for u in cat_urls if any(
            cat in str(u) for cat in allowed_categories)]
        cat_urls = [u for u in cat_urls if all(
            cat not in str(u) for cat in exclude_categories)]

        prod_type_urls = []

        for url in cat_urls:
            drv.get(url)

            time.sleep(6)
            accept_alert(drv, 10)
            close_popups(drv)

            subcats = drv.find_elements_by_css_selector(&#39;div.category-link&gt;a&#39;)
            if len(subcats) &gt; 0:
                for i in drv.find_elements_by_css_selector(&#39;div.category-link&gt;a&#39;):
                    prod_type_urls.append(i.get_attribute(&#39;href&#39;))
            else:
                prod_type_urls.append(url)

        prod_type_urls = [u for u in prod_type_urls if any(
            cat in str(u) for cat in allowed_categories)]
        prod_type_urls = [u for u in prod_type_urls if all(
            cat not in str(u) for cat in exclude_categories)]

        drv.quit()

        df = pd.DataFrame(prod_type_urls, columns=[&#39;url&#39;])
        df[&#39;dept&#39;], df[&#39;category_raw&#39;], df[&#39;subcategory_raw&#39;], df[&#39;product_type&#39;] = zip(
            *df.url.str.split(&#39;/&#39;, expand=True).loc[:, 3:].values)

        def set_cat_subcat_ptype(x):
            if x.product_type is None:
                return x.dept, x.category_raw, x.subcategory_raw
            else:
                return x.category_raw, x.subcategory_raw, x.product_type

        df.category_raw, df.subcategory_raw, df.product_type = zip(
            *df.apply(set_cat_subcat_ptype, axis=1))
        df.drop(columns=[&#39;dept&#39;], inplace=True)
        df.drop_duplicates(subset=&#39;url&#39;, inplace=True)
        df.reset_index(inplace=True, drop=True)
        df[&#39;scraped&#39;] = &#39;N&#39;
        df.to_feather(self.metadata_path/f&#39;bts_product_type_urls_to_extract&#39;)
        return df

    def get_metadata(self, indices: Union[list, range],
                     open_headless: bool, open_with_proxy_server: bool,
                     randomize_proxy_usage: bool,
                     product_meta_data: list = []):
        &#34;&#34;&#34;get_metadata Crawls product listing pages for price, name, brand etc.

        Get Metadata crawls a product type page for example lipstick.
        The function gets individual product urls, names, brands and prices etc. and stores
        in a relational table structure to use later to download product images, scrape reviews and
        other specific information.

        Args:
            indices (Union[list, range]): list of indices or range of indices of product urls to scrape.
            open_headless (bool): Whether to open browser headless.
            open_with_proxy_server (bool): Whether to use proxy server.
            randomize_proxy_usage (bool): Whether to use both proxy and native network in tandem to decrease proxy requests.
            product_meta_data (list, optional): Empty intermediate list to store product metadata during parallel crawl. Defaults to [].
        &#34;&#34;&#34;
        for pt in self.product_type_urls.index[self.product_type_urls.index.isin(indices)]:
            cat_name = self.product_type_urls.loc[pt, &#39;category_raw&#39;]
            product_type = self.product_type_urls.loc[pt, &#39;product_type&#39;]
            product_type_link = self.product_type_urls.loc[pt, &#39;url&#39;]

            self.progress_tracker.loc[pt, &#39;product_type&#39;] = product_type
            # print(self.progress_tracker.loc[pt, &#39;product_type&#39;])
            # print(product_type_link)
            if &#39;best-selling&#39; in product_type.lower() or &#39;new&#39; in product_type.lower():
                self.progress_tracker.loc[pt, &#39;scraped&#39;] = &#39;NA&#39;
                # print(self.progress_tracker.loc[pt, &#39;scraped&#39;])
                continue

            if randomize_proxy_usage:
                use_proxy = np.random.choice([True, False])
            else:
                use_proxy = True
            if open_with_proxy_server:
                # print(use_proxy)
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
                                        path=self.metadata_path)
            else:
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
                                        path=self.metadata_path)

            drv.get(product_type_link)
            time.sleep(15)  # 30
            accept_alert(drv, 10)
            close_popups(drv)

            # load all the products
            self.scroll_down_page(drv, h2=0.8, speed=5)
            time.sleep(5)

            try:
                pages = int(drv.find_element_by_css_selector(
                    &#39;div[class*=&#34;pageControl number&#34;]&#39;).get_attribute(&#34;data-pages&#34;))
            except NoSuchElementException as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod Type: {product_type}&#39;)
                self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType {product_type} has\
                only one page of products.(page link: {product_type_link})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                pages = 1
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod Type: {product_type}&#39;)
                self.progress_tracker.loc[pt, &#39;scraped&#39;] = &#39;NA&#39;
                self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType {product_type}\
                     page not found.(page link: {product_type_link})&#39;,
                                            &#39;utf-8&#39;, &#39;ignore&#39;))

            for page in range(pages):
                self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type}\
                                  getting product from page {page}.(page link: {product_type_link})&#39;,
                                            &#39;utf-8&#39;, &#39;ignore&#39;))

                products = drv.find_elements_by_css_selector(
                    &#39;div[class*=&#34;estore_product_container&#34;]&#39;)

                for product in products:
                    # prod_id = &#34;bts_&#34; + \
                    #     product.get_attribute(&#39;data-productid&#39;).split(&#34;.&#34;)[0]
                    self.scroll_to_element(drv, product)
                    try:
                        product_name = product.find_element_by_css_selector(
                            &#39;div.product_name&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                     product {products.index(product)} metadata extraction failed.\
                                                (page_link: {product_type_link} - page_no: {page})&#39;,
                                                    &#39;utf-8&#39;, &#39;ignore&#39;))
                        product_name = &#39;&#39;

                    try:
                        price = product.find_element_by_css_selector(
                            &#39;div.product_price&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        price = &#39;&#39;
                        self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                      product {products.index(product)} price extraction failed.\
                                                (page_link: {product_type_link} - page_no: {page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

                    try:
                        discount = product.find_element_by_css_selector(
                            &#39;div.product_savePrice&gt;span&#39;).text.split()[-1]
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        discount = &#39;&#39;
                        self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                      product {products.index(product)} no discount/savings.\
                                                (page_link: {product_type_link} - page_no: {page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

                    try:
                        product_page = product.find_element_by_css_selector(
                            &#39;div.product_name&gt;a&#39;).get_attribute(&#39;href&#39;)
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        product_page = &#39;&#39;
                        self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                     product {products.index(product)} product_page extraction failed.\
                                                (page_link: {product_type_link} - page_no: {page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

                    try:
                        rating = product.find_element_by_css_selector(
                            &#39;div.product_rating&gt;span&#39;).get_attribute(&#39;aria-label&#39;).split(&#34; &#34;)[0]
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        rating = &#39;&#39;
                        self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                     product {products.index(product)} rating extraction failed.\
                                                (page_link: {product_type_link} - page_no: {page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

                    if datetime.now().day &lt; 15:
                        meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
                    else:
                        meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;

                    product_data_dict = {&#34;product_name&#34;: product_name, &#34;product_page&#34;: product_page, &#34;brand&#34;: &#39;&#39;,
                                         &#34;price&#34;: price, &#34;discount&#34;: discount, &#34;rating&#34;: rating, &#34;category&#34;: cat_name,
                                         &#34;product_type&#34;: product_type, &#34;new_flag&#34;: &#39;&#39;, &#34;meta_date&#34;: meta_date}
                    product_meta_data.append(product_data_dict)

                if pages != 1:
                    next_page_button = drv.find_element_by_css_selector(
                        &#39;a[title*=&#34;Show next&#34;]&#39;)
                    self.scroll_to_element(drv, next_page_button)
                    ActionChains(drv).move_to_element(
                        next_page_button).click(next_page_button).perform()
                    time.sleep(5)
                    accept_alert(drv, 10)
                    close_popups(drv)
                    self.scroll_down_page(drv, h2=0.8, speed=5)
                    time.sleep(5)

            drv.quit()

            if len(product_meta_data) &gt; 0:
                product_meta_df = pd.DataFrame(product_meta_data)
                product_meta_df.to_feather(
                    self.current_progress_path/f&#39;bts_prod_meta_extract_progress_{product_type}_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}&#39;)
                self.logger.info(
                    f&#39;Completed till IndexPosition: {pt} - ProductType: {product_type}. (URL:{product_type_link})&#39;)
                self.progress_tracker.loc[pt, &#39;scraped&#39;] = &#39;Y&#39;
                # print(self.progress_tracker.loc[pt, &#39;scraped&#39;])
                self.progress_tracker.to_feather(
                    self.metadata_path/&#39;bts_metadata_progress_tracker&#39;)
                # print(self.progress_tracker)
                product_meta_data = []
        self.logger.info(&#39;Metadata Extraction Complete&#39;)
        print(&#39;Metadata Extraction Complete&#39;)
        # self.progress_monitor.info(&#39;Metadata Extraction Complete&#39;)

    def extract(self, download: bool = True, fresh_start: bool = False, auto_fresh_start: bool = False, n_workers: int = 5,
                open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = True,
                start_idx: Optional[int] = None, end_idx: Optional[int] = None, list_of_index=None,
                clean: bool = True, compile_progress_files: bool = False, delete_progress: bool = False) -&gt; None:
        &#34;&#34;&#34;extract method controls all properties of the spiders and runs multi-threaded web crawling.

        Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
        This method has four major functionality:
        * 1. Run the spider
        * 2. Store data in regular intervals to free up ram
        * 3. Compile all crawled data into one file.
        * 4. Clean and push cleaned data to S3 storage for further algorithmic processing.

        Args:
            download (bool, optional): Whether to crawl data from or compile crawled data into one file. Defaults to True.
            fresh_start (bool, optional): Whether to continue last crawl job or start new one. Defaults to False.
            auto_fresh_start (bool, optional): Whether to automatically start a new crawl job if last job was finished.
                                               Defaults to False.
            n_workers (int, optional): No. of parallel threads to run. Defaults to 5.
            open_headless (bool, optional): Whether to open browser headless. Defaults to False.
            open_with_proxy_server (bool, optional): Whether to use ip rotation service. Defaults to True.
            randomize_proxy_usage (bool, optional): Whether to use both proxy and native network in tandem to decrease proxy requests.
                                                    Defaults to True.
            start_idx (Optional[int], optional): Starting index of the links to crawl. Defaults to None.
            end_idx (Optional[int], optional): Ending index of the links to crawl. Defaults to None.
            list_of_index ([type], optional): List of indices or range of indices of product urls to scrape. Defaults to None.
            compile_progress_files (bool, optional): Whether to combine crawled data into one file. Defaults to False.
            clean (bool, optional): Whether to clean the compiled data. Defaults to True.
            delete_progress (bool, optional): Whether to delete intermediate data after compilation into one file. Defaults to False.
        &#34;&#34;&#34;
        def fresh():
            &#34;&#34;&#34;[summary]
            &#34;&#34;&#34;
            self.product_type_urls = self.get_product_type_urls(open_headless=open_headless,
                                                                open_with_proxy_server=open_with_proxy_server)
            # progress tracker: captures scraped and error desc
            self.progress_tracker = pd.DataFrame(index=self.product_type_urls.index, columns=[
                &#39;product_type&#39;, &#39;scraped&#39;, &#39;error_desc&#39;])
            self.progress_tracker.scraped = &#39;N&#39;

        if fresh_start:
            self.logger.info(&#39;Starting Fresh Extraction.&#39;)
            fresh()
        else:
            if Path(self.metadata_path/&#39;bts_product_type_urls_to_extract&#39;).exists():
                self.product_type_urls = pd.read_feather(
                    self.metadata_path/&#39;bts_product_type_urls_to_extract&#39;)
                if Path(self.metadata_path/&#39;bts_metadata_progress_tracker&#39;).exists():
                    self.progress_tracker = pd.read_feather(
                        self.metadata_path/&#39;bts_metadata_progress_tracker&#39;)
                else:
                    self.progress_tracker = pd.DataFrame(index=self.product_type_urls.index, columns=[
                        &#39;product_type&#39;, &#39;scraped&#39;, &#39;error_desc&#39;])
                    self.progress_tracker.scraped = &#39;N&#39;
                    self.progress_tracker.to_feather(
                        self.metadata_path/&#39;bts_metadata_progress_tracker&#39;)
                if sum(self.progress_tracker.scraped == &#39;N&#39;) &gt; 0:
                    self.logger.info(
                        &#39;Continuing Metadata Extraction From Last Run.&#39;)
                    self.product_type_urls = self.product_type_urls[self.product_type_urls.index.isin(
                        self.progress_tracker.index[self.progress_tracker.scraped == &#39;N&#39;].values.tolist())]
                else:
                    if auto_fresh_start:
                        self.logger.info(
                            &#39;Previous Run Was Complete. Starting Fresh Extraction.&#39;)
                        fresh()
                    else:
                        self.logger.info(
                            &#39;Previous Run is Complete.&#39;)
            else:
                self.logger.info(
                    &#39;URL File Not Found. Start Fresh Extraction.&#39;)
        # print(self.progress_tracker)
        if download:
            # set list or range of product indices to crawl
            if list_of_index:
                indices = list_of_index
            elif start_idx and end_idx is None:
                indices = range(start_idx, len(self.product_type_urls))
            elif start_idx is None and end_idx:
                indices = range(0, end_idx)
            elif start_idx is not None and end_idx is not None:
                indices = range(start_idx, end_idx)
            else:
                indices = range(len(self.product_type_urls))
            # print(indices)
            if list_of_index:
                self.get_metadata(indices=list_of_index,
                                  open_headless=open_headless,
                                  open_with_proxy_server=open_with_proxy_server,
                                  randomize_proxy_usage=randomize_proxy_usage,
                                  product_meta_data=[])
            else:
                &#39;&#39;&#39;
                # review_Data and item_data are lists of empty lists so that each namepace of function call will
                # have its separate detail_data
                # list to strore scraped dictionaries. will save memory(ram/hard-disk) consumption. will stop data duplication
                &#39;&#39;&#39;
                if start_idx:
                    lst_of_lst = ranges(
                        indices[-1]+1, n_workers, start_idx=start_idx)
                else:
                    lst_of_lst = ranges(len(indices), n_workers)
                print(lst_of_lst)
                headless = [open_headless for i in lst_of_lst]
                proxy = [open_with_proxy_server for i in lst_of_lst]
                rand_proxy = [randomize_proxy_usage for i in lst_of_lst]
                product_meta_data = [[] for i in lst_of_lst]
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    &#39;&#39;&#39;
                    # but each of the function namespace will be modifying only one metadata tracing file so that progress saving
                    # is tracked correctly. else multiple progress tracker file will be created with difficulty to combine correct
                    # progress information
                    &#39;&#39;&#39;
                    executor.map(self.get_metadata, lst_of_lst,
                                 headless, proxy, rand_proxy, product_meta_data)

        if compile_progress_files:
            self.logger.info(&#39;Creating Combined Metadata File&#39;)
            files = [f for f in self.current_progress_path.glob(
                &#34;bts_prod_meta_extract_progress_*&#34;)]
            li = [pd.read_feather(file) for file in files]
            metadata_df = pd.concat(li, axis=0, ignore_index=True)
            metadata_df.reset_index(inplace=True, drop=True)
            metadata_df[&#39;source&#39;] = self.source

            if datetime.now().day &lt; 15:
                meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
            else:
                meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;
            filename = f&#39;bts_product_metadata_all_{meta_date}&#39;
            metadata_df.to_feather(self.metadata_path/filename)

            self.logger.info(
                f&#39;Metadata file created. Please look for file {filename} in path {self.metadata_path}&#39;)
            print(
                f&#39;Metadata file created. Please look for file {filename} in path {self.metadata_path}&#39;)

            if clean:
                cleaner = Cleaner(path=self.path)
                _ = cleaner.clean(
                    data=self.metadata_path/filename)
                self.logger.info(
                    &#39;Metadata Cleaned and Removed Duplicates for Details/Review/Image Extraction.&#39;)

            if delete_progress:
                shutil.rmtree(
                    f&#39;{self.metadata_path}\\current_progress&#39;, ignore_errors=True)
                self.logger.info(&#39;Progress files deleted&#39;)

    def terminate_logging(self):
        &#34;&#34;&#34;terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.
        &#34;&#34;&#34;
        self.logger.handlers.clear()
        self.prod_meta_log.stop_log()


class DetailReview(Boots):
    &#34;&#34;&#34;DetailReview class carries out scraping of Boots Detail and Review data.

    Args:
        Boots (Browser): Class that initializes folder paths and selenium webdriver for data scraping.
    &#34;&#34;&#34;

    def __init__(self, log: bool = True, path: Path = Path.cwd()):
        &#34;&#34;&#34;__init__ DetailReview class instace initializer.

        This method sets all the folder paths required for Metadata crawler to work.
        If the paths does not exist the paths get automatically created depending on current directory
        or provided directory.

        Args:
            log (bool, optional): Whether to create crawling exception and progess log. Defaults to True.
            path (Path, optional): Folder path where the Metadata will be extracted.
                                   Defaults to current directory(Path.cwd()).

        &#34;&#34;&#34;
        super().__init__(path=path, data_def=&#39;detail_review_image&#39;)
        self.path = Path(path)
        self.detail_current_progress_path = self.detail_path/&#39;current_progress&#39;
        self.detail_current_progress_path.mkdir(parents=True, exist_ok=True)

        self.review_current_progress_path = self.review_path/&#39;current_progress&#39;
        self.review_current_progress_path.mkdir(parents=True, exist_ok=True)

        old_detail_files = list(self.detail_path.glob(
            &#39;bts_product_detail_all*&#39;)) + list(self.detail_path.glob(
                &#39;bts_product_item_all*&#39;))
        for f in old_detail_files:
            shutil.move(str(f), str(self.old_detail_files_path))

        old_clean_detail_files = os.listdir(self.detail_clean_path)
        for f in old_clean_detail_files:
            shutil.move(str(self.detail_clean_path/f),
                        str(self.old_detail_clean_files_path))

        old_review_files = list(self.review_path.glob(
            &#39;bts_product_review_all*&#39;))
        for f in old_review_files:
            shutil.move(str(f), str(self.old_review_files_path))

        old_clean_review_files = os.listdir(self.review_clean_path)
        for f in old_clean_review_files:
            shutil.move(str(self.review_clean_path/f),
                        str(self.old_review_clean_files_path))
        if log:
            self.prod_detail_review_image_log = Logger(
                &#34;bts_prod_review_image_extraction&#34;, path=self.crawl_log_path)
            self.logger, _ = self.prod_detail_review_image_log.start_log()

    def get_details(self, drv: webdriver.Firefox, prod_id: str, product_name: str) -&gt; Tuple[dict, pd.DataFrame]:
        &#34;&#34;&#34;get_detail scrapes individual product pages for price, ingredients, color etc.

        Get Detail crawls product specific page and scrapes data such as ingredients, review rating distribution,
        size specific prices, color, product claims and other information pertaining to one individual product.

        Args:
            drv (webdriver.Firefox): Selenium webdriver with opened product page.
            prod_id (str): Id of the product from metdata.
            product_name (str): Name of the product from metadata.

        Returns:
            Tuple[dict, pd.DataFrame]: Dictionary containing details of a product and dataframe containing item
                                       attributes of a product.
        &#34;&#34;&#34;
        def get_product_attributes(drv: webdriver.Firefox, prod_id: str, product_name: str):
            &#34;&#34;&#34;get_product_attributes uses get_item_attribute method to scrape item details and stores
            in a list which is returned to get_detail method for storing in a product specific dataframe.

            Args:
                drv (webdriver.Firefox): Selenium webdriver with opened product page.
                prod_id (str): Id of the product from metdata.
                product_name (str): Name of the product from metadata.

            Returns:
                list: List containing all product item attributes of multiple varieties with name and id.
            &#34;&#34;&#34;
            # get all the variation of product
            product_attributes = []

            # product_variety = drv.find_elements_by_css_selector(
            #     &#39;li[id*=&#34;size_combo_button_pdp&#34;]&#39;)

            try:
                item_ingredients = drv.find_element_by_xpath(
                    &#39;//div[h3[@id=&#34;product_ingredients&#34;]]&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                item_ingredients = &#39;&#39;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            try:
                item_price = drv.find_element_by_xpath(
                    &#39;//div[@id=&#34;PDP_productPrice&#34;]&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                item_price = &#39;&#39;

            try:
                item_size_price = drv.find_element_by_css_selector(
                    &#39;div.details&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                item_size_price = &#39;&#39;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) item_size does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            item_size_price = item_price + &#34; per &#34; + item_size_price

            if len(item_size_price.split(&#39;|&#39;)) &gt; 0:

                for i in item_size_price.split(&#39;|&#39;):
                    ps = i.split(&#39;per&#39;)
                    item_price = ps[0].strip()
                    item_size = ps[1].strip()
                    product_attributes.append(
                        {&#34;prod_id&#34;: prod_id, &#34;product_name&#34;: product_name,
                         &#34;item_name&#34;: &#39;&#39;, &#34;item_size&#34;: item_size,
                         &#34;item_price&#34;: item_price, &#34;item_ingredients&#34;: item_ingredients
                         }
                    )

            else:
                product_attributes.append(
                    {&#34;prod_id&#34;: prod_id, &#34;product_name&#34;: product_name,
                     &#34;item_name&#34;: &#39;&#39;, &#34;item_size&#34;: item_size_price,
                     &#34;item_price&#34;: item_price, &#34;item_ingredients&#34;: item_ingredients
                     }
                )

            return product_attributes

        try:
            abt_product = drv.find_element_by_css_selector(
                &#39;div[id=&#34;contentOmnipresent&#34;]&#39;).text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            abt_product = &#39;&#39;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) product detail does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        try:
            how_to_use = drv.find_element_by_xpath(
                &#39;//div[h3[@id=&#34;product_how_to_use&#34;]]&#39;).text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            how_to_use = &#39;&#39;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) how_to_use does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        try:
            reviews = drv.find_element_by_css_selector(
                &#39;span[itemprop=&#34;reviewCount&#34;]&#39;).text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            reviews = 0
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) reviews does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        try:
            ratings = drv.find_elements_by_css_selector(
                &#39;div.bv-inline-histogram-ratings-score&gt;span:nth-child(1)&#39;)
            five_star = ratings[0].text
            four_star = ratings[1].text
            three_star = ratings[2].text
            two_star = ratings[3].text
            one_star = ratings[4].text

        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            five_star = &#39;&#39;
            four_star = &#39;&#39;
            three_star = &#39;&#39;
            two_star = &#39;&#39;
            one_star = &#39;&#39;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) rating_distribution does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        detail = {&#34;prod_id&#34;: prod_id,
                  &#34;product_name&#34;: product_name,
                  &#34;abt_product&#34;: abt_product,
                  &#34;abt_brand&#34;: &#39;&#39;,
                  &#34;how_to_use&#34;: how_to_use,
                  &#34;reviews&#34;: reviews,
                  &#34;votes&#34;: &#39;&#39;,
                  &#34;five_star&#34;: five_star,
                  &#34;four_star&#34;: four_star,
                  &#34;three_star&#34;: three_star,
                  &#34;two_star&#34;: two_star,
                  &#34;one_star&#34;: one_star,
                  &#34;would_recommend&#34;: &#39;&#39;,
                  &#34;first_review_date&#34;: &#39;&#39;
                  }

        item = pd.DataFrame(
            get_product_attributes(drv, prod_id, product_name))

        return detail, item

    def get_reviews(self,  drv: webdriver.Firefox, prod_id: str, product_name: str,
                    last_scraped_review_date: str, no_of_reviews: int,
                    incremental: bool = True, reviews: list = []) -&gt; list:
        &#34;&#34;&#34;get_reviews Crawls individual product pages for review text, title, date user attributes etc.

        Args:
            drv (webdriver.Firefox): drv (webdriver.Firefox): Selenium webdriver with opened product page.
            prod_id (str): Id of the product from metdata.
            product_name (str): Name of the product from metadata.
            last_scraped_review_date (str): The last review date scraped as per database.
            no_of_reviews (int): No.of reviews a product has.
            incremental (bool, optional): Whether to scrape reviews incrementally from last scraped review date. Defaults to True.
            reviews (list, optional): Empty intermediate list to store data during parallel crawl. Defaults to []

        Returns:
            list: List od dictionaries containing all the scraped reviews of a product.
        &#34;&#34;&#34;
        # print(no_of_reviews)
        # drv.find_element_by_class_name(&#39;css-2rg6q7&#39;).click()
        if incremental and last_scraped_review_date != &#39;&#39;:
            for i in range(no_of_reviews//30):
                if i &gt;= 100:
                    break
                time.sleep(0.4)
                revs = drv.find_elements_by_css_selector(
                    &#39;ol[class=&#34;bv-content-list bv-content-list-reviews&#34;]&gt;li&#39;)
                date1 = convert_ago_to_date(revs[-1].find_element_by_css_selector(&#39;div.bv-content-datetime&gt;meta[itemprop=&#34;dateCreated&#34;]&#39;
                                                                                  ).get_attribute(&#39;content&#39;))
                date2 = convert_ago_to_date(revs[-2].find_element_by_css_selector(&#39;div.bv-content-datetime&gt;meta[itemprop=&#34;dateCreated&#34;]&#39;
                                                                                  ).get_attribute(&#39;content&#39;))
                try:
                    if pd.to_datetime(date1, infer_datetime_format=True)\
                            &lt; pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                        # print(&#39;breaking incremental&#39;)
                        break
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    try:
                        if pd.to_datetime(date2, infer_datetime_format=True)\
                                &lt; pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                            # print(&#39;breaking incremental&#39;)
                            break
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        self.logger.info(str.encode(f&#39;Product: {product_name} prod_id: {prod_id} \
                                                        last_scraped_review_date to current review date \
                                                        comparision failed.(page: {product_page})&#39;,
                                                    &#39;utf-8&#39;, &#39;ignore&#39;))
                        # print(&#39;in second except block&#39;)
                        continue

                if len(revs) &gt;= 2000:
                    break
                try:
                    accept_alert(drv, 1)
                    close_popups(drv)
                    show_more_review_button = drv.find_element_by_css_selector(
                        &#39;span[class=&#34;bv-content-btn-pages-load-more-text&#34;]&#39;)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Loaded all the reviews. No more reviews exist.&#39;)
                    show_more_review_button = &#39;&#39;
                    break
                if show_more_review_button != &#39;&#39;:
                    self.scroll_to_element(
                        drv, show_more_review_button)
                    ActionChains(drv).move_to_element(
                        show_more_review_button).click(show_more_review_button).perform()
                    # print(&#39;loading more reviews&#39;)
                time.sleep(0.4)
        else:
            for n in range(no_of_reviews//30+5):
                &#39;&#39;&#39;
                code will stop after getting 1800 reviews of one particular product
                when crawling all reviews. By default it will get latest 1800 reviews.
                then in subsequent incremental runs it will get al new reviews on weekly basis
                &#39;&#39;&#39;
                if n &gt;= 100:  # 200:
                    break
                time.sleep(1)

                # close any opened popups by escape
                accept_alert(drv, 1)
                close_popups(drv)
                try:
                    show_more_review_button = drv.find_element_by_css_selector(
                        &#39;span[class=&#34;bv-content-btn-pages-load-more-text&#34;]&#39;)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Loaded all the reviews. No more reviews exist.&#39;)
                    show_more_review_button = &#39;&#39;
                    break
                if show_more_review_button != &#39;&#39;:
                    self.scroll_to_element(
                        drv, show_more_review_button)
                    ActionChains(drv).move_to_element(
                        show_more_review_button).click(show_more_review_button).perform()
                    # print(&#39;loading more reviews&#39;)

        accept_alert(drv, 2)
        close_popups(drv)

        product_reviews = drv.find_elements_by_css_selector(
            &#39;ol[class=&#34;bv-content-list bv-content-list-reviews&#34;]&gt;li&#39;)

        r = 0
        for rev in product_reviews:
            accept_alert(drv, 0.5)
            close_popups(drv)
            try:
                self.scroll_to_element(drv, rev)
                ActionChains(drv).move_to_element(rev).perform()
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to scroll to review.&#39;)
                pass

            try:
                review_text = rev.find_element_by_css_selector(
                    &#39;div.bv-content-summary-body-text&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_text. Skip review.&#39;)
                continue

            try:
                review_date = convert_ago_to_date(
                    rev.find_element_by_css_selector(&#39;div.bv-content-datetime&gt;meta[itemprop=&#34;dateCreated&#34;]&#39;).get_attribute(&#39;content&#39;))
                if pd.to_datetime(review_date, infer_datetime_format=True) &lt;= \
                        pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                    continue
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_date.&#39;)
                review_date = &#39;&#39;

            try:
                review_title = rev.find_element_by_css_selector(
                    &#39;h3.bv-content-title&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_title.&#39;)
                review_title = &#39;&#39;

            try:
                product_variant = rev.find_element_by_class_name(
                    &#39;css-1op1cn7&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract product_variant.&#39;)
                product_variant = &#39;&#39;

            try:
                user_rating = rev.find_element_by_css_selector(
                    &#39;span.bv-rating-stars-container&gt;span.bv-off-screen&#39;).text.split(&#34; &#34;)[0]
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract user_rating.&#39;)
                user_rating = &#39;&#39;

            try:
                helpful_yes = rev.find_element_by_css_selector(
                    &#39;div.bv-content-feedback-btn-container&gt;button:nth-child(1) span.bv-content-btn-count&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Helpful Yes not found.&#39;)
                helpful_yes = &#39;&#39;

            try:
                helpful_no = rev.find_element_by_css_selector(
                    &#39;div.bv-content-feedback-btn-container&gt;button:nth-child(2) span.bv-content-btn-count&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Helpful No not found.&#39;)
                helpful_no = &#39;&#39;

            try:
                recommend = rev.find_element_by_css_selector(
                    &#39;dl[class*=&#34;recommend&#34;] span.bv-content-data-label&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract recommend.&#39;)
                recommend = &#39;&#39;

            reviews.append(
                {
                    &#39;prod_id&#39;: prod_id, &#39;product_name&#39;: product_name,
                    &#39;user_attribute&#39;: &#39;&#39;, &#39;product_variant&#39;: product_variant,
                    &#39;review_title&#39;: review_title, &#39;review_text&#39;: review_text,
                    &#39;review_rating&#39;: user_rating, &#39;recommend&#39;: recommend,
                    &#39;review_date&#39;: review_date, &#34;helpful_y&#34;: helpful_yes,
                    &#34;helpful_n&#34;: helpful_no
                }
            )
        return reviews

    def crawl_page(self, indices: list, open_headless: bool, open_with_proxy_server: bool,
                   randomize_proxy_usage: bool, detail_data: list = [],
                   item_df=pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                                 &#39;item_size&#39;, &#39;item_price&#39;,
                                                 &#39;item_ingredients&#39;]),
                   review_data: list = [], incremental: bool = True):
        &#34;&#34;&#34;crawl_page opens each product url in a sepearate browser to scrape detail and review data together.

        The scraping happens in a multi-threaded manner to extract product information of up to 20 products simultaneously.
        Args:
            indices (list): list of indices or range of indices of product urls to scrape.
            open_headless (bool): Whether to open browser headless.
            open_with_proxy_server (bool): Whether to use ip rotation service.
            randomize_proxy_usage (bool): Whether to use both proxy and native network in tandem to decrease proxy requests.
            detail_data (list, optional): Empty intermediate list to store data during parallel crawl. Defaults to [].
            item_df ([type], optional): Empty intermediate dataframe to store data during parallel crawl.
                                        Defaults to []. Defaults to pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                                                                         &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;]).
            review_data (list, optional): Empty intermediate list to store data during parallel crawl. Defaults to [].
            incremental (bool, optional): Whether to scrape reviews incrementally from last scraped review date. Defaults to True.
        &#34;&#34;&#34;

        def store_data_refresh_mem(detail_data: list, item_df: pd.DataFrame,
                                   review_data: list) -&gt; Tuple[list, pd.DataFrame, list]:
            &#34;&#34;&#34;store_data_refresh_mem method stores crawled data in regular interval to free up system memory.

            Store data after five products are extracted every time to free up RAM.

            Args:
                detail_data (list): List containing crawled detail data to store.
                item_df (pd.DataFrame): Dataframe containing scraped item data to store.
                review_data (list): List containing scraped Review data to store.

            Returns:
                Tuple[list, pd.DataFrame, list]: Empty list, dataframe and list to accumulate data from next product scraping.
            &#34;&#34;&#34;
            pd.DataFrame(detail_data).to_csv(self.detail_current_progress_path /
                                             f&#39;bts_prod_detail_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                                             index=None)
            item_df.reset_index(inplace=True, drop=True)
            item_df.to_csv(self.detail_current_progress_path /
                           f&#39;bts_prod_item_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                           index=None)
            item_df = pd.DataFrame(columns=[
                                   &#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;, &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;])

            pd.DataFrame(review_data).to_csv(self.review_current_progress_path /
                                             f&#39;bts_prod_review_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                                             index=None)
            self.meta.to_csv(
                self.path/&#39;boots/bts_detail_review_image_progress_tracker.csv&#39;, index=None)
            return [], item_df, []

        for prod in self.meta.index[self.meta.index.isin(indices)]:
            #  ignore already extracted products
            if self.meta.loc[prod, &#39;scraped&#39;] in [&#39;Y&#39;, &#39;NA&#39;]:
                continue
            # print(prod, self.meta.loc[prod, &#39;detail_scraped&#39;])
            prod_id = self.meta.loc[prod, &#39;prod_id&#39;]
            product_name = self.meta.loc[prod, &#39;product_name&#39;]
            product_page = self.meta.loc[prod, &#39;product_page&#39;]
            last_scraped_review_date = self.meta.loc[prod,
                                                     &#39;last_scraped_review_date&#39;]
            # create webdriver
            if randomize_proxy_usage:
                use_proxy = np.random.choice([True, False])
            else:
                use_proxy = True
            if open_with_proxy_server:
                # print(use_proxy)
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
                                        path=self.detail_path)
            else:
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
                                        path=self.detail_path)
            # open product page
            drv.get(product_page)
            time.sleep(15)  # 30
            accept_alert(drv, 5)
            close_popups(drv)

            # check product page is valid and exists
            try:
                close_popups(drv)
                accept_alert(drv, 2)
                price = drv.find_element_by_xpath(
                    &#39;//div[@id=&#34;PDP_productPrice&#34;]&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                drv.quit()
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) no longer exists in the previously fetched link.\
                        (link:{product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                self.meta.loc[prod, &#39;detail_scraped&#39;] = &#39;NA&#39;
                continue

            # self.scroll_down_page(drv, speed=6, h2=0.6)
            # time.sleep(5)

            detail, item = self.get_details(drv, prod_id, product_name)

            detail_data.append(detail)
            item_df = pd.concat(
                [item_df, item], axis=0)

            # item_data.append(product_attributes)
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) details extracted successfully&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            try:
                close_popups(drv)
                accept_alert(drv, 1)
                no_of_reviews = int(drv.find_element_by_css_selector(
                    &#39;span[itemprop=&#34;reviewCount&#34;]&#39;).text)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                self.logger.info(str.encode(f&#39;Product: {product_name} prod_id: {prod_id} reviews extraction failed.\
                                              Either product has no reviews or not\
                                              available for sell currently.(page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                no_of_reviews = 0

            if no_of_reviews &gt; 0:
                reviews = self.get_reviews(
                    drv, prod_id, product_name, last_scraped_review_date, no_of_reviews, incremental, reviews=[])

                if len(reviews) &gt; 0:
                    review_data.extend(reviews)

                    if not incremental:
                        self.logger.info(str.encode(
                            f&#39;Product_name: {product_name} prod_id:{prod_id} reviews extracted successfully.(total_reviews: {no_of_reviews}, \
                            extracted_reviews: {len(reviews)}, page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    else:
                        self.logger.info(str.encode(
                            f&#39;Product_name: {product_name} prod_id:{prod_id} new reviews extracted successfully.\
                                (no_of_new_extracted_reviews: {len(reviews)},\
                                page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            else:
                reviews = 0

            if prod != 0 and prod % 5 == 0:
                detail_data, item_df, review_data = store_data_refresh_mem(
                    detail_data, item_df, review_data)

            self.meta.loc[prod, &#39;scraped&#39;] = &#39;Y&#39;
            drv.quit()

        detail_data, item_df, review_data = store_data_refresh_mem(
            detail_data, item_df, review_data)
        self.logger.info(
            f&#39;Extraction Complete for start_idx: {indices[0]} to end_idx: {indices[-1]}. Or for list of values.&#39;)

    def extract(self, metadata: Union[pd.DataFrame, str, Path], download: bool = True, n_workers: int = 5,
                fresh_start: bool = False, auto_fresh_start: bool = False, incremental: bool = True,
                open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = True,
                start_idx: Optional[int] = None, end_idx: Optional[int] = None, list_of_index=None,
                compile_progress_files: bool = False, clean: bool = True, delete_progress: bool = False):
        &#34;&#34;&#34;extract method controls all properties of the spiders and runs multi-threaded web crawling.

        Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
        This method has four major functionality:
        * 1. Run the spider
        * 2. Store data in regular intervals to free up ram
        * 3. Compile all crawled data into one file.
        * 4. Clean and push cleaned data to S3 storage for further algorithmic processing.

        Args:
            metadata (Union[pd.DataFrame, str, Path]): Dataframe containing product specific url, name and id of the products to be scraped.
            download (bool, optional): Whether to crawl data from or compile crawled data into one file. Defaults to True.
            n_workers (int, optional): No. of parallel threads to run. Defaults to 5.
            fresh_start (bool, optional): Whether to continue last crawl job or start new one. Defaults to False.
            auto_fresh_start (bool, optional): Whether to automatically start a new crawl job if last job was finished. Defaults to False.
            incremental (bool, optional): Whether to scrape reviews incrementally from last scraped review date. Defaults to True.
            open_headless (bool, optional):  Whether to open browser headless. Defaults to False.
            open_with_proxy_server (bool, optional): Whether to use ip rotation service. Defaults to True.
            randomize_proxy_usage (bool, optional): Whether to use both proxy and native network in tandem to decrease proxy requests.
            Defaults to False.
            start_idx (Optional[int], optional): Starting index of the links to crawl. Defaults to None.
            end_idx (Optional[int], optional): Ending index of the links to crawl. Defaults to None.
            list_of_index ([type], optional): List of indices or range of indices of product urls to scrape. Defaults to None.
            compile_progress_files (bool, optional): Whether to combine crawled data into one file. Defaults to False.
            clean (bool, optional): Whether to clean the compiled data. Defaults to True.
            delete_progress (bool, optional): Whether to delete intermediate data after compilation into one file. Defaults to False.
        &#34;&#34;&#34;
        def fresh():
            if not isinstance(metadata, pd.core.frame.DataFrame):
                list_of_files = self.metadata_clean_path.glob(
                    &#39;no_cat_cleaned_bts_product_metadata_all*&#39;)
                self.meta = pd.read_feather(max(list_of_files, key=os.path.getctime))[
                    [&#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;, &#39;last_scraped_review_date&#39;]]
            else:
                self.meta = metadata[[
                    &#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;, &#39;last_scraped_review_date&#39;]]
            self.meta.last_scraped_review_date.fillna(&#39;&#39;, inplace=True)
            self.meta[&#39;scraped&#39;] = &#39;N&#39;

        if download:
            if fresh_start:
                fresh()
                self.logger.info(
                    &#39;Starting Fresh Deatil Review Image Extraction.&#39;)
            else:
                if Path(self.path/&#39;boots/bts_detail_review_image_progress_tracker.csv&#39;).exists():
                    self.meta = pd.read_csv(
                        self.path/&#39;boots/bts_detail_review_image_progress_tracker.csv&#39;)
                    if sum(self.meta.scraped == &#39;N&#39;) == 0:
                        if auto_fresh_start:
                            fresh()
                            self.logger.info(
                                &#39;Last Run was Completed. Starting Fresh Extraction.&#39;)
                        else:
                            self.logger.info(
                                &#39;Deatil Review Image extraction for this cycle is complete.&#39;)
                    else:
                        self.logger.info(
                            &#39;Continuing Deatil Review Image Extraction From Last Run.&#39;)
                else:
                    if auto_fresh_start:
                        fresh()
                        self.logger.info(
                            &#39;Deatil Review Image Progress Tracker does not exist. Starting Fresh Extraction.&#39;)

            # set list or range of product indices to crawl
            if list_of_index:
                indices = list_of_index
            elif start_idx and end_idx is None:
                indices = range(start_idx, len(self.meta))
            elif start_idx is None and end_idx:
                indices = range(0, end_idx)
            elif start_idx is not None and end_idx is not None:
                indices = range(start_idx, end_idx)
            else:
                indices = range(len(self.meta))
            print(indices)

            if list_of_index:
                self.crawl_page(indices=list_of_index, incremental=incremental,
                                open_headless=open_headless,
                                open_with_proxy_server=open_with_proxy_server,
                                randomize_proxy_usage=randomize_proxy_usage)
            else:  # By default the code will with 5 concurrent threads. you can change this behaviour by changing n_workers
                if start_idx:
                    lst_of_lst = ranges(
                        indices[-1]+1, n_workers, start_idx=start_idx)
                else:
                    lst_of_lst = ranges(len(indices), n_workers)
                print(lst_of_lst)
                # detail_Data and item_data are lists of empty lists so that each namepace of function call will have its separate detail_data
                # list to strore scraped dictionaries. will save memory(ram/hard-disk) consumption. will stop data duplication
                headless = [open_headless for i in lst_of_lst]
                proxy = [open_with_proxy_server for i in lst_of_lst]
                rand_proxy = [randomize_proxy_usage for i in lst_of_lst]
                detail_data = [[] for i in lst_of_lst]  # type: List
                # item_data=[[] for i in lst_of_lst]  # type: List
                item_df = [pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                                 &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;])
                           for i in lst_of_lst]
                review_data = [[] for i in lst_of_lst]  # type: list
                inc_list = [incremental for i in lst_of_lst]  # type: list
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    # but each of the function namespace will be modifying only one metadata tracing file so that progress saving
                    # is tracked correctly. else multiple progress tracker file will be created with difficulty to combine correct
                    # progress information
                    print(&#39;inside executor&#39;)
                    executor.map(self.crawl_page, lst_of_lst,
                                 headless, proxy, rand_proxy,
                                 detail_data, item_df, review_data,
                                 inc_list)
        try:
            if compile_progress_files:
                self.logger.info(&#39;Creating Combined Detail and Item File&#39;)
                if datetime.now().day &lt; 15:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
                else:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;

                det_li = []
                self.bad_det_li = []
                detail_files = [f for f in self.detail_current_progress_path.glob(
                    &#34;bts_prod_detail_extract_progress_*&#34;)]
                for file in detail_files:
                    try:
                        df = pd.read_csv(file)
                    except Exception:
                        self.bad_det_li.append(file)
                    else:
                        det_li.append(df)

                detail_df = pd.concat(det_li, axis=0, ignore_index=True)
                detail_df.drop_duplicates(inplace=True)
                detail_df.reset_index(inplace=True, drop=True)
                detail_df[&#39;meta_date&#39;] = meta_date
                detail_filename = f&#39;bts_product_detail_all_{meta_date}.csv&#39;
                detail_df.to_csv(self.detail_path/detail_filename, index=None)
                # detail_df.to_feather(self.detail_path/detail_filename)

                item_li = []
                self.bad_item_li = []
                item_files = [f for f in self.detail_current_progress_path.glob(
                    &#34;bts_prod_item_extract_progress_*&#34;)]
                for file in item_files:
                    try:
                        idf = pd.read_csv(file)
                    except Exception:
                        self.bad_item_li.append(file)
                    else:
                        item_li.append(idf)

                item_dataframe = pd.concat(item_li, axis=0, ignore_index=True)
                item_dataframe.drop_duplicates(inplace=True)
                item_dataframe.reset_index(inplace=True, drop=True)
                item_dataframe[&#39;meta_date&#39;] = meta_date
                item_filename = f&#39;bts_product_item_all_{meta_date}.csv&#39;
                item_dataframe.to_csv(
                    self.detail_path/item_filename, index=None)
                # item_df.to_feather(self.detail_path/item_filename)

                self.logger.info(
                    f&#39;Detail and Item files created. Please look for file bts_product_detail_all and\
                        bts_product_item_all in path {self.detail_path}&#39;)
                print(
                    f&#39;Detail and Item files created. Please look for file bts_product_detail_all and\
                        bts_product_item_all in path {self.detail_path}&#39;)

                self.logger.info(&#39;Creating Combined Review File&#39;)

                rev_li = []
                self.bad_rev_li = []
                review_files = [f for f in self.review_current_progress_path.glob(
                    &#34;bts_prod_review_extract_progress_*&#34;)]
                for file in review_files:
                    try:
                        df = pd.read_csv(file)
                    except Exception:
                        self.bad_rev_li.append(file)
                    else:
                        rev_li.append(df)
                rev_df = pd.concat(rev_li, axis=0, ignore_index=True)
                rev_df.drop_duplicates(inplace=True)
                rev_df.reset_index(inplace=True, drop=True)
                rev_df[&#39;meta_date&#39;] = pd.to_datetime(meta_date).date()
                review_filename = f&#39;bts_product_review_all_{meta_date}&#39;
                # , index=None)
                rev_df.to_feather(self.review_path/review_filename)

                self.logger.info(
                    f&#39;Review file created. Please look for file bts_product_review_all in path {self.review_path}&#39;)
                print(
                    f&#39;Review file created. Please look for file bts_product_review_all in path {self.review_path}&#39;)

                if clean:
                    detail_cleaner = Cleaner(path=self.path)
                    self.detail_clean_df = detail_cleaner.clean(
                        self.detail_path/detail_filename)

                    item_cleaner = Cleaner(path=self.path)
                    self.item_clean_df, self.ing_clean_df = item_cleaner.clean(
                        self.detail_path/item_filename)

                    review_cleaner = Cleaner(path=self.path)
                    self.review_clean_df = review_cleaner.clean(
                        self.review_path/review_filename)

                    file_creation_status = True
            else:
                file_creation_status = False
        except Exception as ex:
            log_exception(
                self.logger, additional_information=f&#39;Detail Item Review Combined File Creation Failed.&#39;)
            file_creation_status = False

        if delete_progress and file_creation_status:
            shutil.rmtree(
                f&#39;{self.detail_path}\\current_progress&#39;, ignore_errors=True)
            shutil.rmtree(
                f&#39;{self.review_path}\\current_progress&#39;, ignore_errors=True)
            self.logger.info(&#39;Progress files deleted&#39;)

    def terminate_logging(self):
        &#34;&#34;&#34;terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.
        &#34;&#34;&#34;
        self.logger.handlers.clear()
        self.prod_detail_review_image_log.stop_log()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="meiyume.bts.crawler.DetailReview"><code class="flex name class">
<span>class <span class="ident">DetailReview</span></span>
<span>(</span><span>log:bool=True, path:pathlib.Path=WindowsPath('D:/Amit/Meiyume/meiyume_master_source_codes'))</span>
</code></dt>
<dd>
<div class="desc"><p>DetailReview class carries out scraping of Boots Detail and Review data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>Boots</code></strong> :&ensp;<code>Browser</code></dt>
<dd>Class that initializes folder paths and selenium webdriver for data scraping.</dd>
</dl>
<p><strong>init</strong> DetailReview class instace initializer.</p>
<p>This method sets all the folder paths required for Metadata crawler to work.
If the paths does not exist the paths get automatically created depending on current directory
or provided directory.</p>
<h2 id="args_1">Args</h2>
<dl>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to create crawling exception and progess log. Defaults to True.</dd>
<dt><strong><code>path</code></strong> :&ensp;<code>Path</code>, optional</dt>
<dd>Folder path where the Metadata will be extracted.
Defaults to current directory(Path.cwd()).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DetailReview(Boots):
    &#34;&#34;&#34;DetailReview class carries out scraping of Boots Detail and Review data.

    Args:
        Boots (Browser): Class that initializes folder paths and selenium webdriver for data scraping.
    &#34;&#34;&#34;

    def __init__(self, log: bool = True, path: Path = Path.cwd()):
        &#34;&#34;&#34;__init__ DetailReview class instace initializer.

        This method sets all the folder paths required for Metadata crawler to work.
        If the paths does not exist the paths get automatically created depending on current directory
        or provided directory.

        Args:
            log (bool, optional): Whether to create crawling exception and progess log. Defaults to True.
            path (Path, optional): Folder path where the Metadata will be extracted.
                                   Defaults to current directory(Path.cwd()).

        &#34;&#34;&#34;
        super().__init__(path=path, data_def=&#39;detail_review_image&#39;)
        self.path = Path(path)
        self.detail_current_progress_path = self.detail_path/&#39;current_progress&#39;
        self.detail_current_progress_path.mkdir(parents=True, exist_ok=True)

        self.review_current_progress_path = self.review_path/&#39;current_progress&#39;
        self.review_current_progress_path.mkdir(parents=True, exist_ok=True)

        old_detail_files = list(self.detail_path.glob(
            &#39;bts_product_detail_all*&#39;)) + list(self.detail_path.glob(
                &#39;bts_product_item_all*&#39;))
        for f in old_detail_files:
            shutil.move(str(f), str(self.old_detail_files_path))

        old_clean_detail_files = os.listdir(self.detail_clean_path)
        for f in old_clean_detail_files:
            shutil.move(str(self.detail_clean_path/f),
                        str(self.old_detail_clean_files_path))

        old_review_files = list(self.review_path.glob(
            &#39;bts_product_review_all*&#39;))
        for f in old_review_files:
            shutil.move(str(f), str(self.old_review_files_path))

        old_clean_review_files = os.listdir(self.review_clean_path)
        for f in old_clean_review_files:
            shutil.move(str(self.review_clean_path/f),
                        str(self.old_review_clean_files_path))
        if log:
            self.prod_detail_review_image_log = Logger(
                &#34;bts_prod_review_image_extraction&#34;, path=self.crawl_log_path)
            self.logger, _ = self.prod_detail_review_image_log.start_log()

    def get_details(self, drv: webdriver.Firefox, prod_id: str, product_name: str) -&gt; Tuple[dict, pd.DataFrame]:
        &#34;&#34;&#34;get_detail scrapes individual product pages for price, ingredients, color etc.

        Get Detail crawls product specific page and scrapes data such as ingredients, review rating distribution,
        size specific prices, color, product claims and other information pertaining to one individual product.

        Args:
            drv (webdriver.Firefox): Selenium webdriver with opened product page.
            prod_id (str): Id of the product from metdata.
            product_name (str): Name of the product from metadata.

        Returns:
            Tuple[dict, pd.DataFrame]: Dictionary containing details of a product and dataframe containing item
                                       attributes of a product.
        &#34;&#34;&#34;
        def get_product_attributes(drv: webdriver.Firefox, prod_id: str, product_name: str):
            &#34;&#34;&#34;get_product_attributes uses get_item_attribute method to scrape item details and stores
            in a list which is returned to get_detail method for storing in a product specific dataframe.

            Args:
                drv (webdriver.Firefox): Selenium webdriver with opened product page.
                prod_id (str): Id of the product from metdata.
                product_name (str): Name of the product from metadata.

            Returns:
                list: List containing all product item attributes of multiple varieties with name and id.
            &#34;&#34;&#34;
            # get all the variation of product
            product_attributes = []

            # product_variety = drv.find_elements_by_css_selector(
            #     &#39;li[id*=&#34;size_combo_button_pdp&#34;]&#39;)

            try:
                item_ingredients = drv.find_element_by_xpath(
                    &#39;//div[h3[@id=&#34;product_ingredients&#34;]]&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                item_ingredients = &#39;&#39;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            try:
                item_price = drv.find_element_by_xpath(
                    &#39;//div[@id=&#34;PDP_productPrice&#34;]&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                item_price = &#39;&#39;

            try:
                item_size_price = drv.find_element_by_css_selector(
                    &#39;div.details&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                item_size_price = &#39;&#39;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) item_size does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            item_size_price = item_price + &#34; per &#34; + item_size_price

            if len(item_size_price.split(&#39;|&#39;)) &gt; 0:

                for i in item_size_price.split(&#39;|&#39;):
                    ps = i.split(&#39;per&#39;)
                    item_price = ps[0].strip()
                    item_size = ps[1].strip()
                    product_attributes.append(
                        {&#34;prod_id&#34;: prod_id, &#34;product_name&#34;: product_name,
                         &#34;item_name&#34;: &#39;&#39;, &#34;item_size&#34;: item_size,
                         &#34;item_price&#34;: item_price, &#34;item_ingredients&#34;: item_ingredients
                         }
                    )

            else:
                product_attributes.append(
                    {&#34;prod_id&#34;: prod_id, &#34;product_name&#34;: product_name,
                     &#34;item_name&#34;: &#39;&#39;, &#34;item_size&#34;: item_size_price,
                     &#34;item_price&#34;: item_price, &#34;item_ingredients&#34;: item_ingredients
                     }
                )

            return product_attributes

        try:
            abt_product = drv.find_element_by_css_selector(
                &#39;div[id=&#34;contentOmnipresent&#34;]&#39;).text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            abt_product = &#39;&#39;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) product detail does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        try:
            how_to_use = drv.find_element_by_xpath(
                &#39;//div[h3[@id=&#34;product_how_to_use&#34;]]&#39;).text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            how_to_use = &#39;&#39;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) how_to_use does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        try:
            reviews = drv.find_element_by_css_selector(
                &#39;span[itemprop=&#34;reviewCount&#34;]&#39;).text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            reviews = 0
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) reviews does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        try:
            ratings = drv.find_elements_by_css_selector(
                &#39;div.bv-inline-histogram-ratings-score&gt;span:nth-child(1)&#39;)
            five_star = ratings[0].text
            four_star = ratings[1].text
            three_star = ratings[2].text
            two_star = ratings[3].text
            one_star = ratings[4].text

        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            five_star = &#39;&#39;
            four_star = &#39;&#39;
            three_star = &#39;&#39;
            two_star = &#39;&#39;
            one_star = &#39;&#39;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) rating_distribution does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        detail = {&#34;prod_id&#34;: prod_id,
                  &#34;product_name&#34;: product_name,
                  &#34;abt_product&#34;: abt_product,
                  &#34;abt_brand&#34;: &#39;&#39;,
                  &#34;how_to_use&#34;: how_to_use,
                  &#34;reviews&#34;: reviews,
                  &#34;votes&#34;: &#39;&#39;,
                  &#34;five_star&#34;: five_star,
                  &#34;four_star&#34;: four_star,
                  &#34;three_star&#34;: three_star,
                  &#34;two_star&#34;: two_star,
                  &#34;one_star&#34;: one_star,
                  &#34;would_recommend&#34;: &#39;&#39;,
                  &#34;first_review_date&#34;: &#39;&#39;
                  }

        item = pd.DataFrame(
            get_product_attributes(drv, prod_id, product_name))

        return detail, item

    def get_reviews(self,  drv: webdriver.Firefox, prod_id: str, product_name: str,
                    last_scraped_review_date: str, no_of_reviews: int,
                    incremental: bool = True, reviews: list = []) -&gt; list:
        &#34;&#34;&#34;get_reviews Crawls individual product pages for review text, title, date user attributes etc.

        Args:
            drv (webdriver.Firefox): drv (webdriver.Firefox): Selenium webdriver with opened product page.
            prod_id (str): Id of the product from metdata.
            product_name (str): Name of the product from metadata.
            last_scraped_review_date (str): The last review date scraped as per database.
            no_of_reviews (int): No.of reviews a product has.
            incremental (bool, optional): Whether to scrape reviews incrementally from last scraped review date. Defaults to True.
            reviews (list, optional): Empty intermediate list to store data during parallel crawl. Defaults to []

        Returns:
            list: List od dictionaries containing all the scraped reviews of a product.
        &#34;&#34;&#34;
        # print(no_of_reviews)
        # drv.find_element_by_class_name(&#39;css-2rg6q7&#39;).click()
        if incremental and last_scraped_review_date != &#39;&#39;:
            for i in range(no_of_reviews//30):
                if i &gt;= 100:
                    break
                time.sleep(0.4)
                revs = drv.find_elements_by_css_selector(
                    &#39;ol[class=&#34;bv-content-list bv-content-list-reviews&#34;]&gt;li&#39;)
                date1 = convert_ago_to_date(revs[-1].find_element_by_css_selector(&#39;div.bv-content-datetime&gt;meta[itemprop=&#34;dateCreated&#34;]&#39;
                                                                                  ).get_attribute(&#39;content&#39;))
                date2 = convert_ago_to_date(revs[-2].find_element_by_css_selector(&#39;div.bv-content-datetime&gt;meta[itemprop=&#34;dateCreated&#34;]&#39;
                                                                                  ).get_attribute(&#39;content&#39;))
                try:
                    if pd.to_datetime(date1, infer_datetime_format=True)\
                            &lt; pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                        # print(&#39;breaking incremental&#39;)
                        break
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    try:
                        if pd.to_datetime(date2, infer_datetime_format=True)\
                                &lt; pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                            # print(&#39;breaking incremental&#39;)
                            break
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        self.logger.info(str.encode(f&#39;Product: {product_name} prod_id: {prod_id} \
                                                        last_scraped_review_date to current review date \
                                                        comparision failed.(page: {product_page})&#39;,
                                                    &#39;utf-8&#39;, &#39;ignore&#39;))
                        # print(&#39;in second except block&#39;)
                        continue

                if len(revs) &gt;= 2000:
                    break
                try:
                    accept_alert(drv, 1)
                    close_popups(drv)
                    show_more_review_button = drv.find_element_by_css_selector(
                        &#39;span[class=&#34;bv-content-btn-pages-load-more-text&#34;]&#39;)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Loaded all the reviews. No more reviews exist.&#39;)
                    show_more_review_button = &#39;&#39;
                    break
                if show_more_review_button != &#39;&#39;:
                    self.scroll_to_element(
                        drv, show_more_review_button)
                    ActionChains(drv).move_to_element(
                        show_more_review_button).click(show_more_review_button).perform()
                    # print(&#39;loading more reviews&#39;)
                time.sleep(0.4)
        else:
            for n in range(no_of_reviews//30+5):
                &#39;&#39;&#39;
                code will stop after getting 1800 reviews of one particular product
                when crawling all reviews. By default it will get latest 1800 reviews.
                then in subsequent incremental runs it will get al new reviews on weekly basis
                &#39;&#39;&#39;
                if n &gt;= 100:  # 200:
                    break
                time.sleep(1)

                # close any opened popups by escape
                accept_alert(drv, 1)
                close_popups(drv)
                try:
                    show_more_review_button = drv.find_element_by_css_selector(
                        &#39;span[class=&#34;bv-content-btn-pages-load-more-text&#34;]&#39;)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Loaded all the reviews. No more reviews exist.&#39;)
                    show_more_review_button = &#39;&#39;
                    break
                if show_more_review_button != &#39;&#39;:
                    self.scroll_to_element(
                        drv, show_more_review_button)
                    ActionChains(drv).move_to_element(
                        show_more_review_button).click(show_more_review_button).perform()
                    # print(&#39;loading more reviews&#39;)

        accept_alert(drv, 2)
        close_popups(drv)

        product_reviews = drv.find_elements_by_css_selector(
            &#39;ol[class=&#34;bv-content-list bv-content-list-reviews&#34;]&gt;li&#39;)

        r = 0
        for rev in product_reviews:
            accept_alert(drv, 0.5)
            close_popups(drv)
            try:
                self.scroll_to_element(drv, rev)
                ActionChains(drv).move_to_element(rev).perform()
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to scroll to review.&#39;)
                pass

            try:
                review_text = rev.find_element_by_css_selector(
                    &#39;div.bv-content-summary-body-text&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_text. Skip review.&#39;)
                continue

            try:
                review_date = convert_ago_to_date(
                    rev.find_element_by_css_selector(&#39;div.bv-content-datetime&gt;meta[itemprop=&#34;dateCreated&#34;]&#39;).get_attribute(&#39;content&#39;))
                if pd.to_datetime(review_date, infer_datetime_format=True) &lt;= \
                        pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                    continue
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_date.&#39;)
                review_date = &#39;&#39;

            try:
                review_title = rev.find_element_by_css_selector(
                    &#39;h3.bv-content-title&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_title.&#39;)
                review_title = &#39;&#39;

            try:
                product_variant = rev.find_element_by_class_name(
                    &#39;css-1op1cn7&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract product_variant.&#39;)
                product_variant = &#39;&#39;

            try:
                user_rating = rev.find_element_by_css_selector(
                    &#39;span.bv-rating-stars-container&gt;span.bv-off-screen&#39;).text.split(&#34; &#34;)[0]
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract user_rating.&#39;)
                user_rating = &#39;&#39;

            try:
                helpful_yes = rev.find_element_by_css_selector(
                    &#39;div.bv-content-feedback-btn-container&gt;button:nth-child(1) span.bv-content-btn-count&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Helpful Yes not found.&#39;)
                helpful_yes = &#39;&#39;

            try:
                helpful_no = rev.find_element_by_css_selector(
                    &#39;div.bv-content-feedback-btn-container&gt;button:nth-child(2) span.bv-content-btn-count&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Helpful No not found.&#39;)
                helpful_no = &#39;&#39;

            try:
                recommend = rev.find_element_by_css_selector(
                    &#39;dl[class*=&#34;recommend&#34;] span.bv-content-data-label&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract recommend.&#39;)
                recommend = &#39;&#39;

            reviews.append(
                {
                    &#39;prod_id&#39;: prod_id, &#39;product_name&#39;: product_name,
                    &#39;user_attribute&#39;: &#39;&#39;, &#39;product_variant&#39;: product_variant,
                    &#39;review_title&#39;: review_title, &#39;review_text&#39;: review_text,
                    &#39;review_rating&#39;: user_rating, &#39;recommend&#39;: recommend,
                    &#39;review_date&#39;: review_date, &#34;helpful_y&#34;: helpful_yes,
                    &#34;helpful_n&#34;: helpful_no
                }
            )
        return reviews

    def crawl_page(self, indices: list, open_headless: bool, open_with_proxy_server: bool,
                   randomize_proxy_usage: bool, detail_data: list = [],
                   item_df=pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                                 &#39;item_size&#39;, &#39;item_price&#39;,
                                                 &#39;item_ingredients&#39;]),
                   review_data: list = [], incremental: bool = True):
        &#34;&#34;&#34;crawl_page opens each product url in a sepearate browser to scrape detail and review data together.

        The scraping happens in a multi-threaded manner to extract product information of up to 20 products simultaneously.
        Args:
            indices (list): list of indices or range of indices of product urls to scrape.
            open_headless (bool): Whether to open browser headless.
            open_with_proxy_server (bool): Whether to use ip rotation service.
            randomize_proxy_usage (bool): Whether to use both proxy and native network in tandem to decrease proxy requests.
            detail_data (list, optional): Empty intermediate list to store data during parallel crawl. Defaults to [].
            item_df ([type], optional): Empty intermediate dataframe to store data during parallel crawl.
                                        Defaults to []. Defaults to pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                                                                         &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;]).
            review_data (list, optional): Empty intermediate list to store data during parallel crawl. Defaults to [].
            incremental (bool, optional): Whether to scrape reviews incrementally from last scraped review date. Defaults to True.
        &#34;&#34;&#34;

        def store_data_refresh_mem(detail_data: list, item_df: pd.DataFrame,
                                   review_data: list) -&gt; Tuple[list, pd.DataFrame, list]:
            &#34;&#34;&#34;store_data_refresh_mem method stores crawled data in regular interval to free up system memory.

            Store data after five products are extracted every time to free up RAM.

            Args:
                detail_data (list): List containing crawled detail data to store.
                item_df (pd.DataFrame): Dataframe containing scraped item data to store.
                review_data (list): List containing scraped Review data to store.

            Returns:
                Tuple[list, pd.DataFrame, list]: Empty list, dataframe and list to accumulate data from next product scraping.
            &#34;&#34;&#34;
            pd.DataFrame(detail_data).to_csv(self.detail_current_progress_path /
                                             f&#39;bts_prod_detail_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                                             index=None)
            item_df.reset_index(inplace=True, drop=True)
            item_df.to_csv(self.detail_current_progress_path /
                           f&#39;bts_prod_item_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                           index=None)
            item_df = pd.DataFrame(columns=[
                                   &#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;, &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;])

            pd.DataFrame(review_data).to_csv(self.review_current_progress_path /
                                             f&#39;bts_prod_review_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                                             index=None)
            self.meta.to_csv(
                self.path/&#39;boots/bts_detail_review_image_progress_tracker.csv&#39;, index=None)
            return [], item_df, []

        for prod in self.meta.index[self.meta.index.isin(indices)]:
            #  ignore already extracted products
            if self.meta.loc[prod, &#39;scraped&#39;] in [&#39;Y&#39;, &#39;NA&#39;]:
                continue
            # print(prod, self.meta.loc[prod, &#39;detail_scraped&#39;])
            prod_id = self.meta.loc[prod, &#39;prod_id&#39;]
            product_name = self.meta.loc[prod, &#39;product_name&#39;]
            product_page = self.meta.loc[prod, &#39;product_page&#39;]
            last_scraped_review_date = self.meta.loc[prod,
                                                     &#39;last_scraped_review_date&#39;]
            # create webdriver
            if randomize_proxy_usage:
                use_proxy = np.random.choice([True, False])
            else:
                use_proxy = True
            if open_with_proxy_server:
                # print(use_proxy)
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
                                        path=self.detail_path)
            else:
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
                                        path=self.detail_path)
            # open product page
            drv.get(product_page)
            time.sleep(15)  # 30
            accept_alert(drv, 5)
            close_popups(drv)

            # check product page is valid and exists
            try:
                close_popups(drv)
                accept_alert(drv, 2)
                price = drv.find_element_by_xpath(
                    &#39;//div[@id=&#34;PDP_productPrice&#34;]&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                drv.quit()
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) no longer exists in the previously fetched link.\
                        (link:{product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                self.meta.loc[prod, &#39;detail_scraped&#39;] = &#39;NA&#39;
                continue

            # self.scroll_down_page(drv, speed=6, h2=0.6)
            # time.sleep(5)

            detail, item = self.get_details(drv, prod_id, product_name)

            detail_data.append(detail)
            item_df = pd.concat(
                [item_df, item], axis=0)

            # item_data.append(product_attributes)
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) details extracted successfully&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            try:
                close_popups(drv)
                accept_alert(drv, 1)
                no_of_reviews = int(drv.find_element_by_css_selector(
                    &#39;span[itemprop=&#34;reviewCount&#34;]&#39;).text)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                self.logger.info(str.encode(f&#39;Product: {product_name} prod_id: {prod_id} reviews extraction failed.\
                                              Either product has no reviews or not\
                                              available for sell currently.(page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                no_of_reviews = 0

            if no_of_reviews &gt; 0:
                reviews = self.get_reviews(
                    drv, prod_id, product_name, last_scraped_review_date, no_of_reviews, incremental, reviews=[])

                if len(reviews) &gt; 0:
                    review_data.extend(reviews)

                    if not incremental:
                        self.logger.info(str.encode(
                            f&#39;Product_name: {product_name} prod_id:{prod_id} reviews extracted successfully.(total_reviews: {no_of_reviews}, \
                            extracted_reviews: {len(reviews)}, page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    else:
                        self.logger.info(str.encode(
                            f&#39;Product_name: {product_name} prod_id:{prod_id} new reviews extracted successfully.\
                                (no_of_new_extracted_reviews: {len(reviews)},\
                                page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            else:
                reviews = 0

            if prod != 0 and prod % 5 == 0:
                detail_data, item_df, review_data = store_data_refresh_mem(
                    detail_data, item_df, review_data)

            self.meta.loc[prod, &#39;scraped&#39;] = &#39;Y&#39;
            drv.quit()

        detail_data, item_df, review_data = store_data_refresh_mem(
            detail_data, item_df, review_data)
        self.logger.info(
            f&#39;Extraction Complete for start_idx: {indices[0]} to end_idx: {indices[-1]}. Or for list of values.&#39;)

    def extract(self, metadata: Union[pd.DataFrame, str, Path], download: bool = True, n_workers: int = 5,
                fresh_start: bool = False, auto_fresh_start: bool = False, incremental: bool = True,
                open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = True,
                start_idx: Optional[int] = None, end_idx: Optional[int] = None, list_of_index=None,
                compile_progress_files: bool = False, clean: bool = True, delete_progress: bool = False):
        &#34;&#34;&#34;extract method controls all properties of the spiders and runs multi-threaded web crawling.

        Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
        This method has four major functionality:
        * 1. Run the spider
        * 2. Store data in regular intervals to free up ram
        * 3. Compile all crawled data into one file.
        * 4. Clean and push cleaned data to S3 storage for further algorithmic processing.

        Args:
            metadata (Union[pd.DataFrame, str, Path]): Dataframe containing product specific url, name and id of the products to be scraped.
            download (bool, optional): Whether to crawl data from or compile crawled data into one file. Defaults to True.
            n_workers (int, optional): No. of parallel threads to run. Defaults to 5.
            fresh_start (bool, optional): Whether to continue last crawl job or start new one. Defaults to False.
            auto_fresh_start (bool, optional): Whether to automatically start a new crawl job if last job was finished. Defaults to False.
            incremental (bool, optional): Whether to scrape reviews incrementally from last scraped review date. Defaults to True.
            open_headless (bool, optional):  Whether to open browser headless. Defaults to False.
            open_with_proxy_server (bool, optional): Whether to use ip rotation service. Defaults to True.
            randomize_proxy_usage (bool, optional): Whether to use both proxy and native network in tandem to decrease proxy requests.
            Defaults to False.
            start_idx (Optional[int], optional): Starting index of the links to crawl. Defaults to None.
            end_idx (Optional[int], optional): Ending index of the links to crawl. Defaults to None.
            list_of_index ([type], optional): List of indices or range of indices of product urls to scrape. Defaults to None.
            compile_progress_files (bool, optional): Whether to combine crawled data into one file. Defaults to False.
            clean (bool, optional): Whether to clean the compiled data. Defaults to True.
            delete_progress (bool, optional): Whether to delete intermediate data after compilation into one file. Defaults to False.
        &#34;&#34;&#34;
        def fresh():
            if not isinstance(metadata, pd.core.frame.DataFrame):
                list_of_files = self.metadata_clean_path.glob(
                    &#39;no_cat_cleaned_bts_product_metadata_all*&#39;)
                self.meta = pd.read_feather(max(list_of_files, key=os.path.getctime))[
                    [&#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;, &#39;last_scraped_review_date&#39;]]
            else:
                self.meta = metadata[[
                    &#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;, &#39;last_scraped_review_date&#39;]]
            self.meta.last_scraped_review_date.fillna(&#39;&#39;, inplace=True)
            self.meta[&#39;scraped&#39;] = &#39;N&#39;

        if download:
            if fresh_start:
                fresh()
                self.logger.info(
                    &#39;Starting Fresh Deatil Review Image Extraction.&#39;)
            else:
                if Path(self.path/&#39;boots/bts_detail_review_image_progress_tracker.csv&#39;).exists():
                    self.meta = pd.read_csv(
                        self.path/&#39;boots/bts_detail_review_image_progress_tracker.csv&#39;)
                    if sum(self.meta.scraped == &#39;N&#39;) == 0:
                        if auto_fresh_start:
                            fresh()
                            self.logger.info(
                                &#39;Last Run was Completed. Starting Fresh Extraction.&#39;)
                        else:
                            self.logger.info(
                                &#39;Deatil Review Image extraction for this cycle is complete.&#39;)
                    else:
                        self.logger.info(
                            &#39;Continuing Deatil Review Image Extraction From Last Run.&#39;)
                else:
                    if auto_fresh_start:
                        fresh()
                        self.logger.info(
                            &#39;Deatil Review Image Progress Tracker does not exist. Starting Fresh Extraction.&#39;)

            # set list or range of product indices to crawl
            if list_of_index:
                indices = list_of_index
            elif start_idx and end_idx is None:
                indices = range(start_idx, len(self.meta))
            elif start_idx is None and end_idx:
                indices = range(0, end_idx)
            elif start_idx is not None and end_idx is not None:
                indices = range(start_idx, end_idx)
            else:
                indices = range(len(self.meta))
            print(indices)

            if list_of_index:
                self.crawl_page(indices=list_of_index, incremental=incremental,
                                open_headless=open_headless,
                                open_with_proxy_server=open_with_proxy_server,
                                randomize_proxy_usage=randomize_proxy_usage)
            else:  # By default the code will with 5 concurrent threads. you can change this behaviour by changing n_workers
                if start_idx:
                    lst_of_lst = ranges(
                        indices[-1]+1, n_workers, start_idx=start_idx)
                else:
                    lst_of_lst = ranges(len(indices), n_workers)
                print(lst_of_lst)
                # detail_Data and item_data are lists of empty lists so that each namepace of function call will have its separate detail_data
                # list to strore scraped dictionaries. will save memory(ram/hard-disk) consumption. will stop data duplication
                headless = [open_headless for i in lst_of_lst]
                proxy = [open_with_proxy_server for i in lst_of_lst]
                rand_proxy = [randomize_proxy_usage for i in lst_of_lst]
                detail_data = [[] for i in lst_of_lst]  # type: List
                # item_data=[[] for i in lst_of_lst]  # type: List
                item_df = [pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                                 &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;])
                           for i in lst_of_lst]
                review_data = [[] for i in lst_of_lst]  # type: list
                inc_list = [incremental for i in lst_of_lst]  # type: list
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    # but each of the function namespace will be modifying only one metadata tracing file so that progress saving
                    # is tracked correctly. else multiple progress tracker file will be created with difficulty to combine correct
                    # progress information
                    print(&#39;inside executor&#39;)
                    executor.map(self.crawl_page, lst_of_lst,
                                 headless, proxy, rand_proxy,
                                 detail_data, item_df, review_data,
                                 inc_list)
        try:
            if compile_progress_files:
                self.logger.info(&#39;Creating Combined Detail and Item File&#39;)
                if datetime.now().day &lt; 15:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
                else:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;

                det_li = []
                self.bad_det_li = []
                detail_files = [f for f in self.detail_current_progress_path.glob(
                    &#34;bts_prod_detail_extract_progress_*&#34;)]
                for file in detail_files:
                    try:
                        df = pd.read_csv(file)
                    except Exception:
                        self.bad_det_li.append(file)
                    else:
                        det_li.append(df)

                detail_df = pd.concat(det_li, axis=0, ignore_index=True)
                detail_df.drop_duplicates(inplace=True)
                detail_df.reset_index(inplace=True, drop=True)
                detail_df[&#39;meta_date&#39;] = meta_date
                detail_filename = f&#39;bts_product_detail_all_{meta_date}.csv&#39;
                detail_df.to_csv(self.detail_path/detail_filename, index=None)
                # detail_df.to_feather(self.detail_path/detail_filename)

                item_li = []
                self.bad_item_li = []
                item_files = [f for f in self.detail_current_progress_path.glob(
                    &#34;bts_prod_item_extract_progress_*&#34;)]
                for file in item_files:
                    try:
                        idf = pd.read_csv(file)
                    except Exception:
                        self.bad_item_li.append(file)
                    else:
                        item_li.append(idf)

                item_dataframe = pd.concat(item_li, axis=0, ignore_index=True)
                item_dataframe.drop_duplicates(inplace=True)
                item_dataframe.reset_index(inplace=True, drop=True)
                item_dataframe[&#39;meta_date&#39;] = meta_date
                item_filename = f&#39;bts_product_item_all_{meta_date}.csv&#39;
                item_dataframe.to_csv(
                    self.detail_path/item_filename, index=None)
                # item_df.to_feather(self.detail_path/item_filename)

                self.logger.info(
                    f&#39;Detail and Item files created. Please look for file bts_product_detail_all and\
                        bts_product_item_all in path {self.detail_path}&#39;)
                print(
                    f&#39;Detail and Item files created. Please look for file bts_product_detail_all and\
                        bts_product_item_all in path {self.detail_path}&#39;)

                self.logger.info(&#39;Creating Combined Review File&#39;)

                rev_li = []
                self.bad_rev_li = []
                review_files = [f for f in self.review_current_progress_path.glob(
                    &#34;bts_prod_review_extract_progress_*&#34;)]
                for file in review_files:
                    try:
                        df = pd.read_csv(file)
                    except Exception:
                        self.bad_rev_li.append(file)
                    else:
                        rev_li.append(df)
                rev_df = pd.concat(rev_li, axis=0, ignore_index=True)
                rev_df.drop_duplicates(inplace=True)
                rev_df.reset_index(inplace=True, drop=True)
                rev_df[&#39;meta_date&#39;] = pd.to_datetime(meta_date).date()
                review_filename = f&#39;bts_product_review_all_{meta_date}&#39;
                # , index=None)
                rev_df.to_feather(self.review_path/review_filename)

                self.logger.info(
                    f&#39;Review file created. Please look for file bts_product_review_all in path {self.review_path}&#39;)
                print(
                    f&#39;Review file created. Please look for file bts_product_review_all in path {self.review_path}&#39;)

                if clean:
                    detail_cleaner = Cleaner(path=self.path)
                    self.detail_clean_df = detail_cleaner.clean(
                        self.detail_path/detail_filename)

                    item_cleaner = Cleaner(path=self.path)
                    self.item_clean_df, self.ing_clean_df = item_cleaner.clean(
                        self.detail_path/item_filename)

                    review_cleaner = Cleaner(path=self.path)
                    self.review_clean_df = review_cleaner.clean(
                        self.review_path/review_filename)

                    file_creation_status = True
            else:
                file_creation_status = False
        except Exception as ex:
            log_exception(
                self.logger, additional_information=f&#39;Detail Item Review Combined File Creation Failed.&#39;)
            file_creation_status = False

        if delete_progress and file_creation_status:
            shutil.rmtree(
                f&#39;{self.detail_path}\\current_progress&#39;, ignore_errors=True)
            shutil.rmtree(
                f&#39;{self.review_path}\\current_progress&#39;, ignore_errors=True)
            self.logger.info(&#39;Progress files deleted&#39;)

    def terminate_logging(self):
        &#34;&#34;&#34;terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.
        &#34;&#34;&#34;
        self.logger.handlers.clear()
        self.prod_detail_review_image_log.stop_log()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="meiyume.utils.Boots" href="../utils.html#meiyume.utils.Boots">Boots</a></li>
<li><a title="meiyume.utils.Browser" href="../utils.html#meiyume.utils.Browser">Browser</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="meiyume.bts.crawler.DetailReview.crawl_page"><code class="name flex">
<span>def <span class="ident">crawl_page</span></span>(<span>self, indices:list, open_headless:bool, open_with_proxy_server:bool, randomize_proxy_usage:bool, detail_data:list=[], item_df=Empty DataFrame
Columns: [prod_id, product_name, item_name, item_size, item_price, item_ingredients]
Index: [], review_data:list=[], incremental:bool=True)</span>
</code></dt>
<dd>
<div class="desc"><p>crawl_page opens each product url in a sepearate browser to scrape detail and review data together.</p>
<p>The scraping happens in a multi-threaded manner to extract product information of up to 20 products simultaneously.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>indices</code></strong> :&ensp;<code>list</code></dt>
<dd>list of indices or range of indices of product urls to scrape.</dd>
<dt><strong><code>open_headless</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to open browser headless.</dd>
<dt><strong><code>open_with_proxy_server</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use ip rotation service.</dd>
<dt><strong><code>randomize_proxy_usage</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use both proxy and native network in tandem to decrease proxy requests.</dd>
<dt><strong><code>detail_data</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Empty intermediate list to store data during parallel crawl. Defaults to [].</dd>
<dt><strong><code>item_df</code></strong> :&ensp;<code>[type]</code>, optional</dt>
<dd>Empty intermediate dataframe to store data during parallel crawl.
Defaults to []. Defaults to pd.DataFrame(columns=['prod_id', 'product_name', 'item_name',
'item_size', 'item_price', 'item_ingredients']).</dd>
<dt><strong><code>review_data</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Empty intermediate list to store data during parallel crawl. Defaults to [].</dd>
<dt><strong><code>incremental</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to scrape reviews incrementally from last scraped review date. Defaults to True.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def crawl_page(self, indices: list, open_headless: bool, open_with_proxy_server: bool,
               randomize_proxy_usage: bool, detail_data: list = [],
               item_df=pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                             &#39;item_size&#39;, &#39;item_price&#39;,
                                             &#39;item_ingredients&#39;]),
               review_data: list = [], incremental: bool = True):
    &#34;&#34;&#34;crawl_page opens each product url in a sepearate browser to scrape detail and review data together.

    The scraping happens in a multi-threaded manner to extract product information of up to 20 products simultaneously.
    Args:
        indices (list): list of indices or range of indices of product urls to scrape.
        open_headless (bool): Whether to open browser headless.
        open_with_proxy_server (bool): Whether to use ip rotation service.
        randomize_proxy_usage (bool): Whether to use both proxy and native network in tandem to decrease proxy requests.
        detail_data (list, optional): Empty intermediate list to store data during parallel crawl. Defaults to [].
        item_df ([type], optional): Empty intermediate dataframe to store data during parallel crawl.
                                    Defaults to []. Defaults to pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                                                                     &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;]).
        review_data (list, optional): Empty intermediate list to store data during parallel crawl. Defaults to [].
        incremental (bool, optional): Whether to scrape reviews incrementally from last scraped review date. Defaults to True.
    &#34;&#34;&#34;

    def store_data_refresh_mem(detail_data: list, item_df: pd.DataFrame,
                               review_data: list) -&gt; Tuple[list, pd.DataFrame, list]:
        &#34;&#34;&#34;store_data_refresh_mem method stores crawled data in regular interval to free up system memory.

        Store data after five products are extracted every time to free up RAM.

        Args:
            detail_data (list): List containing crawled detail data to store.
            item_df (pd.DataFrame): Dataframe containing scraped item data to store.
            review_data (list): List containing scraped Review data to store.

        Returns:
            Tuple[list, pd.DataFrame, list]: Empty list, dataframe and list to accumulate data from next product scraping.
        &#34;&#34;&#34;
        pd.DataFrame(detail_data).to_csv(self.detail_current_progress_path /
                                         f&#39;bts_prod_detail_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                                         index=None)
        item_df.reset_index(inplace=True, drop=True)
        item_df.to_csv(self.detail_current_progress_path /
                       f&#39;bts_prod_item_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                       index=None)
        item_df = pd.DataFrame(columns=[
                               &#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;, &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;])

        pd.DataFrame(review_data).to_csv(self.review_current_progress_path /
                                         f&#39;bts_prod_review_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                                         index=None)
        self.meta.to_csv(
            self.path/&#39;boots/bts_detail_review_image_progress_tracker.csv&#39;, index=None)
        return [], item_df, []

    for prod in self.meta.index[self.meta.index.isin(indices)]:
        #  ignore already extracted products
        if self.meta.loc[prod, &#39;scraped&#39;] in [&#39;Y&#39;, &#39;NA&#39;]:
            continue
        # print(prod, self.meta.loc[prod, &#39;detail_scraped&#39;])
        prod_id = self.meta.loc[prod, &#39;prod_id&#39;]
        product_name = self.meta.loc[prod, &#39;product_name&#39;]
        product_page = self.meta.loc[prod, &#39;product_page&#39;]
        last_scraped_review_date = self.meta.loc[prod,
                                                 &#39;last_scraped_review_date&#39;]
        # create webdriver
        if randomize_proxy_usage:
            use_proxy = np.random.choice([True, False])
        else:
            use_proxy = True
        if open_with_proxy_server:
            # print(use_proxy)
            drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
                                    path=self.detail_path)
        else:
            drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
                                    path=self.detail_path)
        # open product page
        drv.get(product_page)
        time.sleep(15)  # 30
        accept_alert(drv, 5)
        close_popups(drv)

        # check product page is valid and exists
        try:
            close_popups(drv)
            accept_alert(drv, 2)
            price = drv.find_element_by_xpath(
                &#39;//div[@id=&#34;PDP_productPrice&#34;]&#39;).text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            drv.quit()
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) no longer exists in the previously fetched link.\
                    (link:{product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            self.meta.loc[prod, &#39;detail_scraped&#39;] = &#39;NA&#39;
            continue

        # self.scroll_down_page(drv, speed=6, h2=0.6)
        # time.sleep(5)

        detail, item = self.get_details(drv, prod_id, product_name)

        detail_data.append(detail)
        item_df = pd.concat(
            [item_df, item], axis=0)

        # item_data.append(product_attributes)
        self.logger.info(str.encode(
            f&#39;product: {product_name} (prod_id: {prod_id}) details extracted successfully&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        try:
            close_popups(drv)
            accept_alert(drv, 1)
            no_of_reviews = int(drv.find_element_by_css_selector(
                &#39;span[itemprop=&#34;reviewCount&#34;]&#39;).text)
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            self.logger.info(str.encode(f&#39;Product: {product_name} prod_id: {prod_id} reviews extraction failed.\
                                          Either product has no reviews or not\
                                          available for sell currently.(page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            no_of_reviews = 0

        if no_of_reviews &gt; 0:
            reviews = self.get_reviews(
                drv, prod_id, product_name, last_scraped_review_date, no_of_reviews, incremental, reviews=[])

            if len(reviews) &gt; 0:
                review_data.extend(reviews)

                if not incremental:
                    self.logger.info(str.encode(
                        f&#39;Product_name: {product_name} prod_id:{prod_id} reviews extracted successfully.(total_reviews: {no_of_reviews}, \
                        extracted_reviews: {len(reviews)}, page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                else:
                    self.logger.info(str.encode(
                        f&#39;Product_name: {product_name} prod_id:{prod_id} new reviews extracted successfully.\
                            (no_of_new_extracted_reviews: {len(reviews)},\
                            page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
        else:
            reviews = 0

        if prod != 0 and prod % 5 == 0:
            detail_data, item_df, review_data = store_data_refresh_mem(
                detail_data, item_df, review_data)

        self.meta.loc[prod, &#39;scraped&#39;] = &#39;Y&#39;
        drv.quit()

    detail_data, item_df, review_data = store_data_refresh_mem(
        detail_data, item_df, review_data)
    self.logger.info(
        f&#39;Extraction Complete for start_idx: {indices[0]} to end_idx: {indices[-1]}. Or for list of values.&#39;)</code></pre>
</details>
</dd>
<dt id="meiyume.bts.crawler.DetailReview.extract"><code class="name flex">
<span>def <span class="ident">extract</span></span>(<span>self, metadata:Union[pandas.core.frame.DataFrame,str,pathlib.Path], download:bool=True, n_workers:int=5, fresh_start:bool=False, auto_fresh_start:bool=False, incremental:bool=True, open_headless:bool=False, open_with_proxy_server:bool=True, randomize_proxy_usage:bool=True, start_idx:Union[int,NoneType]=None, end_idx:Union[int,NoneType]=None, list_of_index=None, compile_progress_files:bool=False, clean:bool=True, delete_progress:bool=False)</span>
</code></dt>
<dd>
<div class="desc"><p>extract method controls all properties of the spiders and runs multi-threaded web crawling.</p>
<p>Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
This method has four major functionality:
* 1. Run the spider
* 2. Store data in regular intervals to free up ram
* 3. Compile all crawled data into one file.
* 4. Clean and push cleaned data to S3 storage for further algorithmic processing.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>metadata</code></strong> :&ensp;<code>Union[pd.DataFrame, str, Path]</code></dt>
<dd>Dataframe containing product specific url, name and id of the products to be scraped.</dd>
<dt><strong><code>download</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to crawl data from or compile crawled data into one file. Defaults to True.</dd>
<dt><strong><code>n_workers</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>No. of parallel threads to run. Defaults to 5.</dd>
<dt><strong><code>fresh_start</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to continue last crawl job or start new one. Defaults to False.</dd>
<dt><strong><code>auto_fresh_start</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to automatically start a new crawl job if last job was finished. Defaults to False.</dd>
<dt><strong><code>incremental</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to scrape reviews incrementally from last scraped review date. Defaults to True.</dd>
<dt><strong><code>open_headless</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to open browser headless. Defaults to False.</dd>
<dt><strong><code>open_with_proxy_server</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use ip rotation service. Defaults to True.</dd>
<dt><strong><code>randomize_proxy_usage</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use both proxy and native network in tandem to decrease proxy requests.</dd>
<dt>Defaults to False.</dt>
<dt><strong><code>start_idx</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>Starting index of the links to crawl. Defaults to None.</dd>
<dt><strong><code>end_idx</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>Ending index of the links to crawl. Defaults to None.</dd>
<dt><strong><code>list_of_index</code></strong> :&ensp;<code>[type]</code>, optional</dt>
<dd>List of indices or range of indices of product urls to scrape. Defaults to None.</dd>
<dt><strong><code>compile_progress_files</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to combine crawled data into one file. Defaults to False.</dd>
<dt><strong><code>clean</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to clean the compiled data. Defaults to True.</dd>
<dt><strong><code>delete_progress</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to delete intermediate data after compilation into one file. Defaults to False.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract(self, metadata: Union[pd.DataFrame, str, Path], download: bool = True, n_workers: int = 5,
            fresh_start: bool = False, auto_fresh_start: bool = False, incremental: bool = True,
            open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = True,
            start_idx: Optional[int] = None, end_idx: Optional[int] = None, list_of_index=None,
            compile_progress_files: bool = False, clean: bool = True, delete_progress: bool = False):
    &#34;&#34;&#34;extract method controls all properties of the spiders and runs multi-threaded web crawling.

    Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
    This method has four major functionality:
    * 1. Run the spider
    * 2. Store data in regular intervals to free up ram
    * 3. Compile all crawled data into one file.
    * 4. Clean and push cleaned data to S3 storage for further algorithmic processing.

    Args:
        metadata (Union[pd.DataFrame, str, Path]): Dataframe containing product specific url, name and id of the products to be scraped.
        download (bool, optional): Whether to crawl data from or compile crawled data into one file. Defaults to True.
        n_workers (int, optional): No. of parallel threads to run. Defaults to 5.
        fresh_start (bool, optional): Whether to continue last crawl job or start new one. Defaults to False.
        auto_fresh_start (bool, optional): Whether to automatically start a new crawl job if last job was finished. Defaults to False.
        incremental (bool, optional): Whether to scrape reviews incrementally from last scraped review date. Defaults to True.
        open_headless (bool, optional):  Whether to open browser headless. Defaults to False.
        open_with_proxy_server (bool, optional): Whether to use ip rotation service. Defaults to True.
        randomize_proxy_usage (bool, optional): Whether to use both proxy and native network in tandem to decrease proxy requests.
        Defaults to False.
        start_idx (Optional[int], optional): Starting index of the links to crawl. Defaults to None.
        end_idx (Optional[int], optional): Ending index of the links to crawl. Defaults to None.
        list_of_index ([type], optional): List of indices or range of indices of product urls to scrape. Defaults to None.
        compile_progress_files (bool, optional): Whether to combine crawled data into one file. Defaults to False.
        clean (bool, optional): Whether to clean the compiled data. Defaults to True.
        delete_progress (bool, optional): Whether to delete intermediate data after compilation into one file. Defaults to False.
    &#34;&#34;&#34;
    def fresh():
        if not isinstance(metadata, pd.core.frame.DataFrame):
            list_of_files = self.metadata_clean_path.glob(
                &#39;no_cat_cleaned_bts_product_metadata_all*&#39;)
            self.meta = pd.read_feather(max(list_of_files, key=os.path.getctime))[
                [&#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;, &#39;last_scraped_review_date&#39;]]
        else:
            self.meta = metadata[[
                &#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;, &#39;last_scraped_review_date&#39;]]
        self.meta.last_scraped_review_date.fillna(&#39;&#39;, inplace=True)
        self.meta[&#39;scraped&#39;] = &#39;N&#39;

    if download:
        if fresh_start:
            fresh()
            self.logger.info(
                &#39;Starting Fresh Deatil Review Image Extraction.&#39;)
        else:
            if Path(self.path/&#39;boots/bts_detail_review_image_progress_tracker.csv&#39;).exists():
                self.meta = pd.read_csv(
                    self.path/&#39;boots/bts_detail_review_image_progress_tracker.csv&#39;)
                if sum(self.meta.scraped == &#39;N&#39;) == 0:
                    if auto_fresh_start:
                        fresh()
                        self.logger.info(
                            &#39;Last Run was Completed. Starting Fresh Extraction.&#39;)
                    else:
                        self.logger.info(
                            &#39;Deatil Review Image extraction for this cycle is complete.&#39;)
                else:
                    self.logger.info(
                        &#39;Continuing Deatil Review Image Extraction From Last Run.&#39;)
            else:
                if auto_fresh_start:
                    fresh()
                    self.logger.info(
                        &#39;Deatil Review Image Progress Tracker does not exist. Starting Fresh Extraction.&#39;)

        # set list or range of product indices to crawl
        if list_of_index:
            indices = list_of_index
        elif start_idx and end_idx is None:
            indices = range(start_idx, len(self.meta))
        elif start_idx is None and end_idx:
            indices = range(0, end_idx)
        elif start_idx is not None and end_idx is not None:
            indices = range(start_idx, end_idx)
        else:
            indices = range(len(self.meta))
        print(indices)

        if list_of_index:
            self.crawl_page(indices=list_of_index, incremental=incremental,
                            open_headless=open_headless,
                            open_with_proxy_server=open_with_proxy_server,
                            randomize_proxy_usage=randomize_proxy_usage)
        else:  # By default the code will with 5 concurrent threads. you can change this behaviour by changing n_workers
            if start_idx:
                lst_of_lst = ranges(
                    indices[-1]+1, n_workers, start_idx=start_idx)
            else:
                lst_of_lst = ranges(len(indices), n_workers)
            print(lst_of_lst)
            # detail_Data and item_data are lists of empty lists so that each namepace of function call will have its separate detail_data
            # list to strore scraped dictionaries. will save memory(ram/hard-disk) consumption. will stop data duplication
            headless = [open_headless for i in lst_of_lst]
            proxy = [open_with_proxy_server for i in lst_of_lst]
            rand_proxy = [randomize_proxy_usage for i in lst_of_lst]
            detail_data = [[] for i in lst_of_lst]  # type: List
            # item_data=[[] for i in lst_of_lst]  # type: List
            item_df = [pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                             &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;])
                       for i in lst_of_lst]
            review_data = [[] for i in lst_of_lst]  # type: list
            inc_list = [incremental for i in lst_of_lst]  # type: list
            with concurrent.futures.ThreadPoolExecutor() as executor:
                # but each of the function namespace will be modifying only one metadata tracing file so that progress saving
                # is tracked correctly. else multiple progress tracker file will be created with difficulty to combine correct
                # progress information
                print(&#39;inside executor&#39;)
                executor.map(self.crawl_page, lst_of_lst,
                             headless, proxy, rand_proxy,
                             detail_data, item_df, review_data,
                             inc_list)
    try:
        if compile_progress_files:
            self.logger.info(&#39;Creating Combined Detail and Item File&#39;)
            if datetime.now().day &lt; 15:
                meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
            else:
                meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;

            det_li = []
            self.bad_det_li = []
            detail_files = [f for f in self.detail_current_progress_path.glob(
                &#34;bts_prod_detail_extract_progress_*&#34;)]
            for file in detail_files:
                try:
                    df = pd.read_csv(file)
                except Exception:
                    self.bad_det_li.append(file)
                else:
                    det_li.append(df)

            detail_df = pd.concat(det_li, axis=0, ignore_index=True)
            detail_df.drop_duplicates(inplace=True)
            detail_df.reset_index(inplace=True, drop=True)
            detail_df[&#39;meta_date&#39;] = meta_date
            detail_filename = f&#39;bts_product_detail_all_{meta_date}.csv&#39;
            detail_df.to_csv(self.detail_path/detail_filename, index=None)
            # detail_df.to_feather(self.detail_path/detail_filename)

            item_li = []
            self.bad_item_li = []
            item_files = [f for f in self.detail_current_progress_path.glob(
                &#34;bts_prod_item_extract_progress_*&#34;)]
            for file in item_files:
                try:
                    idf = pd.read_csv(file)
                except Exception:
                    self.bad_item_li.append(file)
                else:
                    item_li.append(idf)

            item_dataframe = pd.concat(item_li, axis=0, ignore_index=True)
            item_dataframe.drop_duplicates(inplace=True)
            item_dataframe.reset_index(inplace=True, drop=True)
            item_dataframe[&#39;meta_date&#39;] = meta_date
            item_filename = f&#39;bts_product_item_all_{meta_date}.csv&#39;
            item_dataframe.to_csv(
                self.detail_path/item_filename, index=None)
            # item_df.to_feather(self.detail_path/item_filename)

            self.logger.info(
                f&#39;Detail and Item files created. Please look for file bts_product_detail_all and\
                    bts_product_item_all in path {self.detail_path}&#39;)
            print(
                f&#39;Detail and Item files created. Please look for file bts_product_detail_all and\
                    bts_product_item_all in path {self.detail_path}&#39;)

            self.logger.info(&#39;Creating Combined Review File&#39;)

            rev_li = []
            self.bad_rev_li = []
            review_files = [f for f in self.review_current_progress_path.glob(
                &#34;bts_prod_review_extract_progress_*&#34;)]
            for file in review_files:
                try:
                    df = pd.read_csv(file)
                except Exception:
                    self.bad_rev_li.append(file)
                else:
                    rev_li.append(df)
            rev_df = pd.concat(rev_li, axis=0, ignore_index=True)
            rev_df.drop_duplicates(inplace=True)
            rev_df.reset_index(inplace=True, drop=True)
            rev_df[&#39;meta_date&#39;] = pd.to_datetime(meta_date).date()
            review_filename = f&#39;bts_product_review_all_{meta_date}&#39;
            # , index=None)
            rev_df.to_feather(self.review_path/review_filename)

            self.logger.info(
                f&#39;Review file created. Please look for file bts_product_review_all in path {self.review_path}&#39;)
            print(
                f&#39;Review file created. Please look for file bts_product_review_all in path {self.review_path}&#39;)

            if clean:
                detail_cleaner = Cleaner(path=self.path)
                self.detail_clean_df = detail_cleaner.clean(
                    self.detail_path/detail_filename)

                item_cleaner = Cleaner(path=self.path)
                self.item_clean_df, self.ing_clean_df = item_cleaner.clean(
                    self.detail_path/item_filename)

                review_cleaner = Cleaner(path=self.path)
                self.review_clean_df = review_cleaner.clean(
                    self.review_path/review_filename)

                file_creation_status = True
        else:
            file_creation_status = False
    except Exception as ex:
        log_exception(
            self.logger, additional_information=f&#39;Detail Item Review Combined File Creation Failed.&#39;)
        file_creation_status = False

    if delete_progress and file_creation_status:
        shutil.rmtree(
            f&#39;{self.detail_path}\\current_progress&#39;, ignore_errors=True)
        shutil.rmtree(
            f&#39;{self.review_path}\\current_progress&#39;, ignore_errors=True)
        self.logger.info(&#39;Progress files deleted&#39;)</code></pre>
</details>
</dd>
<dt id="meiyume.bts.crawler.DetailReview.get_details"><code class="name flex">
<span>def <span class="ident">get_details</span></span>(<span>self, drv:selenium.webdriver.firefox.webdriver.WebDriver, prod_id:str, product_name:str) >Tuple[dict,pandas.core.frame.DataFrame]</span>
</code></dt>
<dd>
<div class="desc"><p>get_detail scrapes individual product pages for price, ingredients, color etc.</p>
<p>Get Detail crawls product specific page and scrapes data such as ingredients, review rating distribution,
size specific prices, color, product claims and other information pertaining to one individual product.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>drv</code></strong> :&ensp;<code>webdriver.Firefox</code></dt>
<dd>Selenium webdriver with opened product page.</dd>
<dt><strong><code>prod_id</code></strong> :&ensp;<code>str</code></dt>
<dd>Id of the product from metdata.</dd>
<dt><strong><code>product_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the product from metadata.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[dict, pd.DataFrame]</code></dt>
<dd>Dictionary containing details of a product and dataframe containing item
attributes of a product.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_details(self, drv: webdriver.Firefox, prod_id: str, product_name: str) -&gt; Tuple[dict, pd.DataFrame]:
    &#34;&#34;&#34;get_detail scrapes individual product pages for price, ingredients, color etc.

    Get Detail crawls product specific page and scrapes data such as ingredients, review rating distribution,
    size specific prices, color, product claims and other information pertaining to one individual product.

    Args:
        drv (webdriver.Firefox): Selenium webdriver with opened product page.
        prod_id (str): Id of the product from metdata.
        product_name (str): Name of the product from metadata.

    Returns:
        Tuple[dict, pd.DataFrame]: Dictionary containing details of a product and dataframe containing item
                                   attributes of a product.
    &#34;&#34;&#34;
    def get_product_attributes(drv: webdriver.Firefox, prod_id: str, product_name: str):
        &#34;&#34;&#34;get_product_attributes uses get_item_attribute method to scrape item details and stores
        in a list which is returned to get_detail method for storing in a product specific dataframe.

        Args:
            drv (webdriver.Firefox): Selenium webdriver with opened product page.
            prod_id (str): Id of the product from metdata.
            product_name (str): Name of the product from metadata.

        Returns:
            list: List containing all product item attributes of multiple varieties with name and id.
        &#34;&#34;&#34;
        # get all the variation of product
        product_attributes = []

        # product_variety = drv.find_elements_by_css_selector(
        #     &#39;li[id*=&#34;size_combo_button_pdp&#34;]&#39;)

        try:
            item_ingredients = drv.find_element_by_xpath(
                &#39;//div[h3[@id=&#34;product_ingredients&#34;]]&#39;).text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            item_ingredients = &#39;&#39;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        try:
            item_price = drv.find_element_by_xpath(
                &#39;//div[@id=&#34;PDP_productPrice&#34;]&#39;).text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            item_price = &#39;&#39;

        try:
            item_size_price = drv.find_element_by_css_selector(
                &#39;div.details&#39;).text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            item_size_price = &#39;&#39;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) item_size does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        item_size_price = item_price + &#34; per &#34; + item_size_price

        if len(item_size_price.split(&#39;|&#39;)) &gt; 0:

            for i in item_size_price.split(&#39;|&#39;):
                ps = i.split(&#39;per&#39;)
                item_price = ps[0].strip()
                item_size = ps[1].strip()
                product_attributes.append(
                    {&#34;prod_id&#34;: prod_id, &#34;product_name&#34;: product_name,
                     &#34;item_name&#34;: &#39;&#39;, &#34;item_size&#34;: item_size,
                     &#34;item_price&#34;: item_price, &#34;item_ingredients&#34;: item_ingredients
                     }
                )

        else:
            product_attributes.append(
                {&#34;prod_id&#34;: prod_id, &#34;product_name&#34;: product_name,
                 &#34;item_name&#34;: &#39;&#39;, &#34;item_size&#34;: item_size_price,
                 &#34;item_price&#34;: item_price, &#34;item_ingredients&#34;: item_ingredients
                 }
            )

        return product_attributes

    try:
        abt_product = drv.find_element_by_css_selector(
            &#39;div[id=&#34;contentOmnipresent&#34;]&#39;).text
    except Exception as ex:
        log_exception(self.logger,
                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
        abt_product = &#39;&#39;
        self.logger.info(str.encode(
            f&#39;product: {product_name} (prod_id: {prod_id}) product detail does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

    try:
        how_to_use = drv.find_element_by_xpath(
            &#39;//div[h3[@id=&#34;product_how_to_use&#34;]]&#39;).text
    except Exception as ex:
        log_exception(self.logger,
                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
        how_to_use = &#39;&#39;
        self.logger.info(str.encode(
            f&#39;product: {product_name} (prod_id: {prod_id}) how_to_use does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

    try:
        reviews = drv.find_element_by_css_selector(
            &#39;span[itemprop=&#34;reviewCount&#34;]&#39;).text
    except Exception as ex:
        log_exception(self.logger,
                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
        reviews = 0
        self.logger.info(str.encode(
            f&#39;product: {product_name} (prod_id: {prod_id}) reviews does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

    try:
        ratings = drv.find_elements_by_css_selector(
            &#39;div.bv-inline-histogram-ratings-score&gt;span:nth-child(1)&#39;)
        five_star = ratings[0].text
        four_star = ratings[1].text
        three_star = ratings[2].text
        two_star = ratings[3].text
        one_star = ratings[4].text

    except Exception as ex:
        log_exception(self.logger,
                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
        five_star = &#39;&#39;
        four_star = &#39;&#39;
        three_star = &#39;&#39;
        two_star = &#39;&#39;
        one_star = &#39;&#39;
        self.logger.info(str.encode(
            f&#39;product: {product_name} (prod_id: {prod_id}) rating_distribution does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

    detail = {&#34;prod_id&#34;: prod_id,
              &#34;product_name&#34;: product_name,
              &#34;abt_product&#34;: abt_product,
              &#34;abt_brand&#34;: &#39;&#39;,
              &#34;how_to_use&#34;: how_to_use,
              &#34;reviews&#34;: reviews,
              &#34;votes&#34;: &#39;&#39;,
              &#34;five_star&#34;: five_star,
              &#34;four_star&#34;: four_star,
              &#34;three_star&#34;: three_star,
              &#34;two_star&#34;: two_star,
              &#34;one_star&#34;: one_star,
              &#34;would_recommend&#34;: &#39;&#39;,
              &#34;first_review_date&#34;: &#39;&#39;
              }

    item = pd.DataFrame(
        get_product_attributes(drv, prod_id, product_name))

    return detail, item</code></pre>
</details>
</dd>
<dt id="meiyume.bts.crawler.DetailReview.get_reviews"><code class="name flex">
<span>def <span class="ident">get_reviews</span></span>(<span>self, drv:selenium.webdriver.firefox.webdriver.WebDriver, prod_id:str, product_name:str, last_scraped_review_date:str, no_of_reviews:int, incremental:bool=True, reviews:list=[]) >list</span>
</code></dt>
<dd>
<div class="desc"><p>get_reviews Crawls individual product pages for review text, title, date user attributes etc.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>drv</code></strong> :&ensp;<code>webdriver.Firefox</code></dt>
<dd>drv (webdriver.Firefox): Selenium webdriver with opened product page.</dd>
<dt><strong><code>prod_id</code></strong> :&ensp;<code>str</code></dt>
<dd>Id of the product from metdata.</dd>
<dt><strong><code>product_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the product from metadata.</dd>
<dt><strong><code>last_scraped_review_date</code></strong> :&ensp;<code>str</code></dt>
<dd>The last review date scraped as per database.</dd>
<dt><strong><code>no_of_reviews</code></strong> :&ensp;<code>int</code></dt>
<dd>No.of reviews a product has.</dd>
<dt><strong><code>incremental</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to scrape reviews incrementally from last scraped review date. Defaults to True.</dd>
<dt><strong><code>reviews</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Empty intermediate list to store data during parallel crawl. Defaults to []</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>List od dictionaries containing all the scraped reviews of a product.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_reviews(self,  drv: webdriver.Firefox, prod_id: str, product_name: str,
                last_scraped_review_date: str, no_of_reviews: int,
                incremental: bool = True, reviews: list = []) -&gt; list:
    &#34;&#34;&#34;get_reviews Crawls individual product pages for review text, title, date user attributes etc.

    Args:
        drv (webdriver.Firefox): drv (webdriver.Firefox): Selenium webdriver with opened product page.
        prod_id (str): Id of the product from metdata.
        product_name (str): Name of the product from metadata.
        last_scraped_review_date (str): The last review date scraped as per database.
        no_of_reviews (int): No.of reviews a product has.
        incremental (bool, optional): Whether to scrape reviews incrementally from last scraped review date. Defaults to True.
        reviews (list, optional): Empty intermediate list to store data during parallel crawl. Defaults to []

    Returns:
        list: List od dictionaries containing all the scraped reviews of a product.
    &#34;&#34;&#34;
    # print(no_of_reviews)
    # drv.find_element_by_class_name(&#39;css-2rg6q7&#39;).click()
    if incremental and last_scraped_review_date != &#39;&#39;:
        for i in range(no_of_reviews//30):
            if i &gt;= 100:
                break
            time.sleep(0.4)
            revs = drv.find_elements_by_css_selector(
                &#39;ol[class=&#34;bv-content-list bv-content-list-reviews&#34;]&gt;li&#39;)
            date1 = convert_ago_to_date(revs[-1].find_element_by_css_selector(&#39;div.bv-content-datetime&gt;meta[itemprop=&#34;dateCreated&#34;]&#39;
                                                                              ).get_attribute(&#39;content&#39;))
            date2 = convert_ago_to_date(revs[-2].find_element_by_css_selector(&#39;div.bv-content-datetime&gt;meta[itemprop=&#34;dateCreated&#34;]&#39;
                                                                              ).get_attribute(&#39;content&#39;))
            try:
                if pd.to_datetime(date1, infer_datetime_format=True)\
                        &lt; pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                    # print(&#39;breaking incremental&#39;)
                    break
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                try:
                    if pd.to_datetime(date2, infer_datetime_format=True)\
                            &lt; pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                        # print(&#39;breaking incremental&#39;)
                        break
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    self.logger.info(str.encode(f&#39;Product: {product_name} prod_id: {prod_id} \
                                                    last_scraped_review_date to current review date \
                                                    comparision failed.(page: {product_page})&#39;,
                                                &#39;utf-8&#39;, &#39;ignore&#39;))
                    # print(&#39;in second except block&#39;)
                    continue

            if len(revs) &gt;= 2000:
                break
            try:
                accept_alert(drv, 1)
                close_popups(drv)
                show_more_review_button = drv.find_element_by_css_selector(
                    &#39;span[class=&#34;bv-content-btn-pages-load-more-text&#34;]&#39;)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Loaded all the reviews. No more reviews exist.&#39;)
                show_more_review_button = &#39;&#39;
                break
            if show_more_review_button != &#39;&#39;:
                self.scroll_to_element(
                    drv, show_more_review_button)
                ActionChains(drv).move_to_element(
                    show_more_review_button).click(show_more_review_button).perform()
                # print(&#39;loading more reviews&#39;)
            time.sleep(0.4)
    else:
        for n in range(no_of_reviews//30+5):
            &#39;&#39;&#39;
            code will stop after getting 1800 reviews of one particular product
            when crawling all reviews. By default it will get latest 1800 reviews.
            then in subsequent incremental runs it will get al new reviews on weekly basis
            &#39;&#39;&#39;
            if n &gt;= 100:  # 200:
                break
            time.sleep(1)

            # close any opened popups by escape
            accept_alert(drv, 1)
            close_popups(drv)
            try:
                show_more_review_button = drv.find_element_by_css_selector(
                    &#39;span[class=&#34;bv-content-btn-pages-load-more-text&#34;]&#39;)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Loaded all the reviews. No more reviews exist.&#39;)
                show_more_review_button = &#39;&#39;
                break
            if show_more_review_button != &#39;&#39;:
                self.scroll_to_element(
                    drv, show_more_review_button)
                ActionChains(drv).move_to_element(
                    show_more_review_button).click(show_more_review_button).perform()
                # print(&#39;loading more reviews&#39;)

    accept_alert(drv, 2)
    close_popups(drv)

    product_reviews = drv.find_elements_by_css_selector(
        &#39;ol[class=&#34;bv-content-list bv-content-list-reviews&#34;]&gt;li&#39;)

    r = 0
    for rev in product_reviews:
        accept_alert(drv, 0.5)
        close_popups(drv)
        try:
            self.scroll_to_element(drv, rev)
            ActionChains(drv).move_to_element(rev).perform()
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}. Failed to scroll to review.&#39;)
            pass

        try:
            review_text = rev.find_element_by_css_selector(
                &#39;div.bv-content-summary-body-text&#39;).text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_text. Skip review.&#39;)
            continue

        try:
            review_date = convert_ago_to_date(
                rev.find_element_by_css_selector(&#39;div.bv-content-datetime&gt;meta[itemprop=&#34;dateCreated&#34;]&#39;).get_attribute(&#39;content&#39;))
            if pd.to_datetime(review_date, infer_datetime_format=True) &lt;= \
                    pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                continue
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_date.&#39;)
            review_date = &#39;&#39;

        try:
            review_title = rev.find_element_by_css_selector(
                &#39;h3.bv-content-title&#39;).text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_title.&#39;)
            review_title = &#39;&#39;

        try:
            product_variant = rev.find_element_by_class_name(
                &#39;css-1op1cn7&#39;).text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}. Failed to extract product_variant.&#39;)
            product_variant = &#39;&#39;

        try:
            user_rating = rev.find_element_by_css_selector(
                &#39;span.bv-rating-stars-container&gt;span.bv-off-screen&#39;).text.split(&#34; &#34;)[0]
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}. Failed to extract user_rating.&#39;)
            user_rating = &#39;&#39;

        try:
            helpful_yes = rev.find_element_by_css_selector(
                &#39;div.bv-content-feedback-btn-container&gt;button:nth-child(1) span.bv-content-btn-count&#39;).text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}. Helpful Yes not found.&#39;)
            helpful_yes = &#39;&#39;

        try:
            helpful_no = rev.find_element_by_css_selector(
                &#39;div.bv-content-feedback-btn-container&gt;button:nth-child(2) span.bv-content-btn-count&#39;).text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}. Helpful No not found.&#39;)
            helpful_no = &#39;&#39;

        try:
            recommend = rev.find_element_by_css_selector(
                &#39;dl[class*=&#34;recommend&#34;] span.bv-content-data-label&#39;).text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}. Failed to extract recommend.&#39;)
            recommend = &#39;&#39;

        reviews.append(
            {
                &#39;prod_id&#39;: prod_id, &#39;product_name&#39;: product_name,
                &#39;user_attribute&#39;: &#39;&#39;, &#39;product_variant&#39;: product_variant,
                &#39;review_title&#39;: review_title, &#39;review_text&#39;: review_text,
                &#39;review_rating&#39;: user_rating, &#39;recommend&#39;: recommend,
                &#39;review_date&#39;: review_date, &#34;helpful_y&#34;: helpful_yes,
                &#34;helpful_n&#34;: helpful_no
            }
        )
    return reviews</code></pre>
</details>
</dd>
<dt id="meiyume.bts.crawler.DetailReview.terminate_logging"><code class="name flex">
<span>def <span class="ident">terminate_logging</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def terminate_logging(self):
    &#34;&#34;&#34;terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.
    &#34;&#34;&#34;
    self.logger.handlers.clear()
    self.prod_detail_review_image_log.stop_log()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="meiyume.utils.Boots" href="../utils.html#meiyume.utils.Boots">Boots</a></b></code>:
<ul class="hlist">
<li><code><a title="meiyume.utils.Boots.open_browser" href="../utils.html#meiyume.utils.Browser.open_browser">open_browser</a></code></li>
<li><code><a title="meiyume.utils.Boots.open_browser_firefox" href="../utils.html#meiyume.utils.Browser.open_browser_firefox">open_browser_firefox</a></code></li>
<li><code><a title="meiyume.utils.Boots.scroll_down_page" href="../utils.html#meiyume.utils.Browser.scroll_down_page">scroll_down_page</a></code></li>
<li><code><a title="meiyume.utils.Boots.scroll_to_element" href="../utils.html#meiyume.utils.Browser.scroll_to_element">scroll_to_element</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="meiyume.bts.crawler.Metadata"><code class="flex name class">
<span>class <span class="ident">Metadata</span></span>
<span>(</span><span>log:bool=True, path:pathlib.Path=WindowsPath('D:/Amit/Meiyume/meiyume_master_source_codes'))</span>
</code></dt>
<dd>
<div class="desc"><p>Metadata extracts product metadata such as product page url, prices and brand from Boots website.</p>
<p>The Metadata class begins the data crawling process and all other stages depend on
the product urls extracted by Metadata class.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>Boots</code></strong> :&ensp;<code>Browser</code></dt>
<dd>Class that initializes folder paths and selenium webdriver for data scraping.</dd>
</dl>
<p><strong>init</strong> Metadata class instace initializer.</p>
<p>This method sets all the folder paths required for Metadata crawler to work.
If the paths does not exist the paths get automatically created depending on current directory or
provided directory.</p>
<h2 id="args_1">Args</h2>
<dl>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to create crawling exception and progess log. Defaults to True.</dd>
<dt><strong><code>path</code></strong> :&ensp;<code>Path</code>, optional</dt>
<dd>Folder path where the Metadata will be extracted. Defaults to
current directory(Path.cwd()).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Metadata(Boots):
    &#34;&#34;&#34;Metadata extracts product metadata such as product page url, prices and brand from Boots website.

    The Metadata class begins the data crawling process and all other stages depend on
    the product urls extracted by Metadata class.

    Args:
        Boots (Browser): Class that initializes folder paths and selenium webdriver for data scraping.

    &#34;&#34;&#34;
    base_url = &#34;https://www.boots.com&#34;
    info = tldextract.extract(base_url)
    source = info.registered_domain

    @classmethod
    def update_base_url(cls, url: str) -&gt; None:
        &#34;&#34;&#34;update_base_url defines the parent url from where the data scraping process will begin.

        Args:
            url (str): The URL from which the spider will enter the website.

        &#34;&#34;&#34;
        cls.base_url = url
        cls.info = tldextract.extract(cls.base_url)
        cls.source = cls.info.registered_domain

    def __init__(self, log: bool = True, path: Path = Path.cwd()):
        &#34;&#34;&#34;__init__ Metadata class instace initializer.

        This method sets all the folder paths required for Metadata crawler to work.
        If the paths does not exist the paths get automatically created depending on current directory or
        provided directory.

        Args:
            log (bool, optional): Whether to create crawling exception and progess log. Defaults to True.
            path (Path, optional): Folder path where the Metadata will be extracted. Defaults to
                                   current directory(Path.cwd()).

        &#34;&#34;&#34;
        super().__init__(path=path, data_def=&#39;meta&#39;)
        self.path = path
        self.current_progress_path = self.metadata_path/&#39;current_progress&#39;
        self.current_progress_path.mkdir(parents=True, exist_ok=True)

        # move old raw and clean files to old folder
        old_metadata_files = list(self.metadata_path.glob(
            &#39;bts_product_metadata_all*&#39;))
        for f in old_metadata_files:
            shutil.move(str(f), str(self.old_metadata_files_path))

        old_clean_metadata_files = os.listdir(self.metadata_clean_path)
        for f in old_clean_metadata_files:
            shutil.move(str(self.metadata_clean_path/f),
                        str(self.old_metadata_clean_files_path))
        # set logger
        if log:
            self.prod_meta_log = Logger(
                &#34;bts_prod_metadata_extraction&#34;, path=self.crawl_log_path)
            self.logger, _ = self.prod_meta_log.start_log()

    def get_product_type_urls(self, open_headless: bool, open_with_proxy_server: bool) -&gt; pd.DataFrame:
        &#34;&#34;&#34;get_product_type_urls Extract the category/subcategory structure and urls to extract the products of those category/subcategory.

        Extracts the links of pages containing the list of all products structured into
        category/subcategory/product type to effectively stored in relational database.
        Defines the structure of data extraction that helps store unstructured data in a structured manner.

        Args:
            open_headless (bool): Whether to open browser headless.
            open_with_proxy_server (bool): Whether to use proxy server.

        Returns:
            pd.DataFrame: returns pandas dataframe containing urls for getting list of products, category, subcategory etc.
        &#34;&#34;&#34;
        # create webdriver instance
        drv = self.open_browser(
            open_headless=open_headless, open_with_proxy_server=open_with_proxy_server, path=self.metadata_path)

        drv.get(self.base_url)
        time.sleep(15)
        # click and close welcome forms
        accept_alert(drv, 10)
        close_popups(drv)

        try:
            country_popup = WebDriverWait(drv, 3).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, &#39;.estores_overlay_content &gt; a:nth-child(1)&#39;)))
            pop_close_button = drv.find_element_by_css_selector(
                &#39;.estores_overlay_content &gt; a:nth-child(1)&#39;)
            Browser().scroll_to_element(drv, pop_close_button)
            ActionChains(drv).move_to_element(
                pop_close_button).click(pop_close_button).perform()
        except Exception as ex:
            pass

        allowed_categories = [&#39;beauty&#39;, &#39;fragrance&#39;,
                              &#39;toiletries&#39;, &#39;mens&#39;, &#39;wellness&#39;]
        exclude_categories = [&#39;health-pharmacy&#39;, &#39;advice&#39;, &#39;luxury-beauty-premium-beauty-book-an-appointment&#39;,
                              &#39;health-value-packs-and-bundles&#39;, &#39;all-luxury-skincare&#39;, &#39;wellness-supplements&#39;, &#39;travel-essentials&#39;,
                              &#39;opticians&#39;, &#39;feminine-hygiene&#39;, &#39;travel-health&#39;, &#39;sunglasses&#39;, &#39;inspiration&#39;, &#39;food-and-drink&#39;,
                              &#39;nutrition&#39;, &#39;vitaminsandsupplements&#39;, &#39;condoms-sexual-health&#39;, &#39;mens-health-information&#39;,
                              &#39;weightloss&#39;, &#39;menshealth&#39;, &#39;recommended&#39;, &#39;offers&#39;, &#39;all-&#39;, &#39;male-incontinence&#39;, &#39;sustainable-living&#39;,
                              &#39;bootsdental&#39;, &#39;back-to-school-and-nursery&#39;, &#39;luxury-beauty-skincare&#39;, &#39;mens-gift-sets&#39;,
                              &#39;all-face&#39;, &#39;all-eyes&#39;, &#39;all-lips&#39;, &#39;gift/him/mens-aftershave&#39;, &#39;gift/her/luxury-beauty-gift&#39;,
                              &#39;christmas/christmas-3-for-2&#39;, &#39;christmas/gifts-for-her&#39;, &#39;christmas/gifts-for-him&#39;,
                              &#39;christmas/advent-calendars&#39;, &#39;beauty-expert-skincare-expert-skincare-shop-all&#39;,
                              &#39;black-afro-and-textured-hair-straight-hair&#39;, &#39;black-afro-and-textured-hair-wavy&#39;,
                              &#39;black-afro-and-textured-hair-straight-hair&#39;,
                              ]
        # Extracting the category-sub category structure
        cat_urls = []
        for i in drv.find_elements_by_css_selector(&#39;a[id*=&#34;subcategoryLink&#34;]&#39;):
            cat_urls.append(i.get_attribute(&#39;href&#39;))

        cat_urls = [u for u in cat_urls if any(
            cat in str(u) for cat in allowed_categories)]
        cat_urls = [u for u in cat_urls if all(
            cat not in str(u) for cat in exclude_categories)]

        prod_type_urls = []

        for url in cat_urls:
            drv.get(url)

            time.sleep(6)
            accept_alert(drv, 10)
            close_popups(drv)

            subcats = drv.find_elements_by_css_selector(&#39;div.category-link&gt;a&#39;)
            if len(subcats) &gt; 0:
                for i in drv.find_elements_by_css_selector(&#39;div.category-link&gt;a&#39;):
                    prod_type_urls.append(i.get_attribute(&#39;href&#39;))
            else:
                prod_type_urls.append(url)

        prod_type_urls = [u for u in prod_type_urls if any(
            cat in str(u) for cat in allowed_categories)]
        prod_type_urls = [u for u in prod_type_urls if all(
            cat not in str(u) for cat in exclude_categories)]

        drv.quit()

        df = pd.DataFrame(prod_type_urls, columns=[&#39;url&#39;])
        df[&#39;dept&#39;], df[&#39;category_raw&#39;], df[&#39;subcategory_raw&#39;], df[&#39;product_type&#39;] = zip(
            *df.url.str.split(&#39;/&#39;, expand=True).loc[:, 3:].values)

        def set_cat_subcat_ptype(x):
            if x.product_type is None:
                return x.dept, x.category_raw, x.subcategory_raw
            else:
                return x.category_raw, x.subcategory_raw, x.product_type

        df.category_raw, df.subcategory_raw, df.product_type = zip(
            *df.apply(set_cat_subcat_ptype, axis=1))
        df.drop(columns=[&#39;dept&#39;], inplace=True)
        df.drop_duplicates(subset=&#39;url&#39;, inplace=True)
        df.reset_index(inplace=True, drop=True)
        df[&#39;scraped&#39;] = &#39;N&#39;
        df.to_feather(self.metadata_path/f&#39;bts_product_type_urls_to_extract&#39;)
        return df

    def get_metadata(self, indices: Union[list, range],
                     open_headless: bool, open_with_proxy_server: bool,
                     randomize_proxy_usage: bool,
                     product_meta_data: list = []):
        &#34;&#34;&#34;get_metadata Crawls product listing pages for price, name, brand etc.

        Get Metadata crawls a product type page for example lipstick.
        The function gets individual product urls, names, brands and prices etc. and stores
        in a relational table structure to use later to download product images, scrape reviews and
        other specific information.

        Args:
            indices (Union[list, range]): list of indices or range of indices of product urls to scrape.
            open_headless (bool): Whether to open browser headless.
            open_with_proxy_server (bool): Whether to use proxy server.
            randomize_proxy_usage (bool): Whether to use both proxy and native network in tandem to decrease proxy requests.
            product_meta_data (list, optional): Empty intermediate list to store product metadata during parallel crawl. Defaults to [].
        &#34;&#34;&#34;
        for pt in self.product_type_urls.index[self.product_type_urls.index.isin(indices)]:
            cat_name = self.product_type_urls.loc[pt, &#39;category_raw&#39;]
            product_type = self.product_type_urls.loc[pt, &#39;product_type&#39;]
            product_type_link = self.product_type_urls.loc[pt, &#39;url&#39;]

            self.progress_tracker.loc[pt, &#39;product_type&#39;] = product_type
            # print(self.progress_tracker.loc[pt, &#39;product_type&#39;])
            # print(product_type_link)
            if &#39;best-selling&#39; in product_type.lower() or &#39;new&#39; in product_type.lower():
                self.progress_tracker.loc[pt, &#39;scraped&#39;] = &#39;NA&#39;
                # print(self.progress_tracker.loc[pt, &#39;scraped&#39;])
                continue

            if randomize_proxy_usage:
                use_proxy = np.random.choice([True, False])
            else:
                use_proxy = True
            if open_with_proxy_server:
                # print(use_proxy)
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
                                        path=self.metadata_path)
            else:
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
                                        path=self.metadata_path)

            drv.get(product_type_link)
            time.sleep(15)  # 30
            accept_alert(drv, 10)
            close_popups(drv)

            # load all the products
            self.scroll_down_page(drv, h2=0.8, speed=5)
            time.sleep(5)

            try:
                pages = int(drv.find_element_by_css_selector(
                    &#39;div[class*=&#34;pageControl number&#34;]&#39;).get_attribute(&#34;data-pages&#34;))
            except NoSuchElementException as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod Type: {product_type}&#39;)
                self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType {product_type} has\
                only one page of products.(page link: {product_type_link})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                pages = 1
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod Type: {product_type}&#39;)
                self.progress_tracker.loc[pt, &#39;scraped&#39;] = &#39;NA&#39;
                self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType {product_type}\
                     page not found.(page link: {product_type_link})&#39;,
                                            &#39;utf-8&#39;, &#39;ignore&#39;))

            for page in range(pages):
                self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type}\
                                  getting product from page {page}.(page link: {product_type_link})&#39;,
                                            &#39;utf-8&#39;, &#39;ignore&#39;))

                products = drv.find_elements_by_css_selector(
                    &#39;div[class*=&#34;estore_product_container&#34;]&#39;)

                for product in products:
                    # prod_id = &#34;bts_&#34; + \
                    #     product.get_attribute(&#39;data-productid&#39;).split(&#34;.&#34;)[0]
                    self.scroll_to_element(drv, product)
                    try:
                        product_name = product.find_element_by_css_selector(
                            &#39;div.product_name&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                     product {products.index(product)} metadata extraction failed.\
                                                (page_link: {product_type_link} - page_no: {page})&#39;,
                                                    &#39;utf-8&#39;, &#39;ignore&#39;))
                        product_name = &#39;&#39;

                    try:
                        price = product.find_element_by_css_selector(
                            &#39;div.product_price&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        price = &#39;&#39;
                        self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                      product {products.index(product)} price extraction failed.\
                                                (page_link: {product_type_link} - page_no: {page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

                    try:
                        discount = product.find_element_by_css_selector(
                            &#39;div.product_savePrice&gt;span&#39;).text.split()[-1]
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        discount = &#39;&#39;
                        self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                      product {products.index(product)} no discount/savings.\
                                                (page_link: {product_type_link} - page_no: {page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

                    try:
                        product_page = product.find_element_by_css_selector(
                            &#39;div.product_name&gt;a&#39;).get_attribute(&#39;href&#39;)
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        product_page = &#39;&#39;
                        self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                     product {products.index(product)} product_page extraction failed.\
                                                (page_link: {product_type_link} - page_no: {page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

                    try:
                        rating = product.find_element_by_css_selector(
                            &#39;div.product_rating&gt;span&#39;).get_attribute(&#39;aria-label&#39;).split(&#34; &#34;)[0]
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        rating = &#39;&#39;
                        self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                     product {products.index(product)} rating extraction failed.\
                                                (page_link: {product_type_link} - page_no: {page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

                    if datetime.now().day &lt; 15:
                        meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
                    else:
                        meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;

                    product_data_dict = {&#34;product_name&#34;: product_name, &#34;product_page&#34;: product_page, &#34;brand&#34;: &#39;&#39;,
                                         &#34;price&#34;: price, &#34;discount&#34;: discount, &#34;rating&#34;: rating, &#34;category&#34;: cat_name,
                                         &#34;product_type&#34;: product_type, &#34;new_flag&#34;: &#39;&#39;, &#34;meta_date&#34;: meta_date}
                    product_meta_data.append(product_data_dict)

                if pages != 1:
                    next_page_button = drv.find_element_by_css_selector(
                        &#39;a[title*=&#34;Show next&#34;]&#39;)
                    self.scroll_to_element(drv, next_page_button)
                    ActionChains(drv).move_to_element(
                        next_page_button).click(next_page_button).perform()
                    time.sleep(5)
                    accept_alert(drv, 10)
                    close_popups(drv)
                    self.scroll_down_page(drv, h2=0.8, speed=5)
                    time.sleep(5)

            drv.quit()

            if len(product_meta_data) &gt; 0:
                product_meta_df = pd.DataFrame(product_meta_data)
                product_meta_df.to_feather(
                    self.current_progress_path/f&#39;bts_prod_meta_extract_progress_{product_type}_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}&#39;)
                self.logger.info(
                    f&#39;Completed till IndexPosition: {pt} - ProductType: {product_type}. (URL:{product_type_link})&#39;)
                self.progress_tracker.loc[pt, &#39;scraped&#39;] = &#39;Y&#39;
                # print(self.progress_tracker.loc[pt, &#39;scraped&#39;])
                self.progress_tracker.to_feather(
                    self.metadata_path/&#39;bts_metadata_progress_tracker&#39;)
                # print(self.progress_tracker)
                product_meta_data = []
        self.logger.info(&#39;Metadata Extraction Complete&#39;)
        print(&#39;Metadata Extraction Complete&#39;)
        # self.progress_monitor.info(&#39;Metadata Extraction Complete&#39;)

    def extract(self, download: bool = True, fresh_start: bool = False, auto_fresh_start: bool = False, n_workers: int = 5,
                open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = True,
                start_idx: Optional[int] = None, end_idx: Optional[int] = None, list_of_index=None,
                clean: bool = True, compile_progress_files: bool = False, delete_progress: bool = False) -&gt; None:
        &#34;&#34;&#34;extract method controls all properties of the spiders and runs multi-threaded web crawling.

        Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
        This method has four major functionality:
        * 1. Run the spider
        * 2. Store data in regular intervals to free up ram
        * 3. Compile all crawled data into one file.
        * 4. Clean and push cleaned data to S3 storage for further algorithmic processing.

        Args:
            download (bool, optional): Whether to crawl data from or compile crawled data into one file. Defaults to True.
            fresh_start (bool, optional): Whether to continue last crawl job or start new one. Defaults to False.
            auto_fresh_start (bool, optional): Whether to automatically start a new crawl job if last job was finished.
                                               Defaults to False.
            n_workers (int, optional): No. of parallel threads to run. Defaults to 5.
            open_headless (bool, optional): Whether to open browser headless. Defaults to False.
            open_with_proxy_server (bool, optional): Whether to use ip rotation service. Defaults to True.
            randomize_proxy_usage (bool, optional): Whether to use both proxy and native network in tandem to decrease proxy requests.
                                                    Defaults to True.
            start_idx (Optional[int], optional): Starting index of the links to crawl. Defaults to None.
            end_idx (Optional[int], optional): Ending index of the links to crawl. Defaults to None.
            list_of_index ([type], optional): List of indices or range of indices of product urls to scrape. Defaults to None.
            compile_progress_files (bool, optional): Whether to combine crawled data into one file. Defaults to False.
            clean (bool, optional): Whether to clean the compiled data. Defaults to True.
            delete_progress (bool, optional): Whether to delete intermediate data after compilation into one file. Defaults to False.
        &#34;&#34;&#34;
        def fresh():
            &#34;&#34;&#34;[summary]
            &#34;&#34;&#34;
            self.product_type_urls = self.get_product_type_urls(open_headless=open_headless,
                                                                open_with_proxy_server=open_with_proxy_server)
            # progress tracker: captures scraped and error desc
            self.progress_tracker = pd.DataFrame(index=self.product_type_urls.index, columns=[
                &#39;product_type&#39;, &#39;scraped&#39;, &#39;error_desc&#39;])
            self.progress_tracker.scraped = &#39;N&#39;

        if fresh_start:
            self.logger.info(&#39;Starting Fresh Extraction.&#39;)
            fresh()
        else:
            if Path(self.metadata_path/&#39;bts_product_type_urls_to_extract&#39;).exists():
                self.product_type_urls = pd.read_feather(
                    self.metadata_path/&#39;bts_product_type_urls_to_extract&#39;)
                if Path(self.metadata_path/&#39;bts_metadata_progress_tracker&#39;).exists():
                    self.progress_tracker = pd.read_feather(
                        self.metadata_path/&#39;bts_metadata_progress_tracker&#39;)
                else:
                    self.progress_tracker = pd.DataFrame(index=self.product_type_urls.index, columns=[
                        &#39;product_type&#39;, &#39;scraped&#39;, &#39;error_desc&#39;])
                    self.progress_tracker.scraped = &#39;N&#39;
                    self.progress_tracker.to_feather(
                        self.metadata_path/&#39;bts_metadata_progress_tracker&#39;)
                if sum(self.progress_tracker.scraped == &#39;N&#39;) &gt; 0:
                    self.logger.info(
                        &#39;Continuing Metadata Extraction From Last Run.&#39;)
                    self.product_type_urls = self.product_type_urls[self.product_type_urls.index.isin(
                        self.progress_tracker.index[self.progress_tracker.scraped == &#39;N&#39;].values.tolist())]
                else:
                    if auto_fresh_start:
                        self.logger.info(
                            &#39;Previous Run Was Complete. Starting Fresh Extraction.&#39;)
                        fresh()
                    else:
                        self.logger.info(
                            &#39;Previous Run is Complete.&#39;)
            else:
                self.logger.info(
                    &#39;URL File Not Found. Start Fresh Extraction.&#39;)
        # print(self.progress_tracker)
        if download:
            # set list or range of product indices to crawl
            if list_of_index:
                indices = list_of_index
            elif start_idx and end_idx is None:
                indices = range(start_idx, len(self.product_type_urls))
            elif start_idx is None and end_idx:
                indices = range(0, end_idx)
            elif start_idx is not None and end_idx is not None:
                indices = range(start_idx, end_idx)
            else:
                indices = range(len(self.product_type_urls))
            # print(indices)
            if list_of_index:
                self.get_metadata(indices=list_of_index,
                                  open_headless=open_headless,
                                  open_with_proxy_server=open_with_proxy_server,
                                  randomize_proxy_usage=randomize_proxy_usage,
                                  product_meta_data=[])
            else:
                &#39;&#39;&#39;
                # review_Data and item_data are lists of empty lists so that each namepace of function call will
                # have its separate detail_data
                # list to strore scraped dictionaries. will save memory(ram/hard-disk) consumption. will stop data duplication
                &#39;&#39;&#39;
                if start_idx:
                    lst_of_lst = ranges(
                        indices[-1]+1, n_workers, start_idx=start_idx)
                else:
                    lst_of_lst = ranges(len(indices), n_workers)
                print(lst_of_lst)
                headless = [open_headless for i in lst_of_lst]
                proxy = [open_with_proxy_server for i in lst_of_lst]
                rand_proxy = [randomize_proxy_usage for i in lst_of_lst]
                product_meta_data = [[] for i in lst_of_lst]
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    &#39;&#39;&#39;
                    # but each of the function namespace will be modifying only one metadata tracing file so that progress saving
                    # is tracked correctly. else multiple progress tracker file will be created with difficulty to combine correct
                    # progress information
                    &#39;&#39;&#39;
                    executor.map(self.get_metadata, lst_of_lst,
                                 headless, proxy, rand_proxy, product_meta_data)

        if compile_progress_files:
            self.logger.info(&#39;Creating Combined Metadata File&#39;)
            files = [f for f in self.current_progress_path.glob(
                &#34;bts_prod_meta_extract_progress_*&#34;)]
            li = [pd.read_feather(file) for file in files]
            metadata_df = pd.concat(li, axis=0, ignore_index=True)
            metadata_df.reset_index(inplace=True, drop=True)
            metadata_df[&#39;source&#39;] = self.source

            if datetime.now().day &lt; 15:
                meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
            else:
                meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;
            filename = f&#39;bts_product_metadata_all_{meta_date}&#39;
            metadata_df.to_feather(self.metadata_path/filename)

            self.logger.info(
                f&#39;Metadata file created. Please look for file {filename} in path {self.metadata_path}&#39;)
            print(
                f&#39;Metadata file created. Please look for file {filename} in path {self.metadata_path}&#39;)

            if clean:
                cleaner = Cleaner(path=self.path)
                _ = cleaner.clean(
                    data=self.metadata_path/filename)
                self.logger.info(
                    &#39;Metadata Cleaned and Removed Duplicates for Details/Review/Image Extraction.&#39;)

            if delete_progress:
                shutil.rmtree(
                    f&#39;{self.metadata_path}\\current_progress&#39;, ignore_errors=True)
                self.logger.info(&#39;Progress files deleted&#39;)

    def terminate_logging(self):
        &#34;&#34;&#34;terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.
        &#34;&#34;&#34;
        self.logger.handlers.clear()
        self.prod_meta_log.stop_log()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="meiyume.utils.Boots" href="../utils.html#meiyume.utils.Boots">Boots</a></li>
<li><a title="meiyume.utils.Browser" href="../utils.html#meiyume.utils.Browser">Browser</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="meiyume.bts.crawler.Metadata.base_url"><code class="name">var <span class="ident">base_url</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="meiyume.bts.crawler.Metadata.info"><code class="name">var <span class="ident">info</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="meiyume.bts.crawler.Metadata.source"><code class="name">var <span class="ident">source</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="meiyume.bts.crawler.Metadata.update_base_url"><code class="name flex">
<span>def <span class="ident">update_base_url</span></span>(<span>url:str) >NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>update_base_url defines the parent url from where the data scraping process will begin.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>url</code></strong> :&ensp;<code>str</code></dt>
<dd>The URL from which the spider will enter the website.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def update_base_url(cls, url: str) -&gt; None:
    &#34;&#34;&#34;update_base_url defines the parent url from where the data scraping process will begin.

    Args:
        url (str): The URL from which the spider will enter the website.

    &#34;&#34;&#34;
    cls.base_url = url
    cls.info = tldextract.extract(cls.base_url)
    cls.source = cls.info.registered_domain</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="meiyume.bts.crawler.Metadata.extract"><code class="name flex">
<span>def <span class="ident">extract</span></span>(<span>self, download:bool=True, fresh_start:bool=False, auto_fresh_start:bool=False, n_workers:int=5, open_headless:bool=False, open_with_proxy_server:bool=True, randomize_proxy_usage:bool=True, start_idx:Union[int,NoneType]=None, end_idx:Union[int,NoneType]=None, list_of_index=None, clean:bool=True, compile_progress_files:bool=False, delete_progress:bool=False) >NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>extract method controls all properties of the spiders and runs multi-threaded web crawling.</p>
<p>Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
This method has four major functionality:
* 1. Run the spider
* 2. Store data in regular intervals to free up ram
* 3. Compile all crawled data into one file.
* 4. Clean and push cleaned data to S3 storage for further algorithmic processing.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>download</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to crawl data from or compile crawled data into one file. Defaults to True.</dd>
<dt><strong><code>fresh_start</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to continue last crawl job or start new one. Defaults to False.</dd>
<dt><strong><code>auto_fresh_start</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to automatically start a new crawl job if last job was finished.
Defaults to False.</dd>
<dt><strong><code>n_workers</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>No. of parallel threads to run. Defaults to 5.</dd>
<dt><strong><code>open_headless</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to open browser headless. Defaults to False.</dd>
<dt><strong><code>open_with_proxy_server</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use ip rotation service. Defaults to True.</dd>
<dt><strong><code>randomize_proxy_usage</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use both proxy and native network in tandem to decrease proxy requests.
Defaults to True.</dd>
<dt><strong><code>start_idx</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>Starting index of the links to crawl. Defaults to None.</dd>
<dt><strong><code>end_idx</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>Ending index of the links to crawl. Defaults to None.</dd>
<dt><strong><code>list_of_index</code></strong> :&ensp;<code>[type]</code>, optional</dt>
<dd>List of indices or range of indices of product urls to scrape. Defaults to None.</dd>
<dt><strong><code>compile_progress_files</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to combine crawled data into one file. Defaults to False.</dd>
<dt><strong><code>clean</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to clean the compiled data. Defaults to True.</dd>
<dt><strong><code>delete_progress</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to delete intermediate data after compilation into one file. Defaults to False.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract(self, download: bool = True, fresh_start: bool = False, auto_fresh_start: bool = False, n_workers: int = 5,
            open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = True,
            start_idx: Optional[int] = None, end_idx: Optional[int] = None, list_of_index=None,
            clean: bool = True, compile_progress_files: bool = False, delete_progress: bool = False) -&gt; None:
    &#34;&#34;&#34;extract method controls all properties of the spiders and runs multi-threaded web crawling.

    Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
    This method has four major functionality:
    * 1. Run the spider
    * 2. Store data in regular intervals to free up ram
    * 3. Compile all crawled data into one file.
    * 4. Clean and push cleaned data to S3 storage for further algorithmic processing.

    Args:
        download (bool, optional): Whether to crawl data from or compile crawled data into one file. Defaults to True.
        fresh_start (bool, optional): Whether to continue last crawl job or start new one. Defaults to False.
        auto_fresh_start (bool, optional): Whether to automatically start a new crawl job if last job was finished.
                                           Defaults to False.
        n_workers (int, optional): No. of parallel threads to run. Defaults to 5.
        open_headless (bool, optional): Whether to open browser headless. Defaults to False.
        open_with_proxy_server (bool, optional): Whether to use ip rotation service. Defaults to True.
        randomize_proxy_usage (bool, optional): Whether to use both proxy and native network in tandem to decrease proxy requests.
                                                Defaults to True.
        start_idx (Optional[int], optional): Starting index of the links to crawl. Defaults to None.
        end_idx (Optional[int], optional): Ending index of the links to crawl. Defaults to None.
        list_of_index ([type], optional): List of indices or range of indices of product urls to scrape. Defaults to None.
        compile_progress_files (bool, optional): Whether to combine crawled data into one file. Defaults to False.
        clean (bool, optional): Whether to clean the compiled data. Defaults to True.
        delete_progress (bool, optional): Whether to delete intermediate data after compilation into one file. Defaults to False.
    &#34;&#34;&#34;
    def fresh():
        &#34;&#34;&#34;[summary]
        &#34;&#34;&#34;
        self.product_type_urls = self.get_product_type_urls(open_headless=open_headless,
                                                            open_with_proxy_server=open_with_proxy_server)
        # progress tracker: captures scraped and error desc
        self.progress_tracker = pd.DataFrame(index=self.product_type_urls.index, columns=[
            &#39;product_type&#39;, &#39;scraped&#39;, &#39;error_desc&#39;])
        self.progress_tracker.scraped = &#39;N&#39;

    if fresh_start:
        self.logger.info(&#39;Starting Fresh Extraction.&#39;)
        fresh()
    else:
        if Path(self.metadata_path/&#39;bts_product_type_urls_to_extract&#39;).exists():
            self.product_type_urls = pd.read_feather(
                self.metadata_path/&#39;bts_product_type_urls_to_extract&#39;)
            if Path(self.metadata_path/&#39;bts_metadata_progress_tracker&#39;).exists():
                self.progress_tracker = pd.read_feather(
                    self.metadata_path/&#39;bts_metadata_progress_tracker&#39;)
            else:
                self.progress_tracker = pd.DataFrame(index=self.product_type_urls.index, columns=[
                    &#39;product_type&#39;, &#39;scraped&#39;, &#39;error_desc&#39;])
                self.progress_tracker.scraped = &#39;N&#39;
                self.progress_tracker.to_feather(
                    self.metadata_path/&#39;bts_metadata_progress_tracker&#39;)
            if sum(self.progress_tracker.scraped == &#39;N&#39;) &gt; 0:
                self.logger.info(
                    &#39;Continuing Metadata Extraction From Last Run.&#39;)
                self.product_type_urls = self.product_type_urls[self.product_type_urls.index.isin(
                    self.progress_tracker.index[self.progress_tracker.scraped == &#39;N&#39;].values.tolist())]
            else:
                if auto_fresh_start:
                    self.logger.info(
                        &#39;Previous Run Was Complete. Starting Fresh Extraction.&#39;)
                    fresh()
                else:
                    self.logger.info(
                        &#39;Previous Run is Complete.&#39;)
        else:
            self.logger.info(
                &#39;URL File Not Found. Start Fresh Extraction.&#39;)
    # print(self.progress_tracker)
    if download:
        # set list or range of product indices to crawl
        if list_of_index:
            indices = list_of_index
        elif start_idx and end_idx is None:
            indices = range(start_idx, len(self.product_type_urls))
        elif start_idx is None and end_idx:
            indices = range(0, end_idx)
        elif start_idx is not None and end_idx is not None:
            indices = range(start_idx, end_idx)
        else:
            indices = range(len(self.product_type_urls))
        # print(indices)
        if list_of_index:
            self.get_metadata(indices=list_of_index,
                              open_headless=open_headless,
                              open_with_proxy_server=open_with_proxy_server,
                              randomize_proxy_usage=randomize_proxy_usage,
                              product_meta_data=[])
        else:
            &#39;&#39;&#39;
            # review_Data and item_data are lists of empty lists so that each namepace of function call will
            # have its separate detail_data
            # list to strore scraped dictionaries. will save memory(ram/hard-disk) consumption. will stop data duplication
            &#39;&#39;&#39;
            if start_idx:
                lst_of_lst = ranges(
                    indices[-1]+1, n_workers, start_idx=start_idx)
            else:
                lst_of_lst = ranges(len(indices), n_workers)
            print(lst_of_lst)
            headless = [open_headless for i in lst_of_lst]
            proxy = [open_with_proxy_server for i in lst_of_lst]
            rand_proxy = [randomize_proxy_usage for i in lst_of_lst]
            product_meta_data = [[] for i in lst_of_lst]
            with concurrent.futures.ThreadPoolExecutor() as executor:
                &#39;&#39;&#39;
                # but each of the function namespace will be modifying only one metadata tracing file so that progress saving
                # is tracked correctly. else multiple progress tracker file will be created with difficulty to combine correct
                # progress information
                &#39;&#39;&#39;
                executor.map(self.get_metadata, lst_of_lst,
                             headless, proxy, rand_proxy, product_meta_data)

    if compile_progress_files:
        self.logger.info(&#39;Creating Combined Metadata File&#39;)
        files = [f for f in self.current_progress_path.glob(
            &#34;bts_prod_meta_extract_progress_*&#34;)]
        li = [pd.read_feather(file) for file in files]
        metadata_df = pd.concat(li, axis=0, ignore_index=True)
        metadata_df.reset_index(inplace=True, drop=True)
        metadata_df[&#39;source&#39;] = self.source

        if datetime.now().day &lt; 15:
            meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
        else:
            meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;
        filename = f&#39;bts_product_metadata_all_{meta_date}&#39;
        metadata_df.to_feather(self.metadata_path/filename)

        self.logger.info(
            f&#39;Metadata file created. Please look for file {filename} in path {self.metadata_path}&#39;)
        print(
            f&#39;Metadata file created. Please look for file {filename} in path {self.metadata_path}&#39;)

        if clean:
            cleaner = Cleaner(path=self.path)
            _ = cleaner.clean(
                data=self.metadata_path/filename)
            self.logger.info(
                &#39;Metadata Cleaned and Removed Duplicates for Details/Review/Image Extraction.&#39;)

        if delete_progress:
            shutil.rmtree(
                f&#39;{self.metadata_path}\\current_progress&#39;, ignore_errors=True)
            self.logger.info(&#39;Progress files deleted&#39;)</code></pre>
</details>
</dd>
<dt id="meiyume.bts.crawler.Metadata.get_metadata"><code class="name flex">
<span>def <span class="ident">get_metadata</span></span>(<span>self, indices:Union[list,range], open_headless:bool, open_with_proxy_server:bool, randomize_proxy_usage:bool, product_meta_data:list=[])</span>
</code></dt>
<dd>
<div class="desc"><p>get_metadata Crawls product listing pages for price, name, brand etc.</p>
<p>Get Metadata crawls a product type page for example lipstick.
The function gets individual product urls, names, brands and prices etc. and stores
in a relational table structure to use later to download product images, scrape reviews and
other specific information.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>indices</code></strong> :&ensp;<code>Union[list, range]</code></dt>
<dd>list of indices or range of indices of product urls to scrape.</dd>
<dt><strong><code>open_headless</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to open browser headless.</dd>
<dt><strong><code>open_with_proxy_server</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use proxy server.</dd>
<dt><strong><code>randomize_proxy_usage</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use both proxy and native network in tandem to decrease proxy requests.</dd>
<dt><strong><code>product_meta_data</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Empty intermediate list to store product metadata during parallel crawl. Defaults to [].</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_metadata(self, indices: Union[list, range],
                 open_headless: bool, open_with_proxy_server: bool,
                 randomize_proxy_usage: bool,
                 product_meta_data: list = []):
    &#34;&#34;&#34;get_metadata Crawls product listing pages for price, name, brand etc.

    Get Metadata crawls a product type page for example lipstick.
    The function gets individual product urls, names, brands and prices etc. and stores
    in a relational table structure to use later to download product images, scrape reviews and
    other specific information.

    Args:
        indices (Union[list, range]): list of indices or range of indices of product urls to scrape.
        open_headless (bool): Whether to open browser headless.
        open_with_proxy_server (bool): Whether to use proxy server.
        randomize_proxy_usage (bool): Whether to use both proxy and native network in tandem to decrease proxy requests.
        product_meta_data (list, optional): Empty intermediate list to store product metadata during parallel crawl. Defaults to [].
    &#34;&#34;&#34;
    for pt in self.product_type_urls.index[self.product_type_urls.index.isin(indices)]:
        cat_name = self.product_type_urls.loc[pt, &#39;category_raw&#39;]
        product_type = self.product_type_urls.loc[pt, &#39;product_type&#39;]
        product_type_link = self.product_type_urls.loc[pt, &#39;url&#39;]

        self.progress_tracker.loc[pt, &#39;product_type&#39;] = product_type
        # print(self.progress_tracker.loc[pt, &#39;product_type&#39;])
        # print(product_type_link)
        if &#39;best-selling&#39; in product_type.lower() or &#39;new&#39; in product_type.lower():
            self.progress_tracker.loc[pt, &#39;scraped&#39;] = &#39;NA&#39;
            # print(self.progress_tracker.loc[pt, &#39;scraped&#39;])
            continue

        if randomize_proxy_usage:
            use_proxy = np.random.choice([True, False])
        else:
            use_proxy = True
        if open_with_proxy_server:
            # print(use_proxy)
            drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
                                    path=self.metadata_path)
        else:
            drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
                                    path=self.metadata_path)

        drv.get(product_type_link)
        time.sleep(15)  # 30
        accept_alert(drv, 10)
        close_popups(drv)

        # load all the products
        self.scroll_down_page(drv, h2=0.8, speed=5)
        time.sleep(5)

        try:
            pages = int(drv.find_element_by_css_selector(
                &#39;div[class*=&#34;pageControl number&#34;]&#39;).get_attribute(&#34;data-pages&#34;))
        except NoSuchElementException as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod Type: {product_type}&#39;)
            self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType {product_type} has\
            only one page of products.(page link: {product_type_link})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            pages = 1
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod Type: {product_type}&#39;)
            self.progress_tracker.loc[pt, &#39;scraped&#39;] = &#39;NA&#39;
            self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType {product_type}\
                 page not found.(page link: {product_type_link})&#39;,
                                        &#39;utf-8&#39;, &#39;ignore&#39;))

        for page in range(pages):
            self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type}\
                              getting product from page {page}.(page link: {product_type_link})&#39;,
                                        &#39;utf-8&#39;, &#39;ignore&#39;))

            products = drv.find_elements_by_css_selector(
                &#39;div[class*=&#34;estore_product_container&#34;]&#39;)

            for product in products:
                # prod_id = &#34;bts_&#34; + \
                #     product.get_attribute(&#39;data-productid&#39;).split(&#34;.&#34;)[0]
                self.scroll_to_element(drv, product)
                try:
                    product_name = product.find_element_by_css_selector(
                        &#39;div.product_name&#39;).text
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod Type: {product_type}&#39;)
                    self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                 product {products.index(product)} metadata extraction failed.\
                                            (page_link: {product_type_link} - page_no: {page})&#39;,
                                                &#39;utf-8&#39;, &#39;ignore&#39;))
                    product_name = &#39;&#39;

                try:
                    price = product.find_element_by_css_selector(
                        &#39;div.product_price&#39;).text
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod Type: {product_type}&#39;)
                    price = &#39;&#39;
                    self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                  product {products.index(product)} price extraction failed.\
                                            (page_link: {product_type_link} - page_no: {page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

                try:
                    discount = product.find_element_by_css_selector(
                        &#39;div.product_savePrice&gt;span&#39;).text.split()[-1]
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod Type: {product_type}&#39;)
                    discount = &#39;&#39;
                    self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                  product {products.index(product)} no discount/savings.\
                                            (page_link: {product_type_link} - page_no: {page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

                try:
                    product_page = product.find_element_by_css_selector(
                        &#39;div.product_name&gt;a&#39;).get_attribute(&#39;href&#39;)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod Type: {product_type}&#39;)
                    product_page = &#39;&#39;
                    self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                 product {products.index(product)} product_page extraction failed.\
                                            (page_link: {product_type_link} - page_no: {page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

                try:
                    rating = product.find_element_by_css_selector(
                        &#39;div.product_rating&gt;span&#39;).get_attribute(&#39;aria-label&#39;).split(&#34; &#34;)[0]
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod Type: {product_type}&#39;)
                    rating = &#39;&#39;
                    self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                 product {products.index(product)} rating extraction failed.\
                                            (page_link: {product_type_link} - page_no: {page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

                if datetime.now().day &lt; 15:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
                else:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;

                product_data_dict = {&#34;product_name&#34;: product_name, &#34;product_page&#34;: product_page, &#34;brand&#34;: &#39;&#39;,
                                     &#34;price&#34;: price, &#34;discount&#34;: discount, &#34;rating&#34;: rating, &#34;category&#34;: cat_name,
                                     &#34;product_type&#34;: product_type, &#34;new_flag&#34;: &#39;&#39;, &#34;meta_date&#34;: meta_date}
                product_meta_data.append(product_data_dict)

            if pages != 1:
                next_page_button = drv.find_element_by_css_selector(
                    &#39;a[title*=&#34;Show next&#34;]&#39;)
                self.scroll_to_element(drv, next_page_button)
                ActionChains(drv).move_to_element(
                    next_page_button).click(next_page_button).perform()
                time.sleep(5)
                accept_alert(drv, 10)
                close_popups(drv)
                self.scroll_down_page(drv, h2=0.8, speed=5)
                time.sleep(5)

        drv.quit()

        if len(product_meta_data) &gt; 0:
            product_meta_df = pd.DataFrame(product_meta_data)
            product_meta_df.to_feather(
                self.current_progress_path/f&#39;bts_prod_meta_extract_progress_{product_type}_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}&#39;)
            self.logger.info(
                f&#39;Completed till IndexPosition: {pt} - ProductType: {product_type}. (URL:{product_type_link})&#39;)
            self.progress_tracker.loc[pt, &#39;scraped&#39;] = &#39;Y&#39;
            # print(self.progress_tracker.loc[pt, &#39;scraped&#39;])
            self.progress_tracker.to_feather(
                self.metadata_path/&#39;bts_metadata_progress_tracker&#39;)
            # print(self.progress_tracker)
            product_meta_data = []
    self.logger.info(&#39;Metadata Extraction Complete&#39;)
    print(&#39;Metadata Extraction Complete&#39;)</code></pre>
</details>
</dd>
<dt id="meiyume.bts.crawler.Metadata.get_product_type_urls"><code class="name flex">
<span>def <span class="ident">get_product_type_urls</span></span>(<span>self, open_headless:bool, open_with_proxy_server:bool) >pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>get_product_type_urls Extract the category/subcategory structure and urls to extract the products of those category/subcategory.</p>
<p>Extracts the links of pages containing the list of all products structured into
category/subcategory/product type to effectively stored in relational database.
Defines the structure of data extraction that helps store unstructured data in a structured manner.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>open_headless</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to open browser headless.</dd>
<dt><strong><code>open_with_proxy_server</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use proxy server.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>returns pandas dataframe containing urls for getting list of products, category, subcategory etc.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_product_type_urls(self, open_headless: bool, open_with_proxy_server: bool) -&gt; pd.DataFrame:
    &#34;&#34;&#34;get_product_type_urls Extract the category/subcategory structure and urls to extract the products of those category/subcategory.

    Extracts the links of pages containing the list of all products structured into
    category/subcategory/product type to effectively stored in relational database.
    Defines the structure of data extraction that helps store unstructured data in a structured manner.

    Args:
        open_headless (bool): Whether to open browser headless.
        open_with_proxy_server (bool): Whether to use proxy server.

    Returns:
        pd.DataFrame: returns pandas dataframe containing urls for getting list of products, category, subcategory etc.
    &#34;&#34;&#34;
    # create webdriver instance
    drv = self.open_browser(
        open_headless=open_headless, open_with_proxy_server=open_with_proxy_server, path=self.metadata_path)

    drv.get(self.base_url)
    time.sleep(15)
    # click and close welcome forms
    accept_alert(drv, 10)
    close_popups(drv)

    try:
        country_popup = WebDriverWait(drv, 3).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, &#39;.estores_overlay_content &gt; a:nth-child(1)&#39;)))
        pop_close_button = drv.find_element_by_css_selector(
            &#39;.estores_overlay_content &gt; a:nth-child(1)&#39;)
        Browser().scroll_to_element(drv, pop_close_button)
        ActionChains(drv).move_to_element(
            pop_close_button).click(pop_close_button).perform()
    except Exception as ex:
        pass

    allowed_categories = [&#39;beauty&#39;, &#39;fragrance&#39;,
                          &#39;toiletries&#39;, &#39;mens&#39;, &#39;wellness&#39;]
    exclude_categories = [&#39;health-pharmacy&#39;, &#39;advice&#39;, &#39;luxury-beauty-premium-beauty-book-an-appointment&#39;,
                          &#39;health-value-packs-and-bundles&#39;, &#39;all-luxury-skincare&#39;, &#39;wellness-supplements&#39;, &#39;travel-essentials&#39;,
                          &#39;opticians&#39;, &#39;feminine-hygiene&#39;, &#39;travel-health&#39;, &#39;sunglasses&#39;, &#39;inspiration&#39;, &#39;food-and-drink&#39;,
                          &#39;nutrition&#39;, &#39;vitaminsandsupplements&#39;, &#39;condoms-sexual-health&#39;, &#39;mens-health-information&#39;,
                          &#39;weightloss&#39;, &#39;menshealth&#39;, &#39;recommended&#39;, &#39;offers&#39;, &#39;all-&#39;, &#39;male-incontinence&#39;, &#39;sustainable-living&#39;,
                          &#39;bootsdental&#39;, &#39;back-to-school-and-nursery&#39;, &#39;luxury-beauty-skincare&#39;, &#39;mens-gift-sets&#39;,
                          &#39;all-face&#39;, &#39;all-eyes&#39;, &#39;all-lips&#39;, &#39;gift/him/mens-aftershave&#39;, &#39;gift/her/luxury-beauty-gift&#39;,
                          &#39;christmas/christmas-3-for-2&#39;, &#39;christmas/gifts-for-her&#39;, &#39;christmas/gifts-for-him&#39;,
                          &#39;christmas/advent-calendars&#39;, &#39;beauty-expert-skincare-expert-skincare-shop-all&#39;,
                          &#39;black-afro-and-textured-hair-straight-hair&#39;, &#39;black-afro-and-textured-hair-wavy&#39;,
                          &#39;black-afro-and-textured-hair-straight-hair&#39;,
                          ]
    # Extracting the category-sub category structure
    cat_urls = []
    for i in drv.find_elements_by_css_selector(&#39;a[id*=&#34;subcategoryLink&#34;]&#39;):
        cat_urls.append(i.get_attribute(&#39;href&#39;))

    cat_urls = [u for u in cat_urls if any(
        cat in str(u) for cat in allowed_categories)]
    cat_urls = [u for u in cat_urls if all(
        cat not in str(u) for cat in exclude_categories)]

    prod_type_urls = []

    for url in cat_urls:
        drv.get(url)

        time.sleep(6)
        accept_alert(drv, 10)
        close_popups(drv)

        subcats = drv.find_elements_by_css_selector(&#39;div.category-link&gt;a&#39;)
        if len(subcats) &gt; 0:
            for i in drv.find_elements_by_css_selector(&#39;div.category-link&gt;a&#39;):
                prod_type_urls.append(i.get_attribute(&#39;href&#39;))
        else:
            prod_type_urls.append(url)

    prod_type_urls = [u for u in prod_type_urls if any(
        cat in str(u) for cat in allowed_categories)]
    prod_type_urls = [u for u in prod_type_urls if all(
        cat not in str(u) for cat in exclude_categories)]

    drv.quit()

    df = pd.DataFrame(prod_type_urls, columns=[&#39;url&#39;])
    df[&#39;dept&#39;], df[&#39;category_raw&#39;], df[&#39;subcategory_raw&#39;], df[&#39;product_type&#39;] = zip(
        *df.url.str.split(&#39;/&#39;, expand=True).loc[:, 3:].values)

    def set_cat_subcat_ptype(x):
        if x.product_type is None:
            return x.dept, x.category_raw, x.subcategory_raw
        else:
            return x.category_raw, x.subcategory_raw, x.product_type

    df.category_raw, df.subcategory_raw, df.product_type = zip(
        *df.apply(set_cat_subcat_ptype, axis=1))
    df.drop(columns=[&#39;dept&#39;], inplace=True)
    df.drop_duplicates(subset=&#39;url&#39;, inplace=True)
    df.reset_index(inplace=True, drop=True)
    df[&#39;scraped&#39;] = &#39;N&#39;
    df.to_feather(self.metadata_path/f&#39;bts_product_type_urls_to_extract&#39;)
    return df</code></pre>
</details>
</dd>
<dt id="meiyume.bts.crawler.Metadata.terminate_logging"><code class="name flex">
<span>def <span class="ident">terminate_logging</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def terminate_logging(self):
    &#34;&#34;&#34;terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.
    &#34;&#34;&#34;
    self.logger.handlers.clear()
    self.prod_meta_log.stop_log()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="meiyume.utils.Boots" href="../utils.html#meiyume.utils.Boots">Boots</a></b></code>:
<ul class="hlist">
<li><code><a title="meiyume.utils.Boots.open_browser" href="../utils.html#meiyume.utils.Browser.open_browser">open_browser</a></code></li>
<li><code><a title="meiyume.utils.Boots.open_browser_firefox" href="../utils.html#meiyume.utils.Browser.open_browser_firefox">open_browser_firefox</a></code></li>
<li><code><a title="meiyume.utils.Boots.scroll_down_page" href="../utils.html#meiyume.utils.Browser.scroll_down_page">scroll_down_page</a></code></li>
<li><code><a title="meiyume.utils.Boots.scroll_to_element" href="../utils.html#meiyume.utils.Browser.scroll_to_element">scroll_to_element</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="meiyume.bts" href="index.html">meiyume.bts</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="meiyume.bts.crawler.DetailReview" href="#meiyume.bts.crawler.DetailReview">DetailReview</a></code></h4>
<ul class="">
<li><code><a title="meiyume.bts.crawler.DetailReview.crawl_page" href="#meiyume.bts.crawler.DetailReview.crawl_page">crawl_page</a></code></li>
<li><code><a title="meiyume.bts.crawler.DetailReview.extract" href="#meiyume.bts.crawler.DetailReview.extract">extract</a></code></li>
<li><code><a title="meiyume.bts.crawler.DetailReview.get_details" href="#meiyume.bts.crawler.DetailReview.get_details">get_details</a></code></li>
<li><code><a title="meiyume.bts.crawler.DetailReview.get_reviews" href="#meiyume.bts.crawler.DetailReview.get_reviews">get_reviews</a></code></li>
<li><code><a title="meiyume.bts.crawler.DetailReview.terminate_logging" href="#meiyume.bts.crawler.DetailReview.terminate_logging">terminate_logging</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="meiyume.bts.crawler.Metadata" href="#meiyume.bts.crawler.Metadata">Metadata</a></code></h4>
<ul class="">
<li><code><a title="meiyume.bts.crawler.Metadata.base_url" href="#meiyume.bts.crawler.Metadata.base_url">base_url</a></code></li>
<li><code><a title="meiyume.bts.crawler.Metadata.extract" href="#meiyume.bts.crawler.Metadata.extract">extract</a></code></li>
<li><code><a title="meiyume.bts.crawler.Metadata.get_metadata" href="#meiyume.bts.crawler.Metadata.get_metadata">get_metadata</a></code></li>
<li><code><a title="meiyume.bts.crawler.Metadata.get_product_type_urls" href="#meiyume.bts.crawler.Metadata.get_product_type_urls">get_product_type_urls</a></code></li>
<li><code><a title="meiyume.bts.crawler.Metadata.info" href="#meiyume.bts.crawler.Metadata.info">info</a></code></li>
<li><code><a title="meiyume.bts.crawler.Metadata.source" href="#meiyume.bts.crawler.Metadata.source">source</a></code></li>
<li><code><a title="meiyume.bts.crawler.Metadata.terminate_logging" href="#meiyume.bts.crawler.Metadata.terminate_logging">terminate_logging</a></code></li>
<li><code><a title="meiyume.bts.crawler.Metadata.update_base_url" href="#meiyume.bts.crawler.Metadata.update_base_url">update_base_url</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>