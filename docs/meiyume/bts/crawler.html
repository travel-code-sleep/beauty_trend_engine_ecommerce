<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>meiyume.bts.crawler API documentation</title>
<meta name="description" content="The module to crawl Boots website data." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>meiyume.bts.crawler</code></h1>
</header>
<section id="section-intro">
<p>The module to crawl Boots website data.</p>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="meiyume.bts.crawler.DetailReview"><code class="flex name class">
<span>class <span class="ident">DetailReview</span></span>
<span>(</span><span>log: bool = True, path: pathlib.Path = WindowsPath('D:/Amit/Meiyume/meiyume_master_source_codes'))</span>
</code></dt>
<dd>
<div class="desc"><p>DetailReview class carries out scraping of Boots Detail and Review data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>Boots</code></strong> :&ensp;<code>Browser</code></dt>
<dd>Class that initializes folder paths and selenium webdriver for data scraping.</dd>
</dl>
<p><strong>init</strong> DetailReview class instace initializer.</p>
<p>This method sets all the folder paths required for DetailReview crawler to work.
If the paths does not exist the paths get automatically created depending on current directory
or provided directory.</p>
<p>Initializer Tasks:
1. Set folder paths for data reading and storing.
2. Move old scraped and cleaned data files to archive folders before starting new extraction.
3. Create data scraping log files in log path to monitor/correct failed data extraction from website.</p>
<h2 id="args_1">Args</h2>
<dl>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to create crawling exception and progess log. Defaults to True.</dd>
<dt><strong><code>path</code></strong> :&ensp;<code>Path</code>, optional</dt>
<dd>Folder path where the Detail and Review data will be extracted.
Defaults to current directory(Path.cwd()).</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="meiyume.utils.Boots" href="../utils.html#meiyume.utils.Boots">Boots</a></li>
<li><a title="meiyume.utils.Browser" href="../utils.html#meiyume.utils.Browser">Browser</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="meiyume.bts.crawler.DetailReview.crawl_page"><code class="name flex">
<span>def <span class="ident">crawl_page</span></span>(<span>self, indices: list, open_headless: bool, open_with_proxy_server: bool, randomize_proxy_usage: bool, detail_data: list = [], item_df=Empty DataFrame
Columns: [prod_id, product_name, item_name, item_size, item_price, item_ingredients]
Index: [], review_data: list = [], incremental: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>crawl_page opens each product url in a sepearate browser to scrape detail and review data together.</p>
<p>The scraping happens in a multi-threaded manner to extract product information of up to 20 products simultaneously.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>indices</code></strong> :&ensp;<code>list</code></dt>
<dd>list of indices or range of indices of product urls to scrape.</dd>
<dt><strong><code>open_headless</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to open browser headless.</dd>
<dt><strong><code>open_with_proxy_server</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use ip rotation service.</dd>
<dt><strong><code>randomize_proxy_usage</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use both proxy and native network in tandem to decrease proxy requests.</dd>
<dt><strong><code>detail_data</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Empty intermediate list to store data during parallel crawl. Defaults to [].</dd>
<dt><strong><code>item_df</code></strong> :&ensp;<code>[type]</code>, optional</dt>
<dd>Empty intermediate dataframe to store data during parallel crawl.
Defaults to []. Defaults to pd.DataFrame(columns=['prod_id', 'product_name', 'item_name',
'item_size', 'item_price', 'item_ingredients']).</dd>
<dt><strong><code>review_data</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Empty intermediate list to store data during parallel crawl. Defaults to [].</dd>
<dt><strong><code>incremental</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to scrape reviews incrementally from last scraped review date. Defaults to True.</dd>
</dl></div>
</dd>
<dt id="meiyume.bts.crawler.DetailReview.extract"><code class="name flex">
<span>def <span class="ident">extract</span></span>(<span>self, metadata: Union[pandas.core.frame.DataFrame, str, pathlib.Path], download: bool = True, n_workers: int = 5, fresh_start: bool = False, auto_fresh_start: bool = False, incremental: bool = True, open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = True, start_idx: Union[int, NoneType] = None, end_idx: Union[int, NoneType] = None, list_of_index=None, compile_progress_files: bool = False, clean: bool = True, delete_progress: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Extract method controls all properties of the spiders and runs multi-threaded web crawling.</p>
<p>Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
This method has four major functionality:
* 1. Run the spider
* 2. Store data in regular intervals to free up ram
* 3. Compile all crawled data into one file.
* 4. Clean and push cleaned data to S3 storage for further algorithmic processing.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>metadata</code></strong> :&ensp;<code>Union[pd.DataFrame, str, Path]</code></dt>
<dd>Dataframe containing product specific url, name and id of the products to be scraped.</dd>
<dt><strong><code>download</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to crawl data from or compile crawled data into one file. Defaults to True.</dd>
<dt><strong><code>n_workers</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>No. of parallel threads to run. Defaults to 5.</dd>
<dt><strong><code>fresh_start</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to continue last crawl job or start new one. Defaults to False.</dd>
<dt><strong><code>auto_fresh_start</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to automatically start a new crawl job if last job was finished. Defaults to False.</dd>
<dt><strong><code>incremental</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to scrape reviews incrementally from last scraped review date. Defaults to True.</dd>
<dt><strong><code>open_headless</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to open browser headless. Defaults to False.</dd>
<dt><strong><code>open_with_proxy_server</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use ip rotation service. Defaults to True.</dd>
<dt><strong><code>randomize_proxy_usage</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use both proxy and native network in tandem to decrease proxy requests.</dd>
<dt>Defaults to False.</dt>
<dt><strong><code>start_idx</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>Starting index of the links to crawl. Defaults to None.</dd>
<dt><strong><code>end_idx</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>Ending index of the links to crawl. Defaults to None.</dd>
<dt><strong><code>list_of_index</code></strong> :&ensp;<code>[type]</code>, optional</dt>
<dd>List of indices or range of indices of product urls to scrape. Defaults to None.</dd>
<dt><strong><code>compile_progress_files</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to combine crawled data into one file. Defaults to False.</dd>
<dt><strong><code>clean</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to clean the compiled data. Defaults to True.</dd>
<dt><strong><code>delete_progress</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to delete intermediate data after compilation into one file. Defaults to False.</dd>
</dl></div>
</dd>
<dt id="meiyume.bts.crawler.DetailReview.get_details"><code class="name flex">
<span>def <span class="ident">get_details</span></span>(<span>self, drv: selenium.webdriver.chrome.webdriver.WebDriver, prod_id: str, product_name: str) ‑> Tuple[dict, pandas.core.frame.DataFrame]</span>
</code></dt>
<dd>
<div class="desc"><p>get_detail scrapes individual product pages for price, ingredients, color etc.</p>
<p>Get Detail crawls product specific page and scrapes data such as ingredients, review rating distribution,
size specific prices, color, product claims and other information pertaining to one individual product.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>drv</code></strong> :&ensp;<code>webdriver.Chrome</code></dt>
<dd>Selenium webdriver with opened product page.</dd>
<dt><strong><code>prod_id</code></strong> :&ensp;<code>str</code></dt>
<dd>Id of the product from metdata.</dd>
<dt><strong><code>product_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the product from metadata.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[dict, pd.DataFrame]</code></dt>
<dd>Dictionary containing details of a product and dataframe containing item
attributes of a product.</dd>
</dl></div>
</dd>
<dt id="meiyume.bts.crawler.DetailReview.get_reviews"><code class="name flex">
<span>def <span class="ident">get_reviews</span></span>(<span>self, drv: selenium.webdriver.chrome.webdriver.WebDriver, prod_id: str, product_name: str, last_scraped_review_date: str, no_of_reviews: int, incremental: bool = True, reviews: list = []) ‑> list</span>
</code></dt>
<dd>
<div class="desc"><p>get_reviews crawls individual product pages for review text, title, date user attributes etc.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>drv</code></strong> :&ensp;<code>webdriver.Chrome</code></dt>
<dd>drv (webdriver.Chrome): Selenium webdriver with opened product page.</dd>
<dt><strong><code>prod_id</code></strong> :&ensp;<code>str</code></dt>
<dd>Id of the product from metdata.</dd>
<dt><strong><code>product_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the product from metadata.</dd>
<dt><strong><code>last_scraped_review_date</code></strong> :&ensp;<code>str</code></dt>
<dd>The last review date scraped as per database.</dd>
<dt><strong><code>no_of_reviews</code></strong> :&ensp;<code>int</code></dt>
<dd>No.of reviews a product has.</dd>
<dt><strong><code>incremental</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to scrape reviews incrementally from last scraped review date. Defaults to True.</dd>
<dt><strong><code>reviews</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Empty intermediate list to store data during parallel crawl. Defaults to []</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>List of dictionaries containing all the scraped reviews of a product.</dd>
</dl></div>
</dd>
<dt id="meiyume.bts.crawler.DetailReview.terminate_logging"><code class="name flex">
<span>def <span class="ident">terminate_logging</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="meiyume.utils.Boots" href="../utils.html#meiyume.utils.Boots">Boots</a></b></code>:
<ul class="hlist">
<li><code><a title="meiyume.utils.Boots.open_browser" href="../utils.html#meiyume.utils.Browser.open_browser">open_browser</a></code></li>
<li><code><a title="meiyume.utils.Boots.open_browser_firefox" href="../utils.html#meiyume.utils.Browser.open_browser_firefox">open_browser_firefox</a></code></li>
<li><code><a title="meiyume.utils.Boots.scroll_down_page" href="../utils.html#meiyume.utils.Browser.scroll_down_page">scroll_down_page</a></code></li>
<li><code><a title="meiyume.utils.Boots.scroll_to_element" href="../utils.html#meiyume.utils.Browser.scroll_to_element">scroll_to_element</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="meiyume.bts.crawler.Metadata"><code class="flex name class">
<span>class <span class="ident">Metadata</span></span>
<span>(</span><span>log: bool = True, path: pathlib.Path = WindowsPath('D:/Amit/Meiyume/meiyume_master_source_codes'))</span>
</code></dt>
<dd>
<div class="desc"><p>Metadata extracts product metadata such as product page url, prices and brand from Boots website.</p>
<p>The Metadata class begins the data crawling process and all other stages depend on
the product urls extracted by Metadata class.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>Boots</code></strong> :&ensp;<code>Browser</code></dt>
<dd>Class that initializes folder paths and selenium webdriver for data scraping.</dd>
</dl>
<p><strong>init</strong> Metadata class instace initializer.</p>
<p>This method sets all the folder paths required for Metadata crawler to work.
If the paths does not exist the paths get automatically created depending on current directory or
provided directory.</p>
<h2 id="args_1">Args</h2>
<dl>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to create crawling exception and progess log. Defaults to True.</dd>
<dt><strong><code>path</code></strong> :&ensp;<code>Path</code>, optional</dt>
<dd>Folder path where the Metadata will be extracted. Defaults to
current directory(Path.cwd()).</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="meiyume.utils.Boots" href="../utils.html#meiyume.utils.Boots">Boots</a></li>
<li><a title="meiyume.utils.Browser" href="../utils.html#meiyume.utils.Browser">Browser</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="meiyume.bts.crawler.Metadata.base_url"><code class="name">var <span class="ident">base_url</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="meiyume.bts.crawler.Metadata.info"><code class="name">var <span class="ident">info</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="meiyume.bts.crawler.Metadata.source"><code class="name">var <span class="ident">source</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="meiyume.bts.crawler.Metadata.update_base_url"><code class="name flex">
<span>def <span class="ident">update_base_url</span></span>(<span>url: str) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>update_base_url defines the parent url from where the data scraping process will begin.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>url</code></strong> :&ensp;<code>str</code></dt>
<dd>The URL from which the spider will enter the website.</dd>
</dl></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="meiyume.bts.crawler.Metadata.extract"><code class="name flex">
<span>def <span class="ident">extract</span></span>(<span>self, download: bool = True, fresh_start: bool = False, auto_fresh_start: bool = False, n_workers: int = 5, open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = True, start_idx: Union[int, NoneType] = None, end_idx: Union[int, NoneType] = None, list_of_index=None, clean: bool = True, compile_progress_files: bool = False, delete_progress: bool = False) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>extract method controls all properties of the spiders and runs multi-threaded web crawling.</p>
<p>Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
This method has four major functionality:
* 1. Run the spider
* 2. Store data in regular intervals to free up ram
* 3. Compile all crawled data into one file.
* 4. Clean and push cleaned data to S3 storage for further algorithmic processing.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>download</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to crawl data from or compile crawled data into one file. Defaults to True.</dd>
<dt><strong><code>fresh_start</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to continue last crawl job or start new one. Defaults to False.</dd>
<dt><strong><code>auto_fresh_start</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to automatically start a new crawl job if last job was finished.
Defaults to False.</dd>
<dt><strong><code>n_workers</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>No. of parallel threads to run. Defaults to 5.</dd>
<dt><strong><code>open_headless</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to open browser headless. Defaults to False.</dd>
<dt><strong><code>open_with_proxy_server</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use ip rotation service. Defaults to True.</dd>
<dt><strong><code>randomize_proxy_usage</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use both proxy and native network in tandem to decrease proxy requests.
Defaults to True.</dd>
<dt><strong><code>start_idx</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>Starting index of the links to crawl. Defaults to None.</dd>
<dt><strong><code>end_idx</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>Ending index of the links to crawl. Defaults to None.</dd>
<dt><strong><code>list_of_index</code></strong> :&ensp;<code>[type]</code>, optional</dt>
<dd>List of indices or range of indices of product urls to scrape. Defaults to None.</dd>
<dt><strong><code>compile_progress_files</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to combine crawled data into one file. Defaults to False.</dd>
<dt><strong><code>clean</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to clean the compiled data. Defaults to True.</dd>
<dt><strong><code>delete_progress</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to delete intermediate data after compilation into one file. Defaults to False.</dd>
</dl></div>
</dd>
<dt id="meiyume.bts.crawler.Metadata.get_metadata"><code class="name flex">
<span>def <span class="ident">get_metadata</span></span>(<span>self, indices: Union[list, range], open_headless: bool, open_with_proxy_server: bool, randomize_proxy_usage: bool, product_meta_data: list = [])</span>
</code></dt>
<dd>
<div class="desc"><p>get_metadata Crawls product listing pages for price, name, brand etc.</p>
<p>Get Metadata crawls a product type page for example lipstick.
The function gets individual product urls, names, brands and prices etc. and stores
in a relational table structure to use later to download product images, scrape reviews and
other specific information.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>indices</code></strong> :&ensp;<code>Union[list, range]</code></dt>
<dd>list of indices or range of indices of product urls to scrape.</dd>
<dt><strong><code>open_headless</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to open browser headless.</dd>
<dt><strong><code>open_with_proxy_server</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use proxy server.</dd>
<dt><strong><code>randomize_proxy_usage</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use both proxy and native network in tandem to decrease proxy requests.</dd>
<dt><strong><code>product_meta_data</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Empty intermediate list to store product metadata during parallel crawl. Defaults to [].</dd>
</dl></div>
</dd>
<dt id="meiyume.bts.crawler.Metadata.terminate_logging"><code class="name flex">
<span>def <span class="ident">terminate_logging</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="meiyume.utils.Boots" href="../utils.html#meiyume.utils.Boots">Boots</a></b></code>:
<ul class="hlist">
<li><code><a title="meiyume.utils.Boots.open_browser" href="../utils.html#meiyume.utils.Browser.open_browser">open_browser</a></code></li>
<li><code><a title="meiyume.utils.Boots.open_browser_firefox" href="../utils.html#meiyume.utils.Browser.open_browser_firefox">open_browser_firefox</a></code></li>
<li><code><a title="meiyume.utils.Boots.scroll_down_page" href="../utils.html#meiyume.utils.Browser.scroll_down_page">scroll_down_page</a></code></li>
<li><code><a title="meiyume.utils.Boots.scroll_to_element" href="../utils.html#meiyume.utils.Browser.scroll_to_element">scroll_to_element</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="meiyume.bts" href="index.html">meiyume.bts</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="meiyume.bts.crawler.DetailReview" href="#meiyume.bts.crawler.DetailReview">DetailReview</a></code></h4>
<ul class="">
<li><code><a title="meiyume.bts.crawler.DetailReview.crawl_page" href="#meiyume.bts.crawler.DetailReview.crawl_page">crawl_page</a></code></li>
<li><code><a title="meiyume.bts.crawler.DetailReview.extract" href="#meiyume.bts.crawler.DetailReview.extract">extract</a></code></li>
<li><code><a title="meiyume.bts.crawler.DetailReview.get_details" href="#meiyume.bts.crawler.DetailReview.get_details">get_details</a></code></li>
<li><code><a title="meiyume.bts.crawler.DetailReview.get_reviews" href="#meiyume.bts.crawler.DetailReview.get_reviews">get_reviews</a></code></li>
<li><code><a title="meiyume.bts.crawler.DetailReview.terminate_logging" href="#meiyume.bts.crawler.DetailReview.terminate_logging">terminate_logging</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="meiyume.bts.crawler.Metadata" href="#meiyume.bts.crawler.Metadata">Metadata</a></code></h4>
<ul class="two-column">
<li><code><a title="meiyume.bts.crawler.Metadata.base_url" href="#meiyume.bts.crawler.Metadata.base_url">base_url</a></code></li>
<li><code><a title="meiyume.bts.crawler.Metadata.extract" href="#meiyume.bts.crawler.Metadata.extract">extract</a></code></li>
<li><code><a title="meiyume.bts.crawler.Metadata.get_metadata" href="#meiyume.bts.crawler.Metadata.get_metadata">get_metadata</a></code></li>
<li><code><a title="meiyume.bts.crawler.Metadata.info" href="#meiyume.bts.crawler.Metadata.info">info</a></code></li>
<li><code><a title="meiyume.bts.crawler.Metadata.source" href="#meiyume.bts.crawler.Metadata.source">source</a></code></li>
<li><code><a title="meiyume.bts.crawler.Metadata.terminate_logging" href="#meiyume.bts.crawler.Metadata.terminate_logging">terminate_logging</a></code></li>
<li><code><a title="meiyume.bts.crawler.Metadata.update_base_url" href="#meiyume.bts.crawler.Metadata.update_base_url">update_base_url</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>