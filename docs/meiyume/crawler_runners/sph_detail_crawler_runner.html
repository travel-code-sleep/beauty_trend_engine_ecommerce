<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>meiyume.crawler_runners.sph_detail_crawler_runner API documentation</title>
<meta name="description" content="This script runs spider to grab sephora metadata." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>meiyume.crawler_runners.sph_detail_crawler_runner</code></h1>
</header>
<section id="section-intro">
<p>This script runs spider to grab sephora metadata.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;This script runs spider to grab sephora metadata.&#34;&#34;&#34;
import gc
import os
import sys
import time
import warnings
from pathlib import Path

import pandas as pd
from meiyume.sph.crawler import Detail
from meiyume.algorithms import SexyMetaDetail, SexyIngredient
from meiyume.utils import chunks, ranges

warnings.simplefilter(action=&#39;ignore&#39;)

open_with_proxy_server = True


def exclude_scraped_products_from_tracker(detail_crawler: Detail, reset_na: bool = False) -&gt; pd.DataFrame:
    &#34;&#34;&#34;exclude_scraped_products_from_tracker [summary]
    [extended_summary]
    Args:
        detail_crawler (Detail): [description]
        reset_na (bool, optional): [description]. Defaults to False.
    Returns:
        pd.DataFrame: [description]
    &#34;&#34;&#34;
    progress_tracker = pd.read_feather(
        detail_crawler.detail_path/&#39;sph_detail_progress_tracker&#39;)

    if reset_na:
        progress_tracker.detail_scraped[progress_tracker.detail_scraped == &#39;NA&#39;] = &#39;N&#39;

    progress_tracker = progress_tracker[~progress_tracker.detail_scraped.isna(
    )]
    progress_tracker = progress_tracker[progress_tracker.detail_scraped != &#39;Y&#39;]
    progress_tracker = progress_tracker.sample(frac=1).reset_index(drop=True)
    progress_tracker.to_feather(
        detail_crawler.detail_path/&#39;sph_detail_progress_tracker&#39;)
    return progress_tracker


def run_detail_crawler(meta_df: pd.DataFrame, detail_crawler: Detail):
    &#34;&#34;&#34;run_detail_crawler [summary]
    [extended_summary]
    Args:
        meta_df (pd.DataFrame): [description]
        detail_crawler (Detail): [description]
    &#34;&#34;&#34;
    for i in ranges(meta_df.shape[0], 30):
        print(i[0], i[-1])
        if i[0] == 0:
            fresh_start = False
            auto_fresh_start = False
        else:
            fresh_start = False
            auto_fresh_start = False
        detail_crawler.extract(metadata=meta_df, download=True, n_workers=8,
                               fresh_start=fresh_start, auto_fresh_start=auto_fresh_start,
                               start_idx=i[0], end_idx=i[-1],
                               open_headless=False, open_with_proxy_server=open_with_proxy_server,
                               randomize_proxy_usage=True,
                               compile_progress_files=False, clean=False, delete_progress=False)

        detail_crawler.terminate_logging()
        del detail_crawler
        gc.collect()
        time.sleep(5)
        detail_crawler = Detail(
            path=&#34;D:/Amit/Meiyume/meiyume_data/spider_runner&#34;)

    progress_tracker = exclude_scraped_products_from_tracker(
        detail_crawler, reset_na=True)
    n_workers = 4
    trials = 10
    while progress_tracker.detail_scraped[progress_tracker.detail_scraped == &#39;N&#39;].count() != 0:
        detail_crawler.extract(metadata=meta_df, download=True, n_workers=n_workers,
                               fresh_start=False, auto_fresh_start=False,
                               open_headless=False, open_with_proxy_server=open_with_proxy_server,
                               randomize_proxy_usage=True,
                               compile_progress_files=False, clean=False, delete_progress=False)
        if trials &lt;= 4:
            reset_na = True
        else:
            reset_na = False
        progress_tracker = exclude_scraped_products_from_tracker(
            detail_crawler, reset_na=reset_na)

        trials -= 1
        if trials == 0:
            break

    detail_crawler.extract(metadata=None, download=False, fresh_start=False, auto_fresh_start=False,
                           compile_progress_files=True,  clean=True, delete_progress=True)
    # Path(detail_crawler.review_path/&#39;sph_review_progress_tracker.csv&#39;).unlink()
    detail_crawler.terminate_logging()
    del detail_crawler
    gc.collect()


if __name__ == &#34;__main__&#34;:
    detail_crawler = Detail(
        path=&#34;D:/Amit/Meiyume/meiyume_data/spider_runner&#34;)

    gecko_log_path = detail_crawler.detail_path/&#39;service/geckodriver.log&#39;
    if gecko_log_path.exists():
        gecko_log_path.unlink()

    files = list(detail_crawler.detail_crawler_trigger_path.glob(
        &#39;no_cat_cleaned_sph_product_metadata_all*&#39;))

    if len(files) &gt; 0:
        meta_df = pd.read_feather(files[0])

        run_detail_crawler(meta_df=meta_df, detail_crawler=detail_crawler)

        # Path(files[0]).unlink()

        meta_ranker = SexyMetaDetail(
            path=&#39;D:/Amit/Meiyume/meiyume_data/spider_runner&#39;)
        meta_detail = meta_ranker.make(source=&#39;sph&#39;)

        del meta_detail
        gc.collect()

        sexy_ing = SexyIngredient(
            path=&#39;D:/Amit/Meiyume/meiyume_data/spider_runner&#39;)
        ing = sexy_ing.make(source=&#39;sph&#39;)

        del ing
        gc.collect()

        if gecko_log_path.exists():
            gecko_log_path.unlink()
    else:
        print(&#39;*Metadata Trigger File Not Found.*&#39;)
        sys.exit(1)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="meiyume.crawler_runners.sph_detail_crawler_runner.exclude_scraped_products_from_tracker"><code class="name flex">
<span>def <span class="ident">exclude_scraped_products_from_tracker</span></span>(<span>detail_crawler: <a title="meiyume.sph.crawler.Detail" href="../sph/crawler.html#meiyume.sph.crawler.Detail">Detail</a>, reset_na: bool = False) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>exclude_scraped_products_from_tracker [summary]
[extended_summary]</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>detail_crawler</code></strong> :&ensp;<code>Detail</code></dt>
<dd>[description]</dd>
<dt><strong><code>reset_na</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>[description]. Defaults to False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>[description]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def exclude_scraped_products_from_tracker(detail_crawler: Detail, reset_na: bool = False) -&gt; pd.DataFrame:
    &#34;&#34;&#34;exclude_scraped_products_from_tracker [summary]
    [extended_summary]
    Args:
        detail_crawler (Detail): [description]
        reset_na (bool, optional): [description]. Defaults to False.
    Returns:
        pd.DataFrame: [description]
    &#34;&#34;&#34;
    progress_tracker = pd.read_feather(
        detail_crawler.detail_path/&#39;sph_detail_progress_tracker&#39;)

    if reset_na:
        progress_tracker.detail_scraped[progress_tracker.detail_scraped == &#39;NA&#39;] = &#39;N&#39;

    progress_tracker = progress_tracker[~progress_tracker.detail_scraped.isna(
    )]
    progress_tracker = progress_tracker[progress_tracker.detail_scraped != &#39;Y&#39;]
    progress_tracker = progress_tracker.sample(frac=1).reset_index(drop=True)
    progress_tracker.to_feather(
        detail_crawler.detail_path/&#39;sph_detail_progress_tracker&#39;)
    return progress_tracker</code></pre>
</details>
</dd>
<dt id="meiyume.crawler_runners.sph_detail_crawler_runner.run_detail_crawler"><code class="name flex">
<span>def <span class="ident">run_detail_crawler</span></span>(<span>meta_df: pandas.core.frame.DataFrame, detail_crawler: <a title="meiyume.sph.crawler.Detail" href="../sph/crawler.html#meiyume.sph.crawler.Detail">Detail</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>run_detail_crawler [summary]
[extended_summary]</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>meta_df</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>[description]</dd>
<dt><strong><code>detail_crawler</code></strong> :&ensp;<code>Detail</code></dt>
<dd>[description]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_detail_crawler(meta_df: pd.DataFrame, detail_crawler: Detail):
    &#34;&#34;&#34;run_detail_crawler [summary]
    [extended_summary]
    Args:
        meta_df (pd.DataFrame): [description]
        detail_crawler (Detail): [description]
    &#34;&#34;&#34;
    for i in ranges(meta_df.shape[0], 30):
        print(i[0], i[-1])
        if i[0] == 0:
            fresh_start = False
            auto_fresh_start = False
        else:
            fresh_start = False
            auto_fresh_start = False
        detail_crawler.extract(metadata=meta_df, download=True, n_workers=8,
                               fresh_start=fresh_start, auto_fresh_start=auto_fresh_start,
                               start_idx=i[0], end_idx=i[-1],
                               open_headless=False, open_with_proxy_server=open_with_proxy_server,
                               randomize_proxy_usage=True,
                               compile_progress_files=False, clean=False, delete_progress=False)

        detail_crawler.terminate_logging()
        del detail_crawler
        gc.collect()
        time.sleep(5)
        detail_crawler = Detail(
            path=&#34;D:/Amit/Meiyume/meiyume_data/spider_runner&#34;)

    progress_tracker = exclude_scraped_products_from_tracker(
        detail_crawler, reset_na=True)
    n_workers = 4
    trials = 10
    while progress_tracker.detail_scraped[progress_tracker.detail_scraped == &#39;N&#39;].count() != 0:
        detail_crawler.extract(metadata=meta_df, download=True, n_workers=n_workers,
                               fresh_start=False, auto_fresh_start=False,
                               open_headless=False, open_with_proxy_server=open_with_proxy_server,
                               randomize_proxy_usage=True,
                               compile_progress_files=False, clean=False, delete_progress=False)
        if trials &lt;= 4:
            reset_na = True
        else:
            reset_na = False
        progress_tracker = exclude_scraped_products_from_tracker(
            detail_crawler, reset_na=reset_na)

        trials -= 1
        if trials == 0:
            break

    detail_crawler.extract(metadata=None, download=False, fresh_start=False, auto_fresh_start=False,
                           compile_progress_files=True,  clean=True, delete_progress=True)
    # Path(detail_crawler.review_path/&#39;sph_review_progress_tracker.csv&#39;).unlink()
    detail_crawler.terminate_logging()
    del detail_crawler
    gc.collect()</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="meiyume.crawler_runners" href="index.html">meiyume.crawler_runners</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="meiyume.crawler_runners.sph_detail_crawler_runner.exclude_scraped_products_from_tracker" href="#meiyume.crawler_runners.sph_detail_crawler_runner.exclude_scraped_products_from_tracker">exclude_scraped_products_from_tracker</a></code></li>
<li><code><a title="meiyume.crawler_runners.sph_detail_crawler_runner.run_detail_crawler" href="#meiyume.crawler_runners.sph_detail_crawler_runner.run_detail_crawler">run_detail_crawler</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>