<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>meiyume.sph.crawler API documentation</title>
<meta name="description" content="The module to crawl Sephora website data." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>meiyume.sph.crawler</code></h1>
</header>
<section id="section-intro">
<p>The module to crawl Sephora website data.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;The module to crawl Sephora website data.&#34;&#34;&#34;
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from typing import *
import sys
import types
import pdb
import gc
import concurrent.futures
import os
import shutil
import time
from datetime import datetime, timedelta
import warnings
from pathlib import Path
from typing import *
import numpy as np
import pandas as pd
import tldextract
from selenium import webdriver
from selenium.common.exceptions import (ElementClickInterceptedException,
                                        NoSuchElementException,
                                        StaleElementReferenceException,
                                        TimeoutException)
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.remote.webelement import WebElement
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.alert import Alert

from meiyume.cleaner_plus import Cleaner
from meiyume.utils import (Browser, Logger, MeiyumeException, Sephora,
                           accept_alert, close_popups, log_exception,
                           chunks, ranges, convert_ago_to_date)

warnings.simplefilter(action=&#39;ignore&#39;, category=FutureWarning)


class Metadata(Sephora):
    &#34;&#34;&#34;Metadata extracts product metadata such as product page url, prices and brand from Sephora website.

    The Metadata class begins the data crawling process and all other stages depend on the product urls extracted by Metadata class.

    Args:
        Sephora (Browser): Class that initializes folder paths and selenium webdriver for data scraping.

    &#34;&#34;&#34;

    base_url = &#34;https://www.sephora.com&#34;
    info = tldextract.extract(base_url)
    source = info.registered_domain

    @classmethod
    def update_base_url(cls, url: str) -&gt; None:
        &#34;&#34;&#34;update_base_url defines the parent url from where the data scraping process will begin.

        Args:
            url (str): The URL from which the spider will enter the website.

        &#34;&#34;&#34;
        cls.base_url = url
        cls.info = tldextract.extract(cls.base_url)
        cls.source = cls.info.registered_domain

    def __init__(self, log: bool = True, path: Path = Path.cwd()):
        &#34;&#34;&#34;__init__ Metadata class instace initializer.

        This method sets all the folder paths required for Metadata crawler to work.
        If the paths does not exist the paths get automatically created depending on current directory
        or provided directory.

        Args:
            log (bool, optional): Whether to create crawling exception and progess log. Defaults to True.
            path (Path, optional): Folder path where the Metadata will be extracted. Defaults to current directory(Path.cwd()).

        &#34;&#34;&#34;

        super().__init__(path=path, data_def=&#39;meta&#39;)
        self.path = path
        self.current_progress_path = self.metadata_path/&#39;current_progress&#39;
        self.current_progress_path.mkdir(parents=True, exist_ok=True)

        # move old raw and clean files to old folder
        old_metadata_files = list(self.metadata_path.glob(
            &#39;sph_product_metadata_all*&#39;))
        for f in old_metadata_files:
            shutil.move(str(f), str(self.old_metadata_files_path))

        old_clean_metadata_files = os.listdir(self.metadata_clean_path)
        for f in old_clean_metadata_files:
            shutil.move(str(self.metadata_clean_path/f),
                        str(self.old_metadata_clean_files_path))
        # set logger
        if log:
            self.prod_meta_log = Logger(
                &#34;sph_prod_metadata_extraction&#34;, path=self.crawl_log_path)
            self.logger, _ = self.prod_meta_log.start_log()

    def get_product_type_urls(self, open_headless: bool, open_with_proxy_server: bool) -&gt; pd.DataFrame:
        &#34;&#34;&#34;get_product_type_urls Extract the category/subcategory structure and urls to extract the products of those category/subcategory.

        Extracts the links of pages containing the list of all products structured into
        category/subcategory/product type to effectively stored in relational database.
        Defines the structure of data extraction that helps store unstructured data in a structured manner.

        Args:
            open_headless (bool): Whether to open browser headless.
            open_with_proxy_server (bool): Whether to use proxy server.

        Returns:
            pd.DataFrame: returns pandas dataframe containing urls for getting list of products, category, subcategory etc.

        &#34;&#34;&#34;
        # create webdriver instance
        drv = self.open_browser(
            open_headless=open_headless, open_with_proxy_server=open_with_proxy_server, path=self.metadata_path)

        drv.get(self.base_url)
        time.sleep(15)
        # click and close welcome forms
        accept_alert(drv, 10)
        close_popups(drv)

        cats = drv.find_elements_by_css_selector(&#39;a[id^=&#34;top_nav_drop_&#34;]&#39;)
        cat_urls = []
        for c in cats:
            if c.get_attribute(&#39;href&#39;) is not None:
                cat_name, url = (c.get_attribute(&#34;href&#34;).split(&#34;/&#34;)
                                 [-1], c.get_attribute(&#34;href&#34;))
                cat_urls.append((cat_name, url))
                self.logger.info(str.encode(f&#39;Category:- name:{cat_name}, \
                                          url:{url}&#39;, &#34;utf-8&#34;, &#34;ignore&#34;))

        sub_cat_urls = []
        for cu in cat_urls:
            cat_name = cu[0]
            if cat_name in [&#39;brands-list&#39;, &#39;new-beauty-products&#39;, &#39;sephora-collection&#39;]:
                continue
            cat_url = cu[1]
            drv.get(cat_url)

            time.sleep(10)
            accept_alert(drv, 10)
            close_popups(drv)

            sub_cats = drv.find_elements_by_css_selector(
                &#39;a[data-at*=&#34;top_level_category&#34;]&#39;)
            # sub_cats.extend(drv.find_elements_by_class_name(&#34;css-or7ouu&#34;))
            if len(sub_cats) &gt; 0:
                for s in sub_cats:
                    sub_cat_urls.append((cat_name, s.get_attribute(
                        &#34;href&#34;).split(&#34;/&#34;)[-1], s.get_attribute(&#34;href&#34;)))
                    self.logger.info(str.encode(f&#39;SubCategory:- name:{s.get_attribute(&#34;href&#34;).split(&#34;/&#34;)[-1]},\
                                                  url:{s.get_attribute(&#34;href&#34;)}&#39;, &#34;utf-8&#34;, &#34;ignore&#34;))
            else:
                sub_cat_urls.append(
                    (cat_name, cat_url.split(&#39;/&#39;)[-1], cat_url))

        product_type_urls = []
        for su in sub_cat_urls:
            cat_name = su[0]
            sub_cat_name = su[1]
            if any(name in sub_cat_name for name in [&#39;best-selling&#39;, &#39;new&#39;, &#39;mini&#39;]):
                continue
            sub_cat_url = su[2]
            drv.get(sub_cat_url)

            time.sleep(10)
            accept_alert(drv, 10)
            close_popups(drv)

            product_types = drv.find_elements_by_css_selector(
                &#39;a[data-at*=&#34;nth_level&#34;]&#39;)
            if len(product_types) &gt; 0:
                for item in product_types:
                    product_type_urls.append((cat_name, sub_cat_name, item.get_attribute(&#34;href&#34;).split(&#34;/&#34;)[-1],
                                              item.get_attribute(&#34;href&#34;)))
                    self.logger.info(str.encode(f&#39;ProductType:- name:{item.get_attribute(&#34;href&#34;).split(&#34;/&#34;)[-1]},\
                                                  url:{item.get_attribute(&#34;href&#34;)}&#39;, &#34;utf-8&#34;, &#34;ignore&#34;))
            else:
                product_type_urls.append(
                    (cat_name, sub_cat_name, sub_cat_url.split(&#39;/&#39;)[-1], sub_cat_url))

        df = pd.DataFrame(product_type_urls, columns=[
                          &#39;category_raw&#39;, &#39;sub_category_raw&#39;, &#39;product_type&#39;, &#39;url&#39;])

        df_clean = pd.DataFrame(sub_cat_urls, columns=[
                                &#39;category_raw&#39;, &#39;product_type&#39;, &#39;url&#39;])
        df_clean[&#39;sub_category_raw&#39;] = &#39;CLEAN&#39;
        df_clean = df_clean[(df_clean.url.apply(
            lambda x: True if &#39;clean&#39; in x else False)) &amp; (df_clean.product_type != &#39;cleanser&#39;)]

        df_vegan = pd.DataFrame(sub_cat_urls, columns=[
                                &#39;category_raw&#39;, &#39;product_type&#39;, &#39;url&#39;])
        df_vegan[&#39;sub_category_raw&#39;] = &#39;VEGAN&#39;
        df_vegan = df_vegan[df_vegan.url.apply(
            lambda x: True if &#39;vegan&#39; in x.lower() else False)]

        df = pd.concat([df, df_clean, df_vegan], axis=0)
        df.reset_index(inplace=True, drop=True)
        df.to_feather(self.metadata_path/&#39;sph_product_cat_subcat_structure&#39;)
        drv.quit()

        df.drop_duplicates(subset=&#39;url&#39;, inplace=True)
        df.drop(columns=&#39;sub_category_raw&#39;, inplace=True)
        df.reset_index(inplace=True, drop=True)
        df[&#39;scraped&#39;] = &#39;N&#39;
        df.to_feather(self.metadata_path/f&#39;sph_product_type_urls_to_extract&#39;)
        return df

    def get_metadata(self, indices: Union[list, range],
                     open_headless: bool, open_with_proxy_server: bool,
                     randomize_proxy_usage: bool,
                     product_meta_data: list = []) -&gt; None:
        &#34;&#34;&#34;get_metadata Crawls product listing pages for price, name, brand etc.

        Get Metadata crawls a product type page for example lipstick.
        The function gets individual product urls, names, brands and prices etc. and stores
        in a relational table structure to use later to download product images, scrape reviews and
        other specific information.

        Args:
            indices (Union[list, range]): list of indices or range of indices of product urls to scrape.
            open_headless (bool): Whether to open browser headless.
            open_with_proxy_server (bool): Whether to use proxy server.
            randomize_proxy_usage (bool): Whether to use both proxy and native network in tandem to decrease proxy requests.
            product_meta_data (list, optional): Empty intermediate list to store product metadata during parallel crawl. Defaults to [].

        &#34;&#34;&#34;
        for pt in self.product_type_urls.index[self.product_type_urls.index.isin(indices)]:
            cat_name = self.product_type_urls.loc[pt, &#39;category_raw&#39;]
            product_type = self.product_type_urls.loc[pt, &#39;product_type&#39;]
            product_type_link = self.product_type_urls.loc[pt, &#39;url&#39;]

            self.progress_tracker.loc[pt, &#39;product_type&#39;] = product_type
            # print(product_type_link)
            if &#39;best-selling&#39; in product_type.lower() or &#39;new&#39; in product_type.lower():
                self.progress_tracker.loc[pt, &#39;scraped&#39;] = &#39;NA&#39;
                continue

            if randomize_proxy_usage:
                use_proxy = np.random.choice([True, False])
            else:
                use_proxy = True
            if open_with_proxy_server:
                # print(use_proxy)
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
                                        path=self.metadata_path)
            else:
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
                                        path=self.metadata_path)

            drv.get(product_type_link)
            time.sleep(15)  # 30
            accept_alert(drv, 10)
            close_popups(drv)

            try:
                chat_popup_button = WebDriverWait(drv, 3).until(
                    EC.presence_of_element_located((By.XPATH, &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)))
                chat_popup_button = drv.find_element_by_xpath(
                    &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)
                self.scroll_to_element(drv, chat_popup_button)
                ActionChains(drv).move_to_element(
                    chat_popup_button).click(chat_popup_button).perform()
            except TimeoutException:
                pass

            # sort by new products (required to get all new products properly)
            try:
                sort_dropdown = drv.find_element_by_css_selector(
                    &#39;button[id=&#34;cat_sort_menu_trigger&#34;]&#39;)
                Browser().scroll_to_element(drv, sort_dropdown)
                ActionChains(drv).move_to_element(
                    sort_dropdown).click(sort_dropdown).perform()
                drv.find_elements_by_css_selector(
                    &#39;div[id=&#34;cat_sort_menu&#34;]&gt;button&#39;)[2].click()
                time.sleep(10)
                # sort_dropdown = drv.find_element_by_class_name(&#39;css-16tfpwn&#39;)
                # self.scroll_to_element(drv, sort_dropdown)
                # ActionChains(drv).move_to_element(
                #     sort_dropdown).click(sort_dropdown).perform()
                # button = drv.find_element_by_xpath(
                #     &#39;//*[@id=&#34;cat_sort_menu&#34;]/button[3]&#39;)
                # drv.implicitly_wait(4)
                # self.scroll_to_element(drv, button)
                # ActionChains(drv).move_to_element(
                #     button).click(button).perform()
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod Type: {product_type}&#39;)
                self.logger.info(str.encode(
                    f&#39;Category: {cat_name} - ProductType {product_type} cannot sort by NEW.(page link: {product_type_link})&#39;,
                    &#39;utf-8&#39;, &#39;ignore&#39;))
                pass

            # load all the products
            self.scroll_down_page(drv, h2=0.8, speed=3)
            time.sleep(8)

            # check whether on the first page of product type
            try:
                close_popups(drv)
                accept_alert(drv, 2)
                current_page = drv.find_element_by_class_name(
                    &#39;css-g48inl&#39;).text
            except NoSuchElementException as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod Type: {product_type}&#39;)
                self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType {product_type} has\
                only one page of products.(page link: {product_type_link})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                one_page = True
                current_page = 1
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod Type: {product_type}&#39;)
                self.progress_tracker.loc[pt, &#39;scraped&#39;] = &#39;NA&#39;
                self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType {product_type}\
                     page not found.(page link: {product_type_link})&#39;,
                                            &#39;utf-8&#39;, &#39;ignore&#39;))
            else:
                # get a list of all available pages
                one_page = False
                # get next page button
                next_page_button = drv.find_element_by_css_selector(
                    &#39;div&gt;nav&gt;ul&gt;button[aria-label=&#34;Next&#34;]&#39;)
                pages = []
                for page in drv.find_elements_by_class_name(&#39;css-1lk9n5p&#39;):
                    pages.append(page.text)

            # start getting product form each page
            while True:
                cp = 0
                self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type}\
                                  getting product from page {current_page}.(page link: {product_type_link})&#39;,
                                            &#39;utf-8&#39;, &#39;ignore&#39;))
                time.sleep(5)
                close_popups(drv)
                accept_alert(drv, 2)
                products = drv.find_elements_by_css_selector(
                    &#39;div[data-comp=&#34;ProductGrid &#34;]&gt;div&gt;div&gt;a&#39;)
                # print(len(products))

                for p in products:
                    time.sleep(0.5)

                    close_popups(drv)
                    accept_alert(drv, 0.5)

                    self.scroll_to_element(drv, p)
                    ActionChains(drv).move_to_element(p).perform()

                    try:
                        product_name = p.get_attribute(&#39;aria-label&#39;)
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                     product {products.index(p)} metadata extraction failed.\
                                                (page_link: {product_type_link} - page_no: {current_page})&#39;,
                                                    &#39;utf-8&#39;, &#39;ignore&#39;))
                        continue

                    try:
                        new_f = p.find_element_by_css_selector(
                            &#39;div[data-at=&#34;product_badges&#34;]&#39;).text
                        product_new_flag = &#39;NEW&#39;
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        product_new_flag = &#39;&#39;
                        # self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                        #                              product {products.index(p)} product_new_flag extraction failed.\
                        #                         (page_link: {product_type_link} - page_no: {current_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    try:
                        product_page = p.get_attribute(&#39;href&#39;)
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        product_page = &#39;&#39;
                        self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                     product {products.index(p)} product_page extraction failed.\
                                                (page_link: {product_type_link} - page_no: {current_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    try:
                        brand = p.find_element_by_css_selector(
                            &#39;span[data-at=&#34;sku_item_brand&#34;]&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        brand = &#39;&#39;
                        self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                     product {products.index(p)} brand extraction failed.\
                                                (page_link: {product_type_link} - page_no: {current_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    try:
                        rating = p.find_element_by_css_selector(
                            &#39;div[data-comp=&#34;StarRating &#34;]&#39;).get_attribute(&#39;aria-label&#39;)
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        rating = &#39;&#39;
                        self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                     product {products.index(p)} rating extraction failed.\
                                                (page_link: {product_type_link} - page_no: {current_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    try:
                        price = p.find_element_by_css_selector(
                            &#39;span[data-at=&#34;sku_item_price_list&#34;]&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        price = &#39;&#39;
                        self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                      product {products.index(p)} price extraction failed.\
                                                (page_link: {product_type_link} - page_no: {current_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

                    if datetime.now().day &lt; 15:
                        meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
                    else:
                        meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;

                    product_data_dict = {&#34;product_name&#34;: product_name, &#34;product_page&#34;: product_page, &#34;brand&#34;: brand, &#34;price&#34;: price,
                                         &#34;rating&#34;: rating, &#34;category&#34;: cat_name, &#34;product_type&#34;: product_type, &#34;new_flag&#34;: product_new_flag,
                                         &#34;complete_scrape_flag&#34;: &#34;N&#34;, &#34;meta_date&#34;: meta_date}
                    cp += 1
                    self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                 Product: {product_name} - {cp} extracted successfully.\
                                                (page_link: {product_type_link} - page_no: {current_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    product_meta_data.append(product_data_dict)

                if one_page:
                    break
                elif int(current_page) == int(pages[-1]):
                    self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} extraction complete.\
                                                (page_link: {product_type_link} - page_no: {current_page})&#39;,
                                                &#39;utf-8&#39;, &#39;ignore&#39;))
                    break
                else:
                    # go to next page
                    try:
                        self.scroll_to_element(drv, next_page_button)
                        ActionChains(drv).move_to_element(
                            next_page_button).click(next_page_button).perform()
                        time.sleep(15)
                        accept_alert(drv, 10)
                        close_popups(drv)
                        self.scroll_down_page(drv, h2=0.8, speed=3)
                        time.sleep(10)
                        current_page = drv.find_element_by_class_name(
                            &#39;css-g48inl&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        self.logger.info(str.encode(f&#39;Page navigation issue occurred for Category: {cat_name} - \
                                                        ProductType: {product_type} (page_link: {product_type_link} \
                                                        - page_no: {current_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                        break
            drv.quit()

            if len(product_meta_data) &gt; 0:
                product_meta_df = pd.DataFrame(product_meta_data)
                product_meta_df.to_feather(
                    self.current_progress_path/f&#39;sph_prod_meta_extract_progress_{product_type}_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}&#39;)
                self.logger.info(
                    f&#39;Completed till IndexPosition: {pt} - ProductType: {product_type}. (URL:{product_type_link})&#39;)
                self.progress_tracker.loc[pt, &#39;scraped&#39;] = &#39;Y&#39;
                self.progress_tracker.to_feather(
                    self.metadata_path/&#39;sph_metadata_progress_tracker&#39;)
                product_meta_data = []
        self.logger.info(&#39;Metadata Extraction Complete&#39;)
        print(&#39;Metadata Extraction Complete&#39;)

    def extract(self, download: bool = True, fresh_start: bool = False, auto_fresh_start: bool = False, n_workers: int = 5,
                open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = True,
                start_idx: Optional[int] = None, end_idx: Optional[int] = None, list_of_index=None,
                compile_progress_files: bool = False, clean: bool = True, delete_progress: bool = False) -&gt; None:
        &#34;&#34;&#34;extract method controls all properties of the spiders and runs multi-threaded web crawling.

        Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
        This method has four major functionality:
        * 1. Run the spider
        * 2. Store data in regular intervals to free up ram
        * 3. Compile all crawled data into one file.
        * 4. Clean and push cleaned data to S3 storage for further algorithmic processing.

        Args:
            download (bool, optional): Whether to crawl data from or compile crawled data into one file. Defaults to True.
            fresh_start (bool, optional): Whether to continue last crawl job or start new one. Defaults to False.
            auto_fresh_start (bool, optional): Whether to automatically start a new crawl job if last job was finished.
                                               Defaults to False.
            n_workers (int, optional): No. of parallel threads to run. Defaults to 5.
            open_headless (bool, optional): Whether to open browser headless. Defaults to False.
            open_with_proxy_server (bool, optional): Whether to use ip rotation service. Defaults to True.
            randomize_proxy_usage (bool, optional): Whether to use both proxy and native network in tandem to decrease proxy requests.
                                                    Defaults to True.
            start_idx (Optional[int], optional): Starting index of the links to crawl. Defaults to None.
            end_idx (Optional[int], optional): Ending index of the links to crawl. Defaults to None.
            list_of_index ([type], optional): List of indices or range of indices of product urls to scrape. Defaults to None.
            compile_progress_files (bool, optional): Whether to combine crawled data into one file. Defaults to False.
            clean (bool, optional): Whether to clean the compiled data. Defaults to True.
            delete_progress (bool, optional): Whether to delete intermediate data after compilation into one file. Defaults to False.
        &#34;&#34;&#34;
        def fresh():
            &#34;&#34;&#34; If fresh_start is True, this function sets initial parameters for a fresh data crawl.
            &#34;&#34;&#34;
            self.product_type_urls = self.get_product_type_urls(open_headless=open_headless,
                                                                open_with_proxy_server=open_with_proxy_server)
            # progress tracker: captures scraped and error desc
            self.progress_tracker = pd.DataFrame(index=self.product_type_urls.index, columns=[
                &#39;product_type&#39;, &#39;scraped&#39;, &#39;error_desc&#39;])
            self.progress_tracker.scraped = &#39;N&#39;

        if fresh_start:
            self.logger.info(&#39;Starting Fresh Extraction.&#39;)
            fresh()
        else:
            if Path(self.metadata_path/&#39;sph_product_type_urls_to_extract&#39;).exists():
                self.product_type_urls = pd.read_feather(
                    self.metadata_path/&#39;sph_product_type_urls_to_extract&#39;)
                if Path(self.metadata_path/&#39;sph_metadata_progress_tracker&#39;).exists():
                    self.progress_tracker = pd.read_feather(
                        self.metadata_path/&#39;sph_metadata_progress_tracker&#39;)
                else:
                    self.progress_tracker = pd.DataFrame(index=self.product_type_urls.index, columns=[
                        &#39;product_type&#39;, &#39;scraped&#39;, &#39;error_desc&#39;])
                    self.progress_tracker.scraped = &#39;N&#39;
                    self.progress_tracker.to_feather(
                        self.metadata_path/&#39;sph_metadata_progress_tracker&#39;)
                if sum(self.progress_tracker.scraped == &#39;N&#39;) &gt; 0:
                    self.logger.info(
                        &#39;Continuing Metadata Extraction From Last Run.&#39;)
                    self.product_type_urls = self.product_type_urls[self.product_type_urls.index.isin(
                        self.progress_tracker.index[self.progress_tracker.scraped == &#39;N&#39;].values.tolist())]
                else:
                    if auto_fresh_start:
                        self.logger.info(
                            &#39;Previous Run Was Complete. Starting Fresh Extraction.&#39;)
                        fresh()
                    else:
                        self.logger.info(
                            &#39;Previous Run is Complete.&#39;)
            else:
                self.logger.info(
                    &#39;URL File Not Found. Start Fresh Extraction.&#39;)
        # print(self.progress_tracker)
        if download:
            # set list or range of product indices to crawl
            if list_of_index:
                indices = list_of_index
            elif start_idx and end_idx is None:
                indices = range(start_idx, len(self.product_type_urls))
            elif start_idx is None and end_idx:
                indices = range(0, end_idx)
            elif start_idx is not None and end_idx is not None:
                indices = range(start_idx, end_idx)
            else:
                indices = range(len(self.product_type_urls))
            # print(indices)
            if list_of_index:
                self.get_metadata(indices=list_of_index,
                                  open_headless=open_headless,
                                  open_with_proxy_server=open_with_proxy_server,
                                  randomize_proxy_usage=randomize_proxy_usage,
                                  product_meta_data=[])
            else:
                &#39;&#39;&#39;
                # review_Data and item_data are lists of empty lists so that each namepace of function call will
                # have its separate detail_data
                # list to strore scraped dictionaries. will save memory(ram/hard-disk) consumption. will stop data duplication
                &#39;&#39;&#39;
                if start_idx:
                    lst_of_lst = ranges(
                        indices[-1]+1, n_workers, start_idx=start_idx)
                else:
                    lst_of_lst = ranges(len(indices), n_workers)
                print(lst_of_lst)
                headless = [open_headless for i in lst_of_lst]
                proxy = [open_with_proxy_server for i in lst_of_lst]
                rand_proxy = [randomize_proxy_usage for i in lst_of_lst]
                product_meta_data = [[] for i in lst_of_lst]
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    &#39;&#39;&#39;
                    # but each of the function namespace will be modifying only one metadata tracing file so that progress saving
                    # is tracked correctly. else multiple progress tracker file will be created with difficulty to combine correct
                    # progress information
                    &#39;&#39;&#39;
                    executor.map(self.get_metadata, lst_of_lst,
                                 headless, proxy, rand_proxy, product_meta_data)

        if compile_progress_files:
            self.logger.info(&#39;Creating Combined Metadata File&#39;)
            files = [f for f in self.current_progress_path.glob(
                &#34;sph_prod_meta_extract_progress_*&#34;)]
            li = [pd.read_feather(file) for file in files]
            metadata_df = pd.concat(li, axis=0, ignore_index=True)
            metadata_df.reset_index(inplace=True, drop=True)
            metadata_df[&#39;source&#39;] = self.source

            if datetime.now().day &lt; 15:
                meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
            else:
                meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;
            filename = f&#39;sph_product_metadata_all_{meta_date}&#39;
            metadata_df.to_feather(self.metadata_path/filename)

            self.logger.info(
                f&#39;Metadata file created. Please look for file {filename} in path {self.metadata_path}&#39;)
            print(
                f&#39;Metadata file created. Please look for file {filename} in path {self.metadata_path}&#39;)

            if clean:
                cleaner = Cleaner(path=self.path)
                _ = cleaner.clean(
                    data=self.metadata_path/filename)
                self.logger.info(
                    &#39;Metadata Cleaned and Removed Duplicates for Details/Review/Image Extraction.&#39;)

            if delete_progress:
                shutil.rmtree(
                    f&#39;{self.metadata_path}\\current_progress&#39;, ignore_errors=True)
                self.logger.info(&#39;Progress files deleted&#39;)

    def terminate_logging(self):
        &#34;&#34;&#34;terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.
        &#34;&#34;&#34;
        self.logger.handlers.clear()
        self.prod_meta_log.stop_log()

    # def extract_failed_pages(self):
    #     &#34;&#34;&#34;extract_failed_pages [summary]

    #     [extended_summary]
    #     &#34;&#34;&#34;
    #     pass


class Detail(Sephora):
    &#34;&#34;&#34;Detail extracts product details such as ingredients, price, size, color etc. from Sephora website.

    The Detail module utilizes product urls scraped by Metadata by opening the pages one by one and getting
    details for each individual products.

    Args:
        Sephora ([type]): Class that initializes folder paths and selenium webdriver for data scraping.
    &#34;&#34;&#34;

    def __init__(self, path: Path = Path.cwd(), log: bool = True):
        &#34;&#34;&#34;__init__ Deatil class instace initializer.

        This method sets all the folder paths required for Detail crawler to work.
        If the paths does not exist the paths get automatically created depending on
        current directory or provided directory.

        Args:
            log (bool, optional): Whether to create crawling exception and progess log. Defaults to True.
            path (Path, optional): Folder path where the Detail will be extracted. Defaults to current directory(Path.cwd()).
        &#34;&#34;&#34;
        super().__init__(path=path, data_def=&#39;detail&#39;)
        self.path = path
        self.current_progress_path = self.detail_path/&#39;current_progress&#39;
        self.current_progress_path.mkdir(parents=True, exist_ok=True)

        old_detail_files = list(self.detail_path.glob(
            &#39;sph_product_detail_all*&#39;)) + list(self.detail_path.glob(
                &#39;sph_product_item_all*&#39;))
        for f in old_detail_files:
            shutil.move(str(f), str(self.old_detail_files_path))

        old_clean_detail_files = files = os.listdir(self.detail_clean_path)
        for f in old_clean_detail_files:
            shutil.move(str(self.detail_clean_path/f),
                        str(self.old_detail_clean_files_path))
        # set logger
        if log:
            self.prod_detail_log = Logger(&#34;sph_prod_detail_extraction&#34;,
                                          path=self.crawl_log_path)
            self.logger, _ = self.prod_detail_log.start_log()

    def get_detail(self, indices: Union[list, range], open_headless: bool, open_with_proxy_server: bool,
                   randomize_proxy_usage: bool, detail_data: list = [],
                   item_df=pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                                 &#39;item_size&#39;, &#39;item_price&#39;,
                                                 &#39;item_ingredients&#39;])) -&gt; None:
        &#34;&#34;&#34;get_detail scrapes individual product pages for price, ingredients, color etc.

        Get Detail crawls product specific page and scrapes data such as ingredients, review rating distribution,
        size specific prices, color, product claims and other information pertaining to one individual product.

        Args:
            indices (Union[list, range]): list of indices or range of indices of product urls to scrape.
            open_headless (bool): Whether to open browser headless.
            open_with_proxy_server (bool): Whether to use ip rotation service.
            randomize_proxy_usage (bool): Whether to use both proxy and native network in tandem to decrease proxy requests.
            detail_data (list, optional): Empty intermediate list to store data during parallel crawl. Defaults to [].
            item_df ([type], optional): Empty intermediate dataframe to store data during parallel crawl.
                                        Defaults to []. Defaults to pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                                                                         &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;]).
        &#34;&#34;&#34;
        def store_data_refresh_mem(detail_data: list, item_df: pd.DataFrame) -&gt; Tuple[list, pd.DataFrame]:
            &#34;&#34;&#34;store_data_refresh_mem method stores crawled data in regular interval to free up system memory.

            Store data after ten products are extracted every time to free up RAM.

            Args:
                detail_data (list): List containing crawled detail data to store.
                item_df (pd.DataFrame): Dataframe containing scraped item data to store.

            Returns:
                Tuple[list, pd.DataFrame]: Empty list and dataframe to accumulate data from next ten products scrape.
            &#34;&#34;&#34;
            pd.DataFrame(detail_data).to_csv(self.current_progress_path /
                                             f&#39;sph_prod_detail_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                                             index=None)
            item_df.reset_index(inplace=True, drop=True)
            item_df.to_csv(self.current_progress_path /
                           f&#39;sph_prod_item_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                           index=None)
            item_df = pd.DataFrame(columns=[
                                   &#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;, &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;])
            self.meta.to_feather(
                self.detail_path/&#39;sph_detail_progress_tracker&#39;)
            return [], item_df

        def get_item_attributes(drv: webdriver.Firefox, product_name: str, prod_id: str, use_button: bool = False,
                                multi_variety: bool = False, typ=None, ) -&gt; Tuple[str, str, str, str]:
            &#34;&#34;&#34;get_item_attributes scrapes product item specific data such as item_name, size, price, and ingredients.

            Args:
                drv (webdriver.Firefox): Selenium webdriver with opened product page.
                product_name (str): Name of the product from metadata.
                prod_id (str): Id of the product from metdata.
                use_button (bool, optional): Whether to use buttons to extract item name. Defaults to False.
                multi_variety (bool, optional): Whether product has multiple color/size varieties. Defaults to False.
                typ ([type], optional): The product the currently selected. Defaults to None.

            Returns:
                Tuple[str, str, str, str]: item_name, size, price, ingredients.
            &#34;&#34;&#34;
            # drv # type: webdriver.Chrome
            # close popup windows
            close_popups(drv)
            accept_alert(drv, 1)

            try:
                item_price = drv.find_element_by_class_name(&#39;css-1865ad6&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                item_price = &#39;&#39;
            # print(item_price)

            if multi_variety:
                try:
                    if use_button:
                        item_name = typ.find_element_by_tag_name(
                            &#39;button&#39;).get_attribute(&#39;aria-label&#39;)
                    else:
                        item_name = typ.get_attribute(&#39;aria-label&#39;)
                    # print(item_name)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    item_name = &#34;&#34;
                    self.logger.info(str.encode(
                        f&#39;product: {product_name} (prod_id: {prod_id}) item_name does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            else:
                item_name = &#34;&#34;

            try:
                item_size = drv.find_element_by_class_name(&#39;css-128n72s&#39;).text
                # print(item_size)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                item_size = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) item_size does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            # get all tabs
            first_tab = drv.find_element_by_id(f&#39;tab{0}&#39;)
            self.scroll_to_element(drv, first_tab)
            ActionChains(drv).move_to_element(
                first_tab).click(first_tab).perform()
            prod_tabs = []
            prod_tabs = drv.find_elements_by_class_name(&#39;css-1wugx5m&#39;)
            prod_tabs.extend(drv.find_elements_by_class_name(&#39;css-12vae0p&#39;))

            tab_names = []
            for t in prod_tabs:
                tab_names.append(t.text.lower())
            # print(tab_names)

            if &#39;ingredients&#39; in tab_names:
                close_popups(drv)
                accept_alert(drv, 1)
                if len(tab_names) == 5:
                    try:
                        tab_num = 2
                        ing_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                        self.scroll_to_element(drv, ing_button)
                        ActionChains(drv).move_to_element(
                            ing_button).click(ing_button).perform()
                        item_ing = drv.find_element_by_xpath(
                            f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        print(&#39;cant get ingredient but tab exists&#39;)
                        item_ing = &#34;&#34;
                        self.logger.info(str.encode(
                            f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients extraction failed&#39;,
                            &#39;utf-8&#39;, &#39;ignore&#39;))
                elif len(tab_names) == 4:
                    try:
                        tab_num = 1
                        ing_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                        self.scroll_to_element(drv, ing_button)
                        ActionChains(drv).move_to_element(
                            ing_button).click(ing_button).perform()
                        item_ing = drv.find_element_by_xpath(
                            f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        print(&#39;cant get ingredient but tab exists&#39;)
                        item_ing = &#34;&#34;
                        self.logger.info(str.encode(
                            f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients extraction failed.&#39;,
                            &#39;utf-8&#39;, &#39;ignore&#39;))
                elif len(tab_names) &lt; 4:
                    try:
                        tab_num = 0
                        ing_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                        self.scroll_to_element(drv, ing_button)
                        ActionChains(drv).move_to_element(
                            ing_button).click(ing_button).perform()
                        item_ing = drv.find_element_by_xpath(
                            f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        print(&#39;cant get ingredient but tab exists&#39;)
                        item_ing = &#34;&#34;
                        self.logger.info(str.encode(
                            f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients extraction failed.&#39;,
                            &#39;utf-8&#39;, &#39;ignore&#39;))
            else:
                item_ing = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            # print(item_ing)
            return item_name, item_size, item_price, item_ing

        def get_product_attributes(drv: webdriver.Firefox, product_name: str, prod_id: str) -&gt; list:
            &#34;&#34;&#34;get_product_attributes uses get_item_attribute method to scrape item details and stores in a list
            which is returned to get_detail method for storing in a product specific dataframe.

            Args:
                drv (webdriver.Firefox): Selenium webdriver with opened product page.
                product_name (str): Name of the product from metadata.
                prod_id (str): Id of the product from metdata.

            Returns:
                list: List containing all product item attributes of multiple varieties with name and id.
            &#34;&#34;&#34;
            # get all the variation of product
            # close popup windows
            close_popups(drv)
            accept_alert(drv, 1)

            product_variety = []
            try:
                product_variety = drv.find_elements_by_class_name(
                    &#39;css-1j1jwa4&#39;)
                product_variety.extend(
                    drv.find_elements_by_class_name(&#39;css-cl742e&#39;))
                use_button = False
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
            try:
                if len(product_variety) &lt; 1:
                    product_variety = drv.find_elements_by_class_name(
                        &#39;css-5jqxch&#39;)
                    use_button = True
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)

            product_attributes = []

            if len(product_variety) &gt; 0:
                for typ in product_variety:
                    close_popups(drv)
                    accept_alert(drv, 1)
                    try:
                        self.scroll_to_element(drv, typ)
                        ActionChains(drv).move_to_element(
                            typ).click(typ).perform()
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    time.sleep(4)  # 8
                    item_name, item_size, item_price, item_ingredients = get_item_attributes(drv, product_name, prod_id,
                                                                                             multi_variety=True, typ=typ,
                                                                                             use_button=use_button)
                    product_attributes.append({&#34;prod_id&#34;: prod_id, &#34;product_name&#34;: product_name,
                                               &#34;item_name&#34;: item_name, &#34;item_size&#34;: item_size,
                                               &#34;item_price&#34;: item_price, &#34;item_ingredients&#34;: item_ingredients})
            else:
                item_name, item_size, item_price, item_ingredients = get_item_attributes(drv,
                                                                                         product_name, prod_id)
                product_attributes.append({&#34;prod_id&#34;: prod_id, &#34;product_name&#34;: product_name, &#34;item_name&#34;: item_name,
                                           &#34;item_size&#34;: item_size, &#34;item_price&#34;: item_price, &#34;item_ingredients&#34;: item_ingredients})

            return product_attributes

        def get_first_review_date(drv: webdriver.Firefox) -&gt; str:
            &#34;&#34;&#34;get_first_review_date scraped the first review data of the product.

            Args:
                drv (webdriver.Firefox): Selenium webdriver with opened product page.

            Returns:
                str: first review date.
            &#34;&#34;&#34;
            # close popup windows
            close_popups(drv)
            accept_alert(drv, 1)

            try:
                review_sort_trigger = drv.find_element_by_id(
                    &#39;review_filter_sort_trigger&#39;)
                self.scroll_to_element(drv, review_sort_trigger)
                ActionChains(drv).move_to_element(
                    review_sort_trigger).click(review_sort_trigger).perform()
                for btn in drv.find_elements_by_class_name(&#39;css-rfz1gg&#39;):
                    if btn.text.lower() == &#39;oldest&#39;:
                        ActionChains(drv).move_to_element(
                            btn).click(btn).perform()
                        break
                time.sleep(6)
                close_popups(drv)
                accept_alert(drv, 1)
                rev = drv.find_elements_by_class_name(&#39;css-1kk8dps&#39;)[2:]
                try:
                    first_review_date = convert_ago_to_date(
                        rev[0].find_element_by_class_name(&#39;css-h2vfi1&#39;).text)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    try:
                        first_review_date = convert_ago_to_date(
                            rev[1].find_element_by_class_name(&#39;css-h2vfi1&#39;).text)
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        print(&#39;sorted but cant get first review date value&#39;)
                        first_review_date = &#39;&#39;
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                first_review_date = &#39;&#39;
            return first_review_date

        for prod in self.meta.index[self.meta.index.isin(indices)]:
            #  ignore already extracted products
            if self.meta.loc[prod, &#39;detail_scraped&#39;] in [&#39;Y&#39;, &#39;NA&#39;]:
                continue
            # print(prod, self.meta.loc[prod, &#39;detail_scraped&#39;])
            prod_id = self.meta.loc[prod, &#39;prod_id&#39;]
            product_name = self.meta.loc[prod, &#39;product_name&#39;]
            product_page = self.meta.loc[prod, &#39;product_page&#39;]

            # create webdriver
            if randomize_proxy_usage:
                use_proxy = np.random.choice([True, False])
            else:
                use_proxy = True
            if open_with_proxy_server:
                # print(use_proxy)
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
                                        path=self.detail_path)
            else:
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
                                        path=self.detail_path)
            # open product page
            drv.get(product_page)
            time.sleep(20)  # 30
            accept_alert(drv, 10)
            close_popups(drv)

            # check product page is valid and exists
            try:
                close_popups(drv)
                accept_alert(drv, 2)
                price = drv.find_element_by_class_name(&#39;css-1865ad6&#39;)
                self.scroll_to_element(drv, price)
                price = price.text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                drv.quit()
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) no longer exists in the previously fetched link.\
                        (link:{product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                self.meta.loc[prod, &#39;detail_scraped&#39;] = &#39;NA&#39;
                continue

            try:
                chat_popup_button = WebDriverWait(drv, 3).until(
                    EC.presence_of_element_located((By.XPATH, &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)))
                chat_popup_button = drv.find_element_by_xpath(
                    &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)
                self.scroll_to_element(drv, chat_popup_button)
                ActionChains(drv).move_to_element(
                    chat_popup_button).click(chat_popup_button).perform()
            except TimeoutException:
                pass

            # get all product info tabs such as how-to-use, about-brand, ingredients
            prod_tabs = []
            prod_tabs = drv.find_elements_by_class_name(&#39;css-1wugx5m&#39;)
            prod_tabs.extend(drv.find_elements_by_class_name(&#39;css-12vae0p&#39;))

            tab_names = []
            for t in prod_tabs:
                tab_names.append(t.text.lower())

            # no. of votes
            try:
                votes = drv.find_elements_by_class_name(&#39;css-2rg6q7&#39;)[-1].text
                # print(votes)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                votes = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) votes does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            # product details
            if &#39;details&#39; in tab_names:
                try:
                    close_popups(drv)
                    accept_alert(drv, 1)
                    tab_num = tab_names.index(&#39;details&#39;)
                    detail_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                    try:
                        time.sleep(1)
                        self.scroll_to_element(drv, detail_button)
                        ActionChains(drv).move_to_element(
                            detail_button).click(detail_button).perform()
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        details = &#34;&#34;
                    else:
                        try:
                            details = drv.find_element_by_xpath(
                                f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                        except Exception as ex:
                            log_exception(self.logger,
                                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
                            details = &#34;&#34;
                            self.logger.info(str.encode(
                                f&#39;product: {product_name} (prod_id: {prod_id}) product detail text\
                                        does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                        # print(details)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    details = &#34;&#34;
                    self.logger.info(str.encode(
                        f&#39;product: {product_name} (prod_id: {prod_id}) product detail extraction failed&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            else:
                details = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) product detail does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            # how to use
            if &#39;how to use&#39; in tab_names:
                try:
                    close_popups(drv)
                    accept_alert(drv, 1)

                    tab_num = tab_names.index(&#39;how to use&#39;)
                    how_to_use_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                    try:
                        time.sleep(1)
                        self.scroll_to_element(drv, how_to_use_button)
                        ActionChains(drv).move_to_element(
                            how_to_use_button).click(how_to_use_button).perform()
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        how_to_use = &#34;&#34;
                    else:
                        try:
                            how_to_use = drv.find_element_by_xpath(
                                f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                        except Exception as ex:
                            log_exception(self.logger,
                                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
                            how_to_use = &#34;&#34;
                            self.logger.info(str.encode(
                                f&#39;product: {product_name} (prod_id: {prod_id}) how_to_use text\
                                     does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                        # print(how_to_use)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    how_to_use = &#34;&#34;
                    self.logger.info(str.encode(
                        f&#39;product: {product_name} (prod_id: {prod_id}) how_to_use extraction failed&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            else:
                how_to_use = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) how_to_use does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            # about the brand
            if &#39;about the brand&#39; in tab_names:
                try:
                    close_popups(drv)
                    accept_alert(drv, 1)

                    tab_num = tab_names.index(&#39;about the brand&#39;)
                    about_the_brand_button = drv.find_element_by_id(
                        f&#39;tab{tab_num}&#39;)
                    try:
                        time.sleep(1)
                        self.scroll_to_element(drv, about_the_brand_button)
                        ActionChains(drv).move_to_element(
                            about_the_brand_button).click(about_the_brand_button).perform()
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        about_the_brand = &#34;&#34;
                    else:
                        try:
                            about_the_brand = drv.find_element_by_xpath(
                                f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                        except Exception as ex:
                            log_exception(self.logger,
                                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
                            about_the_brand = &#34;&#34;
                            self.logger.info(str.encode(
                                f&#39;product: {product_name} (prod_id: {prod_id}) about_the_brand text\
                                    does not exist&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                        # print(about_the_brand)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    about_the_brand = &#34;&#34;
                    self.logger.info(str.encode(
                        f&#39;product: {product_name} (prod_id: {prod_id}) about_the_brand extraction failed&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            else:
                about_the_brand = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) about_the_brand does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            self.scroll_down_page(drv, h2=0.4, speed=5)
            time.sleep(5)
            try:
                chat_popup_button = WebDriverWait(drv, 3).until(
                    EC.presence_of_element_located((By.XPATH, &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)))
                chat_popup_button = drv.find_element_by_xpath(
                    &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)
                self.scroll_to_element(drv, chat_popup_button)
                ActionChains(drv).move_to_element(
                    chat_popup_button).click(chat_popup_button).perform()
            except TimeoutException:
                pass
            # click no. of reviews
            try:
                review_button = drv.find_element_by_class_name(&#39;css-1pjru6n&#39;)
                self.scroll_to_element(drv, review_button)
                ActionChains(drv).move_to_element(
                    review_button).click(review_button).perform()
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)

            try:
                first_review_date = get_first_review_date(drv)
                # print(first_review_date)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                first_review_date = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) first_review_date scrape failed.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            try:
                close_popups(drv)
                accept_alert(drv, 1)
                reviews = int(drv.find_element_by_class_name(
                    &#39;css-ils4e4&#39;).text.split()[0])
                # print(reviews)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                reviews = 0
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) reviews does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            try:
                close_popups(drv)
                accept_alert(drv, 1)
                rating_distribution = drv.find_element_by_class_name(
                    &#39;css-960eb6&#39;).text.split(&#39;\n&#39;)
                # print(rating_distribution)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                rating_distribution = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) rating_distribution does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            try:
                close_popups(drv)
                accept_alert(drv, 1)
                would_recommend = drv.find_element_by_class_name(
                    &#39;css-k9ne19&#39;).text
                # print(would_recommend)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                would_recommend = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) would_recommend does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            product_attributes = pd.DataFrame(
                get_product_attributes(drv, product_name, prod_id))
            item_df = pd.concat(
                [item_df, pd.DataFrame(product_attributes)], axis=0)

            detail_data.append({&#39;prod_id&#39;: prod_id, &#39;product_name&#39;: product_name, &#39;abt_product&#39;: details,
                                &#39;how_to_use&#39;: how_to_use, &#39;abt_brand&#39;: about_the_brand,
                                &#39;reviews&#39;: reviews, &#39;votes&#39;: votes, &#39;rating_dist&#39;: rating_distribution,
                                &#39;would_recommend&#39;: would_recommend, &#39;first_review_date&#39;: first_review_date})
            # item_data.append(product_attributes)
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) details extracted successfully&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            self.meta.loc[prod, &#39;detail_scraped&#39;] = &#39;Y&#39;

            if prod != 0 and prod % 10 == 0:
                if len(detail_data) &gt; 0:
                    detail_data, item_df = store_data_refresh_mem(
                        detail_data, item_df)
            drv.quit()

        detail_data, item_df = store_data_refresh_mem(
            detail_data, item_df)
        self.logger.info(
            f&#39;Detail Extraction Complete for start_idx: (indices[0]) to end_idx: {indices[-1]}. Or for list of values.&#39;)

    def extract(self, metadata: pd.DataFrame, download: bool = True, n_workers: int = 5,
                fresh_start: bool = False, auto_fresh_start: bool = False,
                open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = False,
                start_idx: Optional[int] = None, end_idx: Optional[int] = None, list_of_index=None,
                compile_progress_files: bool = False, clean: bool = True, delete_progress: bool = False):
        &#34;&#34;&#34;extract method controls all properties of the spiders and runs multi-threaded web crawling.

        Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
        This method has four major functionality:
        * 1. Run the spider
        * 2. Store data in regular intervals to free up ram
        * 3. Compile all crawled data into one file.
        * 4. Clean and push cleaned data to S3 storage for further algorithmic processing.

        Args:
            metadata (pd.DataFrame): Dataframe containing product specific url, name and id of the products to be scraped.
            download (bool, optional): Whether to crawl data from or compile crawled data into one file. Defaults to True.
            n_workers (int, optional): No. of parallel threads to run. Defaults to 5.
            fresh_start (bool, optional): Whether to continue last crawl job or start new one. Defaults to False.
            auto_fresh_start (bool, optional): Whether to automatically start a new crawl job if last job was finished. Defaults to False.
            open_headless (bool, optional):  Whether to open browser headless. Defaults to False.
            open_with_proxy_server (bool, optional): Whether to use ip rotation service. Defaults to True.
            randomize_proxy_usage (bool, optional): Whether to use both proxy and native network in tandem to decrease proxy requests.
                                                    Defaults to False.
            start_idx (Optional[int], optional): Starting index of the links to crawl. Defaults to None.
            end_idx (Optional[int], optional): Ending index of the links to crawl. Defaults to None.
            list_of_index ([type], optional): List of indices or range of indices of product urls to scrape. Defaults to None.
            compile_progress_files (bool, optional): Whether to combine crawled data into one file. Defaults to False.
            clean (bool, optional): Whether to clean the compiled data. Defaults to True.
            delete_progress (bool, optional): Whether to delete intermediate data after compilation into one file. Defaults to False.
        &#34;&#34;&#34;
        &#39;&#39;&#39;
        change metadata read logic.add logic to look for metadata in a folder path. if metadata is found in the folder path
        detail data crawler is triggered
        &#39;&#39;&#39;
        # list_of_files = self.metadata_clean_path.glob(
        #     &#39;no_cat_cleaned_sph_product_metadata_all*&#39;)
        # self.meta = pd.read_feather(max(list_of_files, key=os.path.getctime))[
        #     [&#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;]]

        def fresh():
            &#34;&#34;&#34; If fresh_start is True, this function sets initial parameters for a fresh data crawl.
            &#34;&#34;&#34;
            if not isinstance(metadata, pd.core.frame.DataFrame):
                list_of_files = self.metadata_clean_path.glob(
                    &#39;no_cat_cleaned_sph_product_metadata_all*&#39;)
                self.meta = pd.read_feather(max(list_of_files, key=os.path.getctime))[
                    [&#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;]]
            else:
                self.meta = metadata[[
                    &#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;]]

            self.meta[&#39;detail_scraped&#39;] = &#39;N&#39;

        if download:
            if fresh_start:
                fresh()
                self.logger.info(
                    &#39;Starting Fresh Detail Extraction.&#39;)
            else:
                if Path(self.detail_path/&#39;sph_detail_progress_tracker&#39;).exists():
                    self.meta = pd.read_feather(
                        self.detail_path/&#39;sph_detail_progress_tracker&#39;)
                    if sum(self.meta.detail_scraped == &#39;N&#39;) == 0:
                        if auto_fresh_start:
                            fresh()
                            self.logger.info(
                                &#39;Last Run was Completed. Starting Fresh Extraction.&#39;)
                        else:
                            self.logger.info(
                                &#39;Detail extraction for this cycle is complete.&#39;)
                    else:
                        self.logger.info(
                            &#39;Continuing Detail Extraction From Last Run.&#39;)
                else:
                    fresh()
                    self.logger.info(
                        &#39;Detail Progress Tracker does not exist. Starting Fresh Extraction.&#39;)

            # set list or range of product indices to crawl
            if list_of_index:
                indices = list_of_index
            elif start_idx and end_idx is None:
                indices = range(start_idx, len(self.meta))
            elif start_idx is None and end_idx:
                indices = range(0, end_idx)
            elif start_idx is not None and end_idx is not None:
                indices = range(start_idx, end_idx)
            else:
                indices = range(len(self.meta))
            print(indices)

            if list_of_index:
                self.get_detail(indices=list_of_index,
                                open_headless=open_headless,
                                open_with_proxy_server=open_with_proxy_server,
                                randomize_proxy_usage=randomize_proxy_usage)
            else:  # By default the code will with 5 concurrent threads. you can change this behaviour by changing n_workers
                if start_idx:
                    lst_of_lst = ranges(
                        indices[-1]+1, n_workers, start_idx=start_idx)
                else:
                    lst_of_lst = ranges(len(indices), n_workers)
                print(lst_of_lst)
                # detail_Data and item_data are lists of empty lists so that each namepace of function call will have its separate detail_data
                # list to strore scraped dictionaries. will save memory(ram/hard-disk) consumption. will stop data duplication
                headless = [open_headless for i in lst_of_lst]
                proxy = [open_with_proxy_server for i in lst_of_lst]
                rand_proxy = [randomize_proxy_usage for i in lst_of_lst]
                detail_data = [[] for i in lst_of_lst]  # type: List
                # item_data=[[] for i in lst_of_lst]  # type: List
                item_df = [pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                                 &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;])
                           for i in lst_of_lst]
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    # but each of the function namespace will be modifying only one metadata tracing file so that progress saving
                    # is tracked correctly. else multiple progress tracker file will be created with difficulty to combine correct
                    # progress information
                    print(&#39;inside executor&#39;)
                    executor.map(self.get_detail, lst_of_lst,
                                 headless, proxy, rand_proxy,
                                 detail_data, item_df)
        try:
            if compile_progress_files:
                self.logger.info(&#39;Creating Combined Detail and Item File&#39;)
                if datetime.now().day &lt; 15:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
                else:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;

                det_li = []
                self.bad_det_li = []
                detail_files = [f for f in self.current_progress_path.glob(
                    &#34;sph_prod_detail_extract_progress_*&#34;)]
                for file in detail_files:
                    try:
                        df = pd.read_csv(file)
                    except Exception:
                        self.bad_det_li.append(file)
                    else:
                        det_li.append(df)

                detail_df = pd.concat(det_li, axis=0, ignore_index=True)
                detail_df.drop_duplicates(inplace=True)
                detail_df.reset_index(inplace=True, drop=True)
                detail_df[&#39;meta_date&#39;] = meta_date
                detail_filename = f&#39;sph_product_detail_all_{meta_date}.csv&#39;
                detail_df.to_csv(self.detail_path/detail_filename, index=None)
                # detail_df.to_feather(self.detail_path/detail_filename)

                item_li = []
                self.bad_item_li = []
                item_files = [f for f in self.current_progress_path.glob(
                    &#34;sph_prod_item_extract_progress_*&#34;)]
                for file in item_files:
                    try:
                        idf = pd.read_csv(file)
                    except Exception:
                        self.bad_item_li.append(file)
                    else:
                        item_li.append(idf)

                item_dataframe = pd.concat(item_li, axis=0, ignore_index=True)
                item_dataframe.drop_duplicates(inplace=True)
                item_dataframe.reset_index(inplace=True, drop=True)
                item_dataframe[&#39;meta_date&#39;] = meta_date
                item_filename = f&#39;sph_product_item_all_{meta_date}.csv&#39;
                item_dataframe.to_csv(
                    self.detail_path/item_filename, index=None)
                # item_df.to_feather(self.detail_path/item_filename)

                self.logger.info(
                    f&#39;Detail and Item files created. Please look for file sph_product_detail_all and\
                        sph_product_item_all in path {self.detail_path}&#39;)
                print(
                    f&#39;Detail and Item files created. Please look for file sph_product_detail_all and\
                        sph_product_item_all in path {self.detail_path}&#39;)

                if clean:
                    detail_cleaner = Cleaner(path=self.path)
                    self.detail_clean_df = detail_cleaner.clean(
                        self.detail_path/detail_filename)
                    del detail_cleaner
                    gc.collect()

                    item_cleaner = Cleaner(path=self.path)
                    self.item_clean_df, self.ing_clean_df = item_cleaner.clean(
                        self.detail_path/item_filename)
                    del item_cleaner
                    gc.collect()

                    file_creation_status = True
            else:
                file_creation_status = False
        except Exception as ex:
            log_exception(
                self.logger, additional_information=f&#39;Detail Item Combined File Creation Failed.&#39;)
            file_creation_status = False

        if delete_progress and file_creation_status:
            shutil.rmtree(
                f&#39;{self.detail_path}\\current_progress&#39;, ignore_errors=True)
            self.logger.info(&#39;Progress files deleted&#39;)

    def terminate_logging(self):
        &#34;&#34;&#34;terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.
        &#34;&#34;&#34;
        self.logger.handlers.clear()
        self.prod_detail_log.stop_log()


class Review(Sephora):
    &#34;&#34;&#34;Review extracts product reviews, review rating and user attributes from Sephora website.

    The Review module utilizes product urls scraped by Metadata by opening the pages one by one and getting
    reviews for each individual products.

    Args:
        Sephora ([type]): Class that initializes folder paths and selenium webdriver for data scraping.
    &#34;&#34;&#34;

    def __init__(self, log: bool = True, path: Path = Path.cwd()):
        &#34;&#34;&#34;__init__ Review class instace initializer.

        This method sets all the folder paths required for Review crawler to work.
        If the paths does not exist the paths get automatically created depending on
        current directory or provided directory.

        Args:
            log (bool, optional): Whether to create crawling exception and progess log. Defaults to True.
            path (Path, optional): Folder path where the Review will be extracted. Defaults to current directory(Path.cwd()).
        &#34;&#34;&#34;
        super().__init__(path=path, data_def=&#39;review&#39;)
        self.path = path
        self.current_progress_path = self.review_path/&#39;current_progress&#39;
        self.current_progress_path.mkdir(parents=True, exist_ok=True)

        old_review_files = list(self.review_path.glob(
            &#39;sph_product_review_all*&#39;))
        for f in old_review_files:
            shutil.move(str(f), str(self.old_review_files_path))

        old_clean_review_files = os.listdir(self.review_clean_path)
        for f in old_clean_review_files:
            shutil.move(str(self.review_clean_path/f),
                        str(self.old_review_clean_files_path))
        if log:
            self.prod_review_log = Logger(
                &#34;sph_prod_review_extraction&#34;, path=self.crawl_log_path)
            self.logger, _ = self.prod_review_log.start_log()

    def get_reviews(self, indices: list, open_headless: bool, open_with_proxy_server: bool,
                    randomize_proxy_usage: bool,
                    review_data: list = [], incremental: bool = True):
        &#34;&#34;&#34;get_reviews Crawls individual product pages for review text, title, date user attributes etc.

        Args:
            indices (list): list of indices or range of indices of product urls to scrape.
            open_headless (bool): Whether to open browser headless.
            open_with_proxy_server (bool): Whether to use ip rotation service.
            randomize_proxy_usage (bool): Whether to use both proxy and native network in tandem to decrease proxy requests.
            review_data (list, optional): Empty intermediate list to store data during parallel crawl. Defaults to [].
            incremental (bool, optional): Whether to scrape reviews incrementally from last scraped review date. Defaults to True.
        &#34;&#34;&#34;
        def store_data_refresh_mem(review_data: list) -&gt; list:
            &#34;&#34;&#34;store_data_refresh_mem method stores crawled data in regular interval to free up system memory.

            Store data after each product&#39;s reviews are extracted to free up RAM.

            Args:
                review_data (list): List containing scraped Review data to store.

            Returns:
                list: Empty list to accumulate data from next product scraping.
            &#34;&#34;&#34;
            pd.DataFrame(review_data).to_csv(self.current_progress_path /
                                             f&#39;sph_prod_review_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                                             index=None)

            self.meta.to_csv(
                self.review_path/&#39;sph_review_progress_tracker.csv&#39;, index=None)
            return []

        for prod in self.meta.index[self.meta.index.isin(indices)]:
            if self.meta.loc[prod, &#39;review_scraped&#39;] in [&#39;Y&#39;, &#39;NA&#39;] or self.meta.loc[prod, &#39;review_scraped&#39;] is np.nan:
                continue
            prod_id = self.meta.loc[prod, &#39;prod_id&#39;]
            product_name = self.meta.loc[prod, &#39;product_name&#39;]
            product_page = self.meta.loc[prod, &#39;product_page&#39;]

            last_scraped_review_date = self.meta.loc[prod,
                                                     &#39;last_scraped_review_date&#39;]
            # print(last_scraped_review_date)

            if randomize_proxy_usage:
                use_proxy = np.random.choice([True, False])
            else:
                use_proxy = True
            if open_with_proxy_server:
                # print(use_proxy)
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
                                        path=self.detail_path)
                # drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
                #                                 path=self.detail_path)
            else:
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
                                        path=self.detail_path)
                # drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
                #                                 path=self.detail_path)

            drv.get(product_page)
            time.sleep(10)  # 30
            accept_alert(drv, 10)
            close_popups(drv)

            self.scroll_down_page(drv, speed=6, h2=0.6)
            time.sleep(5)

            try:
                close_popups(drv)
                accept_alert(drv, 1)
                no_of_reviews = int(drv.find_element_by_class_name(
                    &#39;css-ils4e4&#39;).text.split()[0])
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                self.logger.info(str.encode(f&#39;Product: {product_name} prod_id: {prod_id} reviews extraction failed.\
                                              Either product has no reviews or not\
                                              available for sell currently.(page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                no_of_reviews = 0
                self.meta.loc[prod, &#39;review_scraped&#39;] = &#34;NA&#34;
                self.meta.to_csv(
                    self.review_path/&#39;sph_review_progress_tracker.csv&#39;, index=None)
                drv.quit()
                # print(&#39;in except - continue&#39;)
                continue

            # print(no_of_reviews)
            # drv.find_element_by_class_name(&#39;css-2rg6q7&#39;).click()
            if incremental and last_scraped_review_date != &#39;&#39;:
                for n in range(no_of_reviews//6):
                    if n &gt; 400:
                        break

                    time.sleep(0.4)
                    revs = drv.find_elements_by_class_name(
                        &#39;css-1kk8dps&#39;)[2:]

                    try:
                        if pd.to_datetime(convert_ago_to_date(revs[-1].find_element_by_class_name(&#39;css-1t84k9w&#39;).text),
                                          infer_datetime_format=True)\
                                &lt; pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                            # print(&#39;breaking incremental&#39;)
                            break
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        try:
                            if pd.to_datetime(convert_ago_to_date(revs[-2].find_element_by_class_name(&#39;css-1t84k9w&#39;).text),
                                              infer_datetime_format=True)\
                                    &lt; pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                                # print(&#39;breaking incremental&#39;)
                                break
                        except Exception as ex:
                            log_exception(self.logger,
                                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
                            self.logger.info(str.encode(f&#39;Product: {product_name} prod_id: {prod_id} \
                                                         last_scraped_review_date to current review date \
                                                         comparision failed.(page: {product_page})&#39;,
                                                        &#39;utf-8&#39;, &#39;ignore&#39;))
                            # print(&#39;in second except block&#39;)
                            continue
                    try:
                        show_more_review_button = drv.find_element_by_class_name(
                            &#39;css-xswy5p&#39;)
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}. Failed to get show more review button.&#39;)
                    else:
                        try:
                            self.scroll_to_element(
                                drv, show_more_review_button)
                            ActionChains(drv).move_to_element(
                                show_more_review_button).click(show_more_review_button).perform()
                        except Exception as ex:
                            log_exception(self.logger,
                                          additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)
                            accept_alert(drv, 1)
                            close_popups(drv)
                            try:
                                self.scroll_to_element(
                                    drv, show_more_review_button)
                                ActionChains(drv).move_to_element(
                                    show_more_review_button).click(show_more_review_button).perform()
                            except Exception as ex:
                                log_exception(self.logger,
                                              additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)

            else:
                # print(&#39;inside get all reviews&#39;)
                # 6 because for click sephora shows 6 reviews. additional 25 no. of clicks for buffer.
                for n in range(no_of_reviews//6+10):
                    &#39;&#39;&#39;
                    code will stop after getting 1800 reviews of one particular product
                    when crawling all reviews. By default it will get latest 1800 reviews.
                    then in subsequent incremental runs it will get al new reviews on weekly basis
                    &#39;&#39;&#39;
                    if n &gt;= 400:  # 200:
                        break
                    time.sleep(1)
                    # close any opened popups by escape
                    accept_alert(drv, 1)
                    close_popups(drv)
                    try:
                        show_more_review_button = drv.find_element_by_class_name(
                            &#39;css-xswy5p&#39;)
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}. Failed to get show more review button.&#39;)
                    else:
                        try:
                            self.scroll_to_element(
                                drv, show_more_review_button)
                            ActionChains(drv).move_to_element(
                                show_more_review_button).click(show_more_review_button).perform()
                        except Exception as ex:
                            log_exception(self.logger,
                                          additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)
                            accept_alert(drv, 1)
                            close_popups(drv)
                            try:
                                self.scroll_to_element(
                                    drv, show_more_review_button)
                                ActionChains(drv).move_to_element(
                                    show_more_review_button).click(show_more_review_button).perform()
                            except Exception as ex:
                                log_exception(self.logger,
                                              additional_information=f&#39;Prod ID: {prod_id}.\
                                                   Failed to click on show more review button.&#39;)
                                try:
                                    self.scroll_to_element(
                                        drv, show_more_review_button)
                                    ActionChains(drv).move_to_element(
                                        show_more_review_button).click(show_more_review_button).perform()
                                except Exception as ex:
                                    log_exception(self.logger,
                                                  additional_information=f&#39;Prod ID: {prod_id}.\
                                                   Failed to click on show more review button.&#39;)
                                    accept_alert(drv, 2)
                                    close_popups(drv)
                                    try:
                                        self.scroll_to_element(
                                            drv, show_more_review_button)
                                        ActionChains(drv).move_to_element(
                                            show_more_review_button).click(show_more_review_button).perform()
                                    except Exception as ex:
                                        log_exception(self.logger,
                                                      additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)
                                        if n &lt; (no_of_reviews//6):
                                            self.logger.info(str.encode(f&#39;Product: {product_name} - prod_id \
                                                {prod_id} breaking click next review loop.\
                                                                        [total_reviews:{no_of_reviews} loaded_reviews:{n}]\
                                                                        (page link: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                                            self.logger.info(str.encode(f&#39;Product: {product_name} - prod_id {prod_id} cant load all reviews.\
                                                                          Check click next 6 reviews\
                                                                          code section(page link: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                                        break

            accept_alert(drv, 2)
            close_popups(drv)

            product_reviews = drv.find_elements_by_class_name(
                &#39;css-1kk8dps&#39;)[2:]

            # print(&#39;starting extraction&#39;)
            r = 0
            for rev in product_reviews:
                accept_alert(drv, 0.5)
                close_popups(drv)
                self.scroll_to_element(drv, rev)
                ActionChains(drv).move_to_element(rev).perform()

                try:
                    try:
                        review_text = rev.find_element_by_class_name(
                            &#39;css-1jg2pb9&#39;).text
                    except NoSuchElementException:
                        review_text = rev.find_element_by_class_name(
                            &#39;css-429528&#39;).text
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_text. Skip review.&#39;)
                    continue

                try:
                    review_date = convert_ago_to_date(
                        rev.find_element_by_class_name(&#39;css-h2vfi1&#39;).text)
                    if pd.to_datetime(review_date, infer_datetime_format=True) &lt; \
                            pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                        continue
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_date.&#39;)
                    review_date = &#39;&#39;

                try:
                    review_title = rev.find_element_by_class_name(
                        &#39;css-1jfmule&#39;).text
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_title.&#39;)
                    review_title = &#39;&#39;

                try:
                    product_variant = rev.find_element_by_class_name(
                        &#39;css-1op1cn7&#39;).text
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to extract product_variant.&#39;)
                    product_variant = &#39;&#39;

                try:
                    user_rating = rev.find_element_by_class_name(
                        &#39;css-3z5ot7&#39;).get_attribute(&#39;aria-label&#39;)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to extract user_rating.&#39;)
                    user_rating = &#39;&#39;

                try:
                    user_attribute = [{&#39;_&#39;.join(u.lower().split()[0:-1]): u.lower().split()[-1]}
                                      for u in rev.find_element_by_class_name(&#39;css-ecreye&#39;).text.split(&#39;\n&#39;)]
                    # user_attribute = []
                    # for u in rev.find_elements_by_class_name(&#39;css-j5yt83&#39;):
                    #     user_attribute.append(
                    #         {&#39;_&#39;.join(u.text.lower().split()[0:-1]): u.text.lower().split()[-1]})
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to extract user_attribute.&#39;)
                    user_attribute = []

                try:
                    recommend = rev.find_element_by_class_name(
                        &#39;css-1tf5yph&#39;).text
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to extract recommend.&#39;)
                    recommend = &#39;&#39;

                try:
                    helpful = rev.find_element_by_class_name(&#39;css-b7zg5r&#39;).text
                    # helpful = []
                    # for h in rev.find_elements_by_class_name(&#39;css-39esqn&#39;):
                    #     helpful.append(h.text)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to extract helpful.&#39;)
                    helpful = &#39;&#39;

                review_data.append({&#39;prod_id&#39;: prod_id, &#39;product_name&#39;: product_name,
                                    &#39;user_attribute&#39;: user_attribute, &#39;product_variant&#39;: product_variant,
                                    &#39;review_title&#39;: review_title, &#39;review_text&#39;: review_text,
                                    &#39;review_rating&#39;: user_rating, &#39;recommend&#39;: recommend,
                                    &#39;review_date&#39;: review_date,   &#39;helpful&#39;: helpful})
            drv.quit()
            self.meta.loc[prod, &#39;review_scraped&#39;] = &#39;Y&#39;
            review_data = store_data_refresh_mem(review_data)
            if not incremental:
                self.logger.info(str.encode(
                    f&#39;Product_name: {product_name} prod_id:{prod_id} reviews extracted successfully.(total_reviews: {no_of_reviews}, \
                    extracted_reviews: {len(product_reviews)}, page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            else:
                self.logger.info(str.encode(
                    f&#39;Product_name: {product_name} prod_id:{prod_id} new reviews extracted successfully.\
                        (no_of_new_extracted_reviews: {len(product_reviews)},\
                         page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
        # save the final review file
        review_data = store_data_refresh_mem(review_data)

    def extract(self, metadata: Union[pd.DataFrame, str, Path], download: bool = True, n_workers: int = 5,
                fresh_start: bool = False, auto_fresh_start: bool = False, incremental: bool = True,
                open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = True,
                start_idx: Optional[int] = None, end_idx: Optional[int] = None, list_of_index=None,
                compile_progress_files: bool = False, clean: bool = True, delete_progress: bool = False) -&gt; None:
        &#34;&#34;&#34;extract method controls all properties of the spiders and runs multi-threaded web crawling.

        Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
        This method has four major functionality:
        * 1. Run the spider
        * 2. Store data in regular intervals to free up ram
        * 3. Compile all crawled data into one file.
        * 4. Clean and push cleaned data to S3 storage for further algorithmic processing.

        Args:
            metadata (Union[pd.DataFrame, str, Path]): Dataframe containing product specific url, name and id of the products to be scraped.
            download (bool, optional): Whether to crawl data from or compile crawled data into one file. Defaults to True.
            n_workers (int, optional): No. of parallel threads to run. Defaults to 5.
            fresh_start (bool, optional): Whether to continue last crawl job or start new one. Defaults to False.
            auto_fresh_start (bool, optional): Whether to automatically start a new crawl job if last job was finished. Defaults to False.
            incremental (bool, optional): Whether to scrape reviews incrementally from last scraped review date. Defaults to True.
            open_headless (bool, optional):  Whether to open browser headless. Defaults to False.
            open_with_proxy_server (bool, optional): Whether to use ip rotation service. Defaults to True.
            randomize_proxy_usage (bool, optional): Whether to use both proxy and native network in tandem to decrease proxy requests.
            Defaults to False.
            start_idx (Optional[int], optional): Starting index of the links to crawl. Defaults to None.
            end_idx (Optional[int], optional): Ending index of the links to crawl. Defaults to None.
            list_of_index ([type], optional): List of indices or range of indices of product urls to scrape. Defaults to None.
            compile_progress_files (bool, optional): Whether to combine crawled data into one file. Defaults to False.
            clean (bool, optional): Whether to clean the compiled data. Defaults to True.
            delete_progress (bool, optional): Whether to delete intermediate data after compilation into one file. Defaults to False.
        &#34;&#34;&#34;
        def fresh():
            &#34;&#34;&#34; If fresh_start is True, this function sets initial parameters for a fresh data crawl.
            &#34;&#34;&#34;
            if not isinstance(metadata, pd.core.frame.DataFrame):
                list_of_files = self.metadata_clean_path.glob(
                    &#39;no_cat_cleaned_sph_product_metadata_all*&#39;)
                self.meta = pd.read_feather(max(list_of_files, key=os.path.getctime))[
                    [&#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;, &#39;last_scraped_review_date&#39;]]
            else:
                self.meta = metadata[[
                    &#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;, &#39;last_scraped_review_date&#39;]]
            self.meta.last_scraped_review_date.fillna(&#39;&#39;, inplace=True)
            self.meta[&#39;review_scraped&#39;] = &#39;N&#39;

        if download:
            if fresh_start:
                fresh()
            else:
                if Path(self.review_path/&#39;sph_review_progress_tracker.csv&#39;).exists():
                    self.meta = pd.read_csv(
                        self.review_path/&#39;sph_review_progress_tracker.csv&#39;)
                    if sum(self.meta.review_scraped == &#39;N&#39;) == 0:
                        if auto_fresh_start:
                            fresh()
                            self.logger.info(
                                &#39;Last Run was Completed. Starting Fresh Extraction.&#39;)
                        else:
                            self.logger.info(
                                f&#39;Review extraction for this cycle is complete. Please check files in path: {self.review_path}&#39;)
                            print(
                                f&#39;Review extraction for this cycle is complete. Please check files in path: {self.review_path}&#39;)
                    else:
                        self.logger.info(
                            &#39;Continuing Review Extraction From Last Run.&#39;)
                else:
                    fresh()
                    self.logger.info(
                        &#39;Review Progress Tracker not found. Starting Fresh Extraction.&#39;)

            # set list or range of product indices to crawl
            if list_of_index:
                indices = list_of_index
            elif start_idx and end_idx is None:
                indices = range(start_idx, len(self.meta))
            elif start_idx is None and end_idx:
                indices = range(0, end_idx)
            elif start_idx is not None and end_idx is not None:
                indices = range(start_idx, end_idx)
            else:
                indices = range(len(self.meta))
            # print(indices)

            if list_of_index:
                self.get_reviews(
                    indices=list_of_index, incremental=incremental, open_headless=open_headless,
                    open_with_proxy_server=open_with_proxy_server, randomize_proxy_usage=randomize_proxy_usage)
            else:  # By default the code will with 5 concurrent threads. you can change this behaviour by changing n_workers
                &#39;&#39;&#39;
                # review_Data and item_data are lists of empty lists so that each namepace of function call will
                # have its separate detail_data
                # list to strore scraped dictionaries. will save memory(ram/hard-disk) consumption. will stop data duplication
                &#39;&#39;&#39;
                if start_idx:
                    lst_of_lst = ranges(
                        indices[-1]+1, n_workers, start_idx=start_idx)
                else:
                    lst_of_lst = ranges(len(indices), n_workers)
                print(lst_of_lst)
                # lst_of_lst2 = list(
                #     chunks(indices, len(indices)//n_workers))  # type: list

                # print(lst_of_lst, &#39;\n&#39;, lst_of_lst2)

                headless = [open_headless for i in lst_of_lst]
                proxy = [open_with_proxy_server for i in lst_of_lst]
                rand_proxy = [randomize_proxy_usage for i in lst_of_lst]
                review_data = [[] for i in lst_of_lst]  # type: list
                inc_list = [incremental for i in lst_of_lst]  # type: list
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    &#39;&#39;&#39;
                    # but each of the function namespace will be modifying only one metadata tracing file so that progress saving
                    # is tracked correctly. else multiple progress tracker file will be created with difficulty to combine correct
                    # progress information
                    &#39;&#39;&#39;
                    executor.map(self.get_reviews, lst_of_lst, headless, proxy,
                                 rand_proxy, review_data, inc_list)
        try:
            if compile_progress_files:
                self.logger.info(&#39;Creating Combined Review File&#39;)
                if datetime.now().day &lt; 15:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
                else:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;
                rev_li = []
                self.bad_rev_li = []
                review_files = [f for f in self.current_progress_path.glob(
                    &#34;sph_prod_review_extract_progress_*&#34;)]
                for file in review_files:
                    try:
                        df = pd.read_csv(file)
                    except Exception:
                        self.bad_rev_li.append(file)
                    else:
                        rev_li.append(df)
                rev_df = pd.concat(rev_li, axis=0, ignore_index=True)
                rev_df.drop_duplicates(inplace=True)
                rev_df.reset_index(inplace=True, drop=True)
                rev_df[&#39;meta_date&#39;] = pd.to_datetime(meta_date).date()
                review_filename = f&#39;sph_product_review_all_{pd.to_datetime(meta_date).date()}&#39;
                # , index=None)
                rev_df.to_feather(self.review_path/review_filename)

                self.logger.info(
                    f&#39;Review file created. Please look for file sph_product_review_all in path {self.review_path}&#39;)
                print(
                    f&#39;Review file created. Please look for file sph_product_review_all in path {self.review_path}&#39;)

                if clean:
                    cleaner = Cleaner(path=self.path)
                    self.review_clean_df = cleaner.clean(
                        self.review_path/review_filename)
                    file_creation_status = True
            else:
                file_creation_status = False
        except Exception as ex:
            log_exception(
                self.logger, additional_information=f&#39;Review Combined File Creation Failed.&#39;)
            file_creation_status = False

        if delete_progress and file_creation_status:
            shutil.rmtree(
                f&#39;{self.review_path}\\current_progress&#39;, ignore_errors=True)
            self.logger.info(&#39;Progress files deleted&#39;)

    def terminate_logging(self):
        &#34;&#34;&#34;terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.
        &#34;&#34;&#34;
        self.logger.handlers.clear()
        self.prod_review_log.stop_log()


class Image(Sephora):

    &#34;&#34;&#34;Image [summary]

    [extended_summary]

    Args:
        Sephora ([type]): [description]

    &#34;&#34;&#34;

    def __init__(self, path: Path = Path.cwd(), log: bool = True):
        &#34;&#34;&#34;__init__ [summary]

        [extended_summary]

        Args:
            path (Path, optional): [description]. Defaults to Path.cwd().
            log (bool, optional): [description]. Defaults to True.
        &#34;&#34;&#34;
        super().__init__(path=path, data_def=&#39;image&#39;)

        if log:
            self.prod_image_log = Logger(
                &#34;sph_prod_image_extraction&#34;, path=self.crawl_log_path)
            self.logger, _ = self.prod_image_log.start_log()

    def get_images(self, indices: Union[list, range], open_headless: bool,
                   open_with_proxy_server: bool,
                   randomize_proxy_usage: bool) -&gt; None:
        &#34;&#34;&#34;get_images scrapes individual product images.

        Get Images download up to four images for on product.

        Args:
            indices (Union[list, range]): list of indices or range of indices of product urls to scrape.
            open_headless (bool): Whether to open browser headless.
            open_with_proxy_server (bool): Whether to use ip rotation service.
            randomize_proxy_usage (bool): Whether to use both proxy and native network in tandem to decrease proxy requests.
        &#34;&#34;&#34;

        for prod in self.meta.index[self.meta.index.isin(indices)]:
            if self.meta.loc[prod, &#39;image_scraped&#39;] in [&#39;Y&#39;, &#39;NA&#39;]:
                continue
            prod_id = self.meta.loc[prod, &#39;prod_id&#39;]
            product_page = self.meta.loc[prod, &#39;product_page&#39;]

            # create webdriver
            if randomize_proxy_usage:
                use_proxy = np.random.choice([True, False])
            else:
                use_proxy = True
            if open_with_proxy_server:
                # print(use_proxy)
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
                                        open_for_screenshot=True, path=self.image_path)
            else:
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
                                        open_for_screenshot=True, path=self.image_path)
            # open product page
            drv.get(product_page)
            time.sleep(15)  # 30
            accept_alert(drv, 10)
            close_popups(drv)

            try:
                product_text = drv.find_element_by_class_name(
                    &#39;css-1wag3se&#39;).text
                if &#39;productnotcarried&#39; in product_text.lower():
                    self.logger.info(str.encode(f&#39;prod_id: {prod_id} image extraction failed.\
                                            Product may not be available for sell currently.(page: {product_page})&#39;,
                                                &#39;utf-8&#39;, &#39;ignore&#39;))
                    self.meta.loc[prod, &#39;image_scraped&#39;] = &#39;NA&#39;
                    self.meta.to_csv(
                        self.image_path/&#39;sph_image_progress_tracker.csv&#39;, index=None)
                    drv.quit()
                    continue
            except Exception as ex:
                log_exception(
                    self.logger, additional_information=f&#39;Prod ID: {prod_id}&#39;)

            # get image elements
            try:
                accept_alert(drv, 1)
                close_popups(drv)
                images = drv.find_elements_by_class_name(&#39;css-11rgy2w&#39;)
                if len(images) == 0:
                    try:
                        accept_alert(drv, 1)
                        close_popups(drv)
                        images = drv.find_elements_by_class_name(&#39;css-11rgy2w&#39;)
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        self.logger.info(str.encode(f&#39;prod_id: {prod_id} failed to get image sources.\
                                                    (page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                        self.meta.loc[prod, &#39;image_scraped&#39;] = &#39;NA&#39;
                        self.meta.to_csv(
                            self.image_path/&#39;sph_image_progress_tracker.csv&#39;, index=None)
                        drv.quit()
                        continue
            except Exception:
                continue
            else:
                if len(images) == 0:
                    self.logger.info(str.encode(f&#39;{prod_id} image extraction failed.\
                                                    (page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    self.meta.loc[prod, &#39;image_scraped&#39;] = &#39;NA&#39;
                    self.meta.to_csv(
                        self.image_path/&#39;sph_image_progress_tracker.csv&#39;, index=None)
                    drv.quit()
                    continue
            # get image urls
            sources = [i.find_element_by_tag_name(
                &#39;img&#39;).get_attribute(&#39;src&#39;) for i in images]
            sources = [i.split(&#39;?&#39;)[0] for i in sources]

            if len(sources) &gt; 4:
                sources = sources[:3]

            self.current_image_path = self.image_path/prod_id
            if not self.current_image_path.exists():
                self.current_image_path.mkdir(parents=True, exist_ok=True)

            # download images
            image_count = 0
            try:
                for src in sources:
                    drv.get(src)
                    time.sleep(2)
                    accept_alert(drv, 1)
                    close_popups(drv)
                    image_count += 1
                    image_name = f&#39;{prod_id}_image_{image_count}.jpg&#39;
                    drv.save_screenshot(
                        str(self.current_image_path/image_name))
                self.meta.loc[prod, &#39;image_scraped&#39;] = &#39;Y&#39;
                drv.quit()
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                if image_count &lt;= 1:
                    self.logger.info(str.encode(f&#39;prod_id: {prod_id} image extraction failed.\
                                                    (page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    self.meta.loc[prod, &#39;image_scraped&#39;] = &#39;NA&#39;
                    self.meta.to_csv(
                        self.image_path/&#39;sph_image_progress_tracker.csv&#39;, index=None)
                drv.quit()
                continue

            if prod % 10 == 0 and prod != 0:
                self.meta.to_csv(
                    self.image_path/&#39;sph_image_progress_tracker.csv&#39;, index=None)
        self.meta.to_csv(
            self.image_path/&#39;sph_image_progress_tracker.csv&#39;, index=None)

    def extract(self, metadata: Union[pd.DataFrame, str, Path], download: bool = True,
                n_workers: int = 5, fresh_start: bool = False, auto_fresh_start: bool = False,
                open_headless: bool = False, open_with_proxy_server: bool = True,
                randomize_proxy_usage: bool = True,
                start_idx: Optional[int] = None, end_idx: Optional[int] = None, list_of_index=None,
                ):
        &#34;&#34;&#34;extract method controls all properties of the spiders and runs multi-threaded web crawling.

        Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
        This method has four major functionality:
        * 1. Run the spider
        * 2. Store data in regular intervals to free up ram
        * 3. Compile all crawled data into one file.
        * 4. Clean and push cleaned data to S3 storage for further algorithmic processing.

        Args:
            metadata (pd.DataFrame): Dataframe containing product specific url, name and id of the products to be scraped.
            download (bool, optional): Whether to crawl data from or compile crawled data into one file. Defaults to True.
            n_workers (int, optional): No. of parallel threads to run. Defaults to 5.
            fresh_start (bool, optional): Whether to continue last crawl job or start new one. Defaults to False.
            auto_fresh_start (bool, optional): Whether to automatically start a new crawl job if last job was finished. Defaults to False.
            open_headless (bool, optional):  Whether to open browser headless. Defaults to False.
            open_with_proxy_server (bool, optional): Whether to use ip rotation service. Defaults to True.
            randomize_proxy_usage (bool, optional): Whether to use both proxy and native network in tandem to decrease proxy requests.
                                                    Defaults to False.
            start_idx (Optional[int], optional): Starting index of the links to crawl. Defaults to None.
            end_idx (Optional[int], optional): Ending index of the links to crawl. Defaults to None.
            list_of_index ([type], optional): List of indices or range of indices of product urls to scrape. Defaults to None.
            compile_progress_files (bool, optional): Whether to combine crawled data into one file. Defaults to False.
            clean (bool, optional): Whether to clean the compiled data. Defaults to True.
            delete_progress (bool, optional): Whether to delete intermediate data after compilation into one file. Defaults to False.
        &#34;&#34;&#34;
        def fresh():
            self.meta = metadata[[&#39;prod_id&#39;, &#39;product_page&#39;]]
            self.meta[&#39;image_scraped&#39;] = &#39;N&#39;

        if download:
            if fresh_start:
                fresh()
            else:
                if Path(self.image_path/&#39;sph_image_progress_tracker.csv&#39;).exists():
                    self.meta = pd.read_csv(
                        self.image_path/&#39;sph_image_progress_tracker.csv&#39;)
                    if sum(self.meta.image_scraped == &#39;N&#39;) == 0:
                        if auto_fresh_start:
                            fresh()
                            self.logger.info(
                                &#39;Last Run was Completed. Starting Fresh Extraction.&#39;)
                        else:
                            self.logger.info(
                                f&#39;Image extraction for this cycle is complete. Please check files in path: {self.image_path}&#39;)
                            print(
                                f&#39;Image extraction for this cycle is complete. Please check files in path: {self.image_path}&#39;)
                    else:
                        self.logger.info(
                            &#39;Continuing Image Extraction From Last Run.&#39;)

            self.meta.to_csv(
                self.image_path/&#39;sph_image_progress_tracker.csv&#39;, index=None)
            self.meta.reset_index(inplace=True, drop=True)

            # set list or range of product indices to crawl
            if list_of_index:
                indices = list_of_index
            elif start_idx and end_idx is None:
                indices = range(start_idx, len(self.meta))
            elif start_idx is None and end_idx:
                indices = range(0, end_idx)
            elif start_idx is not None and end_idx is not None:
                indices = range(start_idx, end_idx)
            else:
                indices = range(len(self.meta))

            if list_of_index:
                self.get_images(
                    indices=list_of_index, open_headless=open_headless,
                    open_with_proxy_server=open_with_proxy_server,
                    randomize_proxy_usage=randomize_proxy_usage)
            else:  # By default the code will with 5 concurrent threads. you can change this behaviour by changing n_workers
                if start_idx:
                    lst_of_lst = ranges(
                        indices[-1]+1, n_workers, start_idx=start_idx)
                else:
                    lst_of_lst = ranges(len(indices), n_workers)
                print(lst_of_lst)

                headless = [open_headless for i in lst_of_lst]
                proxy = [open_with_proxy_server for i in lst_of_lst]
                rand_proxy = [randomize_proxy_usage for i in lst_of_lst]

                with concurrent.futures.ThreadPoolExecutor() as executor:
                    executor.map(self.get_images, lst_of_lst, headless,
                                 proxy, rand_proxy)

        self.logger.info(
            f&#39;Image files are downloaded to product specific folders. \
            Please look for file sph_product_review_all in path {self.image_path}&#39;)
        print(
            f&#39;Image files are downloaded to product specific folders. \
            Please look for file sph_product_review_all in path {self.image_path}&#39;)

    def terminate_logging(self) -&gt; None:
        &#34;&#34;&#34;terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.
        &#34;&#34;&#34;
        self.logger.handlers.clear()
        self.prod_image_log.stop_log()


class DetailReview(Sephora):
    &#34;&#34;&#34;DetailReview [summary]

    [extended_summary]

    Args:
        Sephora ([type]): [description]
    &#34;&#34;&#34;

    def __init__(self, log: bool = True, path: Path = Path.cwd()):
        &#34;&#34;&#34;__init__ [summary]

        [extended_summary]

        Args:
            log (bool, optional): [description]. Defaults to True.
            path (Path, optional): [description]. Defaults to Path.cwd().
        &#34;&#34;&#34;
        super().__init__(path=path, data_def=&#39;detail_review_image&#39;)
        self.path = path
        self.detail_current_progress_path = self.detail_path/&#39;current_progress&#39;
        self.detail_current_progress_path.mkdir(parents=True, exist_ok=True)

        self.review_current_progress_path = self.review_path/&#39;current_progress&#39;
        self.review_current_progress_path.mkdir(parents=True, exist_ok=True)

        old_detail_files = list(self.detail_path.glob(
            &#39;sph_product_detail_all*&#39;)) + list(self.detail_path.glob(
                &#39;sph_product_item_all*&#39;))
        for f in old_detail_files:
            shutil.move(str(f), str(self.old_detail_files_path))

        old_clean_detail_files = files = os.listdir(self.detail_clean_path)
        for f in old_clean_detail_files:
            shutil.move(str(self.detail_clean_path/f),
                        str(self.old_detail_clean_files_path))

        old_review_files = list(self.review_path.glob(
            &#39;sph_product_review_all*&#39;))
        for f in old_review_files:
            shutil.move(str(f), str(self.old_review_files_path))

        old_clean_review_files = os.listdir(self.review_clean_path)
        for f in old_clean_review_files:
            shutil.move(str(self.review_clean_path/f),
                        str(self.old_review_clean_files_path))
        if log:
            self.prod_detail_review_image_log = Logger(
                &#34;sph_prod_review_extraction&#34;, path=self.crawl_log_path)
            self.logger, _ = self.prod_detail_review_image_log.start_log()

    def get_details(self, drv: webdriver.Firefox, prod_id: str, product_name: str) -&gt; Tuple[dict, pd.DataFrame]:
        &#34;&#34;&#34;get_detail [summary]

        [extended_summary]

        Args:
            drv (webdriver.Firefox): [description]
            prod_id (str): [description]
            product_name (str): [description]

        Returns:
            Tuple[dict, pd.DataFrame]: [description]
        &#34;&#34;&#34;

        def get_item_attributes(drv: webdriver.Firefox, product_name: str, prod_id: str, use_button: bool = False,
                                multi_variety: bool = False, typ=None, ) -&gt; Tuple[str, str, str, str]:
            &#34;&#34;&#34;get_item_attributes [summary]

            [extended_summary]

            Args:
                drv (webdriver.Chrome): [description]
                product_name (str): [description]
                prod_id (str): [description]
                use_button (bool, optional): [description]. Defaults to False.
                multi_variety (bool, optional): [description]. Defaults to False.
                typ ([type], optional): [description]. Defaults to None.

            Returns:
                Tuple[str, str, str, str]: [description]
            &#34;&#34;&#34;
            # drv # type: webdriver.Chrome
            # close popup windows
            close_popups(drv)
            accept_alert(drv, 1)

            try:
                item_price = drv.find_element_by_class_name(&#39;css-1865ad6&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                item_price = &#39;&#39;
            # print(item_price)

            if multi_variety:
                try:
                    if use_button:
                        item_name = typ.find_element_by_tag_name(
                            &#39;button&#39;).get_attribute(&#39;aria-label&#39;)
                    else:
                        item_name = typ.get_attribute(&#39;aria-label&#39;)
                    # print(item_name)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    item_name = &#34;&#34;
                    self.logger.info(str.encode(
                        f&#39;product: {product_name} (prod_id: {prod_id}) item_name does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            else:
                item_name = &#34;&#34;

            try:
                item_size = drv.find_element_by_class_name(&#39;css-128n72s&#39;).text
                # print(item_size)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                item_size = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) item_size does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            # get all tabs
            first_tab = drv.find_element_by_id(f&#39;tab{0}&#39;)
            self.scroll_to_element(drv, first_tab)
            ActionChains(drv).move_to_element(
                first_tab).click(first_tab).perform()
            prod_tabs = []
            prod_tabs = drv.find_elements_by_class_name(&#39;css-1wugx5m&#39;)
            prod_tabs.extend(drv.find_elements_by_class_name(&#39;css-12vae0p&#39;))

            tab_names = []
            for t in prod_tabs:
                tab_names.append(t.text.lower())
            # print(tab_names)

            if &#39;ingredients&#39; in tab_names:
                close_popups(drv)
                accept_alert(drv, 1)
                if len(tab_names) == 5:
                    try:
                        tab_num = 2
                        ing_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                        self.scroll_to_element(drv, ing_button)
                        ActionChains(drv).move_to_element(
                            ing_button).click(ing_button).perform()
                        item_ing = drv.find_element_by_xpath(
                            f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        print(&#39;cant get ingredient but tab exists&#39;)
                        item_ing = &#34;&#34;
                        self.logger.info(str.encode(
                            f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients extraction failed&#39;,
                            &#39;utf-8&#39;, &#39;ignore&#39;))
                elif len(tab_names) == 4:
                    try:
                        tab_num = 1
                        ing_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                        self.scroll_to_element(drv, ing_button)
                        ActionChains(drv).move_to_element(
                            ing_button).click(ing_button).perform()
                        item_ing = drv.find_element_by_xpath(
                            f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        print(&#39;cant get ingredient but tab exists&#39;)
                        item_ing = &#34;&#34;
                        self.logger.info(str.encode(
                            f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients extraction failed.&#39;,
                            &#39;utf-8&#39;, &#39;ignore&#39;))
                elif len(tab_names) &lt; 4:
                    try:
                        tab_num = 0
                        ing_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                        self.scroll_to_element(drv, ing_button)
                        ActionChains(drv).move_to_element(
                            ing_button).click(ing_button).perform()
                        item_ing = drv.find_element_by_xpath(
                            f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        print(&#39;cant get ingredient but tab exists&#39;)
                        item_ing = &#34;&#34;
                        self.logger.info(str.encode(
                            f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients extraction failed.&#39;,
                            &#39;utf-8&#39;, &#39;ignore&#39;))
            else:
                item_ing = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            # print(item_ing)
            return item_name, item_size, item_price, item_ing

        def get_product_attributes(drv: webdriver.Firefox, product_name: str, prod_id: str) -&gt; list:
            &#34;&#34;&#34;get_product_attributes [summary]

            [extended_summary]

            Args:
                drv (webdriver.Chrome): [description]
                product_name (str): [description]
                prod_id (str): [description]

            Returns:
                list: [description]
            &#34;&#34;&#34;
            # get all the variation of product
            # close popup windows
            close_popups(drv)
            accept_alert(drv, 1)

            product_variety = []
            try:
                product_variety = drv.find_elements_by_class_name(
                    &#39;css-1j1jwa4&#39;)
                product_variety.extend(
                    drv.find_elements_by_class_name(&#39;css-cl742e&#39;))
                use_button = False
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
            try:
                if len(product_variety) &lt; 1:
                    product_variety = drv.find_elements_by_class_name(
                        &#39;css-5jqxch&#39;)
                    use_button = True
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)

            product_attributes = []

            if len(product_variety) &gt; 0:
                for typ in product_variety:
                    close_popups(drv)
                    accept_alert(drv, 1)
                    try:
                        self.scroll_to_element(drv, typ)
                        ActionChains(drv).move_to_element(
                            typ).click(typ).perform()
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    time.sleep(4)  # 8
                    item_name, item_size, item_price, item_ingredients = get_item_attributes(drv, product_name, prod_id,
                                                                                             multi_variety=True, typ=typ,
                                                                                             use_button=use_button)
                    product_attributes.append({&#34;prod_id&#34;: prod_id, &#34;product_name&#34;: product_name,
                                               &#34;item_name&#34;: item_name, &#34;item_size&#34;: item_size,
                                               &#34;item_price&#34;: item_price, &#34;item_ingredients&#34;: item_ingredients})
            else:
                item_name, item_size, item_price, item_ingredients = get_item_attributes(drv,
                                                                                         product_name, prod_id)
                product_attributes.append({&#34;prod_id&#34;: prod_id, &#34;product_name&#34;: product_name, &#34;item_name&#34;: item_name,
                                           &#34;item_size&#34;: item_size, &#34;item_price&#34;: item_price, &#34;item_ingredients&#34;: item_ingredients})

            return product_attributes

        def get_first_review_date(drv: webdriver.Firefox) -&gt; str:
            &#34;&#34;&#34;get_first_review_date [summary]

            [extended_summary]

            Args:
                drv (webdriver.Chrome): [description]

            Returns:
                str: [description]
            &#34;&#34;&#34;
            # close popup windows
            close_popups(drv)
            accept_alert(drv, 1)

            try:
                review_sort_trigger = drv.find_element_by_id(
                    &#39;review_filter_sort_trigger&#39;)
                self.scroll_to_element(drv, review_sort_trigger)
                ActionChains(drv).move_to_element(
                    review_sort_trigger).click(review_sort_trigger).perform()
                for btn in drv.find_elements_by_class_name(&#39;css-rfz1gg&#39;):
                    if btn.text.lower() == &#39;oldest&#39;:
                        ActionChains(drv).move_to_element(
                            btn).click(btn).perform()
                        break
                time.sleep(6)
                close_popups(drv)
                accept_alert(drv, 1)
                rev = drv.find_elements_by_class_name(&#39;css-1kk8dps&#39;)[2:]
                try:
                    first_review_date = convert_ago_to_date(
                        rev[0].find_element_by_class_name(&#39;css-h2vfi1&#39;).text)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    try:
                        first_review_date = convert_ago_to_date(
                            rev[1].find_element_by_class_name(&#39;css-h2vfi1&#39;).text)
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        print(&#39;sorted but cant get first review date value&#39;)
                        first_review_date = &#39;&#39;
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                first_review_date = &#39;&#39;
            return first_review_date

        # get all product info tabs such as how-to-use, about-brand, ingredients
        prod_tabs = []
        prod_tabs = drv.find_elements_by_class_name(&#39;css-1wugx5m&#39;)
        prod_tabs.extend(drv.find_elements_by_class_name(&#39;css-12vae0p&#39;))

        tab_names = []
        for t in prod_tabs:
            tab_names.append(t.text.lower())

        # no. of votes
        try:
            votes = drv.find_elements_by_class_name(&#39;css-2rg6q7&#39;)[-1].text
            # print(votes)
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            votes = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) votes does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        # product details
        if &#39;details&#39; in tab_names:
            try:
                close_popups(drv)
                accept_alert(drv, 1)
                tab_num = tab_names.index(&#39;details&#39;)
                detail_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                try:
                    time.sleep(1)
                    self.scroll_to_element(drv, detail_button)
                    ActionChains(drv).move_to_element(
                        detail_button).click(detail_button).perform()
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    details = &#34;&#34;
                else:
                    try:
                        details = drv.find_element_by_xpath(
                            f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        details = &#34;&#34;
                        self.logger.info(str.encode(
                            f&#39;product: {product_name} (prod_id: {prod_id}) product detail text\
                                    does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    # print(details)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                details = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) product detail extraction failed&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
        else:
            details = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) product detail does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        # how to use
        if &#39;how to use&#39; in tab_names:
            try:
                close_popups(drv)
                accept_alert(drv, 1)

                tab_num = tab_names.index(&#39;how to use&#39;)
                how_to_use_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                try:
                    time.sleep(1)
                    self.scroll_to_element(drv, how_to_use_button)
                    ActionChains(drv).move_to_element(
                        how_to_use_button).click(how_to_use_button).perform()
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    how_to_use = &#34;&#34;
                else:
                    try:
                        how_to_use = drv.find_element_by_xpath(
                            f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        how_to_use = &#34;&#34;
                        self.logger.info(str.encode(
                            f&#39;product: {product_name} (prod_id: {prod_id}) how_to_use text\
                                    does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    # print(how_to_use)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                how_to_use = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) how_to_use extraction failed&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
        else:
            how_to_use = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) how_to_use does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        # about the brand
        if &#39;about the brand&#39; in tab_names:
            try:
                close_popups(drv)
                accept_alert(drv, 1)

                tab_num = tab_names.index(&#39;about the brand&#39;)
                about_the_brand_button = drv.find_element_by_id(
                    f&#39;tab{tab_num}&#39;)
                try:
                    time.sleep(1)
                    self.scroll_to_element(drv, about_the_brand_button)
                    ActionChains(drv).move_to_element(
                        about_the_brand_button).click(about_the_brand_button).perform()
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    about_the_brand = &#34;&#34;
                else:
                    try:
                        about_the_brand = drv.find_element_by_xpath(
                            f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        about_the_brand = &#34;&#34;
                        self.logger.info(str.encode(
                            f&#39;product: {product_name} (prod_id: {prod_id}) about_the_brand text\
                                does not exist&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    # print(about_the_brand)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                about_the_brand = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) about_the_brand extraction failed&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
        else:
            about_the_brand = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) about_the_brand does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        self.scroll_down_page(drv, h2=0.4, speed=5)
        time.sleep(5)
        try:
            chat_popup_button = WebDriverWait(drv, 3).until(
                EC.presence_of_element_located((By.XPATH, &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)))
            chat_popup_button = drv.find_element_by_xpath(
                &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)
            self.scroll_to_element(drv, chat_popup_button)
            ActionChains(drv).move_to_element(
                chat_popup_button).click(chat_popup_button).perform()
        except TimeoutException:
            pass
        # click no. of reviews
        try:
            review_button = drv.find_element_by_class_name(&#39;css-1pjru6n&#39;)
            self.scroll_to_element(drv, review_button)
            ActionChains(drv).move_to_element(
                review_button).click(review_button).perform()
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)

        try:
            first_review_date = get_first_review_date(drv)
            # print(first_review_date)
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            first_review_date = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) first_review_date scrape failed.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        try:
            close_popups(drv)
            accept_alert(drv, 1)
            reviews = int(drv.find_element_by_class_name(
                &#39;css-ils4e4&#39;).text.split()[0])
            # print(reviews)
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            reviews = 0
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) reviews does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        try:
            close_popups(drv)
            accept_alert(drv, 1)
            rating_distribution = drv.find_element_by_class_name(
                &#39;css-960eb6&#39;).text.split(&#39;\n&#39;)
            # print(rating_distribution)
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            rating_distribution = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) rating_distribution does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        try:
            close_popups(drv)
            accept_alert(drv, 1)
            would_recommend = drv.find_element_by_class_name(
                &#39;css-k9ne19&#39;).text
            # print(would_recommend)
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            would_recommend = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) would_recommend does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        detail = {&#39;prod_id&#39;: prod_id, &#39;product_name&#39;: product_name, &#39;abt_product&#39;: details,
                  &#39;how_to_use&#39;: how_to_use, &#39;abt_brand&#39;: about_the_brand,
                  &#39;reviews&#39;: reviews, &#39;votes&#39;: votes, &#39;rating_dist&#39;: rating_distribution,
                  &#39;would_recommend&#39;: would_recommend, &#39;first_review_date&#39;: first_review_date}

        item = pd.DataFrame(
            get_product_attributes(drv, product_name, prod_id))

        return detail, item

    def get_reviews(self,  drv: webdriver.Firefox, prod_id: str, product_name: str,
                    last_scraped_review_date: str, no_of_reviews: int,
                    incremental: bool = True, reviews: list = []) -&gt; list:
        &#34;&#34;&#34;get_reviews [summary]

        [extended_summary]

        Args:
            drv (webdriver.Firefox): [description]
            prod_id (str): [description]
            product_name (str): [description]
            last_scraped_review_date (str): [description]
            no_of_reviews (int): [description]
            incremental (bool, optional): [description]. Defaults to True.
            reviews (list, optional): [description]. Defaults to [].

        Returns:
            list: [description]
        &#34;&#34;&#34;
        # print(no_of_reviews)
        # drv.find_element_by_class_name(&#39;css-2rg6q7&#39;).click()
        if incremental and last_scraped_review_date != &#39;&#39;:
            for n in range(no_of_reviews//6):
                if n &gt; 400:
                    break

                time.sleep(0.4)
                revs = drv.find_elements_by_class_name(
                    &#39;css-1kk8dps&#39;)[2:]

                try:
                    if pd.to_datetime(convert_ago_to_date(revs[-1].find_element_by_class_name(&#39;css-1t84k9w&#39;).text),
                                      infer_datetime_format=True)\
                            &lt; pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                        # print(&#39;breaking incremental&#39;)
                        break
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    try:
                        if pd.to_datetime(convert_ago_to_date(revs[-2].find_element_by_class_name(&#39;css-1t84k9w&#39;).text),
                                          infer_datetime_format=True)\
                                &lt; pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                            # print(&#39;breaking incremental&#39;)
                            break
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        self.logger.info(str.encode(f&#39;Product: {product_name} prod_id: {prod_id} \
                                                        last_scraped_review_date to current review date \
                                                        comparision failed.(page: {product_page})&#39;,
                                                    &#39;utf-8&#39;, &#39;ignore&#39;))
                        # print(&#39;in second except block&#39;)
                        continue
                try:
                    show_more_review_button = drv.find_element_by_class_name(
                        &#39;css-xswy5p&#39;)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to get show more review button.&#39;)
                else:
                    try:
                        self.scroll_to_element(
                            drv, show_more_review_button)
                        ActionChains(drv).move_to_element(
                            show_more_review_button).click(show_more_review_button).perform()
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)
                        accept_alert(drv, 1)
                        close_popups(drv)
                        try:
                            self.scroll_to_element(
                                drv, show_more_review_button)
                            ActionChains(drv).move_to_element(
                                show_more_review_button).click(show_more_review_button).perform()
                        except Exception as ex:
                            log_exception(self.logger,
                                          additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)

        else:
            # print(&#39;inside get all reviews&#39;)
            # 6 because for click sephora shows 6 reviews. additional 25 no. of clicks for buffer.
            for n in range(no_of_reviews//6+10):
                &#39;&#39;&#39;
                code will stop after getting 1800 reviews of one particular product
                when crawling all reviews. By default it will get latest 1800 reviews.
                then in subsequent incremental runs it will get al new reviews on weekly basis
                &#39;&#39;&#39;
                if n &gt;= 400:  # 200:
                    break
                time.sleep(1)
                # close any opened popups by escape
                accept_alert(drv, 1)
                close_popups(drv)
                try:
                    show_more_review_button = drv.find_element_by_class_name(
                        &#39;css-xswy5p&#39;)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to get show more review button.&#39;)
                else:
                    try:
                        self.scroll_to_element(
                            drv, show_more_review_button)
                        ActionChains(drv).move_to_element(
                            show_more_review_button).click(show_more_review_button).perform()
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)
                        accept_alert(drv, 1)
                        close_popups(drv)
                        try:
                            self.scroll_to_element(
                                drv, show_more_review_button)
                            ActionChains(drv).move_to_element(
                                show_more_review_button).click(show_more_review_button).perform()
                        except Exception as ex:
                            log_exception(self.logger,
                                          additional_information=f&#39;Prod ID: {prod_id}.\
                                                Failed to click on show more review button.&#39;)
                            try:
                                self.scroll_to_element(
                                    drv, show_more_review_button)
                                ActionChains(drv).move_to_element(
                                    show_more_review_button).click(show_more_review_button).perform()
                            except Exception as ex:
                                log_exception(self.logger,
                                              additional_information=f&#39;Prod ID: {prod_id}.\
                                                Failed to click on show more review button.&#39;)
                                accept_alert(drv, 2)
                                close_popups(drv)
                                try:
                                    self.scroll_to_element(
                                        drv, show_more_review_button)
                                    ActionChains(drv).move_to_element(
                                        show_more_review_button).click(show_more_review_button).perform()
                                except Exception as ex:
                                    log_exception(self.logger,
                                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)
                                    if n &lt; (no_of_reviews//6):
                                        self.logger.info(str.encode(f&#39;Product: {product_name} - prod_id \
                                            {prod_id} breaking click next review loop.\
                                                                    [total_reviews:{no_of_reviews} loaded_reviews:{n}]\
                                                                    (page link: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                                        self.logger.info(str.encode(f&#39;Product: {product_name} - prod_id {prod_id} cant load all reviews.\
                                                                        Check click next 6 reviews\
                                                                        code section(page link: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                                    break

        accept_alert(drv, 2)
        close_popups(drv)

        product_reviews = drv.find_elements_by_class_name(
            &#39;css-1kk8dps&#39;)[2:]

        # print(&#39;starting extraction&#39;)
        r = 0
        for rev in product_reviews:
            accept_alert(drv, 0.5)
            close_popups(drv)
            self.scroll_to_element(drv, rev)
            ActionChains(drv).move_to_element(rev).perform()

            try:
                try:
                    review_text = rev.find_element_by_class_name(
                        &#39;css-1jg2pb9&#39;).text
                except NoSuchElementException:
                    review_text = rev.find_element_by_class_name(
                        &#39;css-429528&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_text. Skip review.&#39;)
                continue

            try:
                review_date = convert_ago_to_date(
                    rev.find_element_by_class_name(&#39;css-h2vfi1&#39;).text)
                if pd.to_datetime(review_date, infer_datetime_format=True) &lt;= \
                        pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                    continue
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_date.&#39;)
                review_date = &#39;&#39;

            try:
                review_title = rev.find_element_by_class_name(
                    &#39;css-1jfmule&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_title.&#39;)
                review_title = &#39;&#39;

            try:
                product_variant = rev.find_element_by_class_name(
                    &#39;css-1op1cn7&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract product_variant.&#39;)
                product_variant = &#39;&#39;

            try:
                user_rating = rev.find_element_by_class_name(
                    &#39;css-3z5ot7&#39;).get_attribute(&#39;aria-label&#39;)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract user_rating.&#39;)
                user_rating = &#39;&#39;

            try:
                user_attribute = [{&#39;_&#39;.join(u.lower().split()[0:-1]): u.lower().split()[-1]}
                                  for u in rev.find_element_by_class_name(&#39;css-ecreye&#39;).text.split(&#39;\n&#39;)]
                # user_attribute = []
                # for u in rev.find_elements_by_class_name(&#39;css-j5yt83&#39;):
                #     user_attribute.append(
                #         {&#39;_&#39;.join(u.text.lower().split()[0:-1]): u.text.lower().split()[-1]})
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract user_attribute.&#39;)
                user_attribute = []

            try:
                recommend = rev.find_element_by_class_name(
                    &#39;css-1tf5yph&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract recommend.&#39;)
                recommend = &#39;&#39;

            try:
                helpful = rev.find_element_by_class_name(&#39;css-b7zg5r&#39;).text
                # helpful = []
                # for h in rev.find_elements_by_class_name(&#39;css-39esqn&#39;):
                #     helpful.append(h.text)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract helpful.&#39;)
                helpful = &#39;&#39;

            reviews.append({&#39;prod_id&#39;: prod_id, &#39;product_name&#39;: product_name,
                            &#39;user_attribute&#39;: user_attribute, &#39;product_variant&#39;: product_variant,
                            &#39;review_title&#39;: review_title, &#39;review_text&#39;: review_text,
                            &#39;review_rating&#39;: user_rating, &#39;recommend&#39;: recommend,
                            &#39;review_date&#39;: review_date,   &#39;helpful&#39;: helpful})
        return reviews

    def crawl_page(self, indices: list, open_headless: bool, open_with_proxy_server: bool,
                   randomize_proxy_usage: bool, detail_data: list = [],
                   item_df=pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                                 &#39;item_size&#39;, &#39;item_price&#39;,
                                                 &#39;item_ingredients&#39;]),
                   review_data: list = [], incremental: bool = True):
        &#34;&#34;&#34;crawl_page

        [extended_summary]

        Args:
            indices (list): [description]
            open_headless (bool): [description]
            open_with_proxy_server (bool): [description]
            randomize_proxy_usage (bool): [description]
            detail_data (list, optional): [description]. Defaults to [].
            item_df ([type], optional): [description]. Defaults to pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
            &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;]).
            review_data (list, optional): [description]. Defaults to [].
            incremental (bool, optional): [description]. Defaults to True.
        &#34;&#34;&#34;

        def store_data_refresh_mem(detail_data: list, item_df: pd.DataFrame,
                                   review_data: list) -&gt; Tuple[list, pd.DataFrame, list]:
            &#34;&#34;&#34;store_data_refresh_mem [summary]

            [extended_summary]

            Args:
                detail_data (list): [description]
                item_df (pd.DataFrame): [description]
                review_data (list): [description]

            Returns:
                Tuple[list, pd.DataFrame, list]: [description]
            &#34;&#34;&#34;
            pd.DataFrame(detail_data).to_csv(self.detail_current_progress_path /
                                             f&#39;sph_prod_detail_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                                             index=None)
            item_df.reset_index(inplace=True, drop=True)
            item_df.to_csv(self.detail_current_progress_path /
                           f&#39;sph_prod_item_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                           index=None)
            item_df = pd.DataFrame(columns=[
                                   &#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;, &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;])

            pd.DataFrame(review_data).to_csv(self.review_current_progress_path /
                                             f&#39;sph_prod_review_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                                             index=None)
            self.meta.to_csv(
                self.path/&#39;sph_detail_review_image_progress_tracker.csv&#39;, index=None)
            return [], item_df, []

        for prod in self.meta.index[self.meta.index.isin(indices)]:
            #  ignore already extracted products
            if self.meta.loc[prod, &#39;scraped&#39;] in [&#39;Y&#39;, &#39;NA&#39;]:
                continue
            # print(prod, self.meta.loc[prod, &#39;detail_scraped&#39;])
            prod_id = self.meta.loc[prod, &#39;prod_id&#39;]
            product_name = self.meta.loc[prod, &#39;product_name&#39;]
            product_page = self.meta.loc[prod, &#39;product_page&#39;]
            last_scraped_review_date = self.meta.loc[prod,
                                                     &#39;last_scraped_review_date&#39;]
            # create webdriver
            if randomize_proxy_usage:
                use_proxy = np.random.choice([True, False])
            else:
                use_proxy = True
            if open_with_proxy_server:
                # print(use_proxy)
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
                                        path=self.detail_path)
            else:
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
                                        path=self.detail_path)
            # open product page
            drv.get(product_page)
            time.sleep(20)  # 30
            accept_alert(drv, 10)
            close_popups(drv)

            self.scroll_down_page(drv, speed=6, h2=0.6)
            time.sleep(5)

            try:
                product_text = drv.find_element_by_class_name(
                    &#39;css-1wag3se&#39;).text
                if &#39;productnotcarried&#39; in product_text.lower():
                    self.logger.info(str.encode(f&#39;Product Name: {product_name}, Product ID: {prod_id} extraction failed.\
                                            Product may not be available for sell currently.(Page: {product_page})&#39;,
                                                &#39;utf-8&#39;, &#39;ignore&#39;))
                    self.meta.loc[prod, &#39;scraped&#39;] = &#39;NA&#39;
                    self.meta.to_csv(
                        self.path/&#39;sph_detail_review_image_progress_tracker.csv&#39;, index=None)
                    drv.quit()
                    continue
            except Exception as ex:
                log_exception(
                    self.logger, additional_information=f&#39;Prod ID: {prod_id}&#39;)

            # check product page is valid and exists
            try:
                close_popups(drv)
                accept_alert(drv, 2)
                price = drv.find_element_by_class_name(&#39;css-1865ad6&#39;)
                self.scroll_to_element(drv, price)
                price = price.text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                drv.quit()
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) no longer exists in the previously fetched link.\
                        (link:{product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                self.meta.loc[prod, &#39;detail_scraped&#39;] = &#39;NA&#39;
                continue

            try:
                chat_popup_button = WebDriverWait(drv, 3).until(
                    EC.presence_of_element_located((By.XPATH, &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)))
                chat_popup_button = drv.find_element_by_xpath(
                    &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)
                self.scroll_to_element(drv, chat_popup_button)
                ActionChains(drv).move_to_element(
                    chat_popup_button).click(chat_popup_button).perform()
            except TimeoutException:
                pass

            detail, item = self.get_details(drv, prod_id, product_name)

            detail_data.append(detail)
            item_df = pd.concat(
                [item_df, item], axis=0)

            # item_data.append(product_attributes)
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) details extracted successfully&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            try:
                close_popups(drv)
                accept_alert(drv, 1)
                no_of_reviews = int(drv.find_element_by_class_name(
                    &#39;css-ils4e4&#39;).text.split()[0])
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                self.logger.info(str.encode(f&#39;Product: {product_name} prod_id: {prod_id} reviews extraction failed.\
                                              Either product has no reviews or not\
                                              available for sell currently.(page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                no_of_reviews = 0
                # self.meta.loc[prod, &#39;review_scraped&#39;] = &#34;NA&#34;
                # self.meta.to_csv(
                #     self.review_path/&#39;sph_review_progress_tracker.csv&#39;, index=None)
                # drv.quit()
                # # print(&#39;in except - continue&#39;)
                # continue

            if no_of_reviews &gt; 0:
                reviews = self.get_reviews(
                    drv, prod_id, product_name, last_scraped_review_date, no_of_reviews, incremental)
                if len(reviews) &gt; 0:
                    review_data.extend(reviews)

            if not incremental:
                self.logger.info(str.encode(
                    f&#39;Product_name: {product_name} prod_id:{prod_id} reviews extracted successfully.(total_reviews: {no_of_reviews}, \
                    extracted_reviews: {len(reviews)}, page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            else:
                self.logger.info(str.encode(
                    f&#39;Product_name: {product_name} prod_id:{prod_id} new reviews extracted successfully.\
                        (no_of_new_extracted_reviews: {len(reviews)},\
                         page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            if prod != 0 and prod % 5 == 0:
                detail_data, item_df, review_data = store_data_refresh_mem(
                    detail_data, item_df, review_data)

            self.meta.loc[prod, &#39;scraped&#39;] = &#39;Y&#39;
            drv.quit()

        detail_data, item_df, review_data = store_data_refresh_mem(
            detail_data, item_df, review_data)
        self.logger.info(
            f&#39;Extraction Complete for start_idx: (indices[0]) to end_idx: {indices[-1]}. Or for list of values.&#39;)

    def extract(self, metadata: Union[pd.DataFrame, str, Path], download: bool = True, n_workers: int = 5,
                fresh_start: bool = False, auto_fresh_start: bool = False, incremental: bool = True,
                open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = True,
                start_idx: Optional[int] = None, end_idx: Optional[int] = None, list_of_index=None,
                compile_progress_files: bool = False, clean: bool = True, delete_progress: bool = False):
        &#34;&#34;&#34;extract [summary]

        [extended_summary]

        Args:
            metadata (Union[pd.DataFrame, str, Path]): [description]
            download (bool, optional): [description]. Defaults to True.
            n_workers (int, optional): [description]. Defaults to 5.
            fresh_start (bool, optional): [description]. Defaults to False.
            auto_fresh_start (bool, optional): [description]. Defaults to False.
            incremental (bool, optional): [description]. Defaults to True.
            open_headless (bool, optional): [description]. Defaults to False.
            open_with_proxy_server (bool, optional): [description]. Defaults to True.
            randomize_proxy_usage (bool, optional): [description]. Defaults to True.
            start_idx (Optional[int], optional): [description]. Defaults to None.
            end_idx (Optional[int], optional): [description]. Defaults to None.
            list_of_index ([type], optional): [description]. Defaults to None.
            compile_progress_files (bool, optional): [description]. Defaults to False.
            clean (bool, optional): [description]. Defaults to True.
            delete_progress (bool, optional): [description]. Defaults to False.
        &#34;&#34;&#34;
        def fresh():
            if not isinstance(metadata, pd.core.frame.DataFrame):
                list_of_files = self.metadata_clean_path.glob(
                    &#39;no_cat_cleaned_sph_product_metadata_all*&#39;)
                self.meta = pd.read_feather(max(list_of_files, key=os.path.getctime))[
                    [&#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;, &#39;last_scraped_review_date&#39;]]
            else:
                self.meta = metadata[[
                    &#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;, &#39;last_scraped_review_date&#39;]]
            self.meta.last_scraped_review_date.fillna(&#39;&#39;, inplace=True)
            self.meta[&#39;scraped&#39;] = &#39;N&#39;

        if download:
            if fresh_start:
                fresh()
                self.logger.info(
                    &#39;Starting Fresh Deatil Review Image Extraction.&#39;)
            else:
                if Path(self.path/&#39;sph_detail_review_image_progress_tracker.csv&#39;).exists():
                    self.meta = pd.read_csv(
                        self.path/&#39;sph_detail_review_image_progress_tracker.csv&#39;)
                    if sum(self.meta.scraped == &#39;N&#39;) == 0:
                        if auto_fresh_start:
                            fresh()
                            self.logger.info(
                                &#39;Last Run was Completed. Starting Fresh Extraction.&#39;)
                        else:
                            self.logger.info(
                                &#39;Deatil Review Image extraction for this cycle is complete.&#39;)
                    else:
                        self.logger.info(
                            &#39;Continuing Deatil Review Image Extraction From Last Run.&#39;)
                else:
                    fresh()
                    self.logger.info(
                        &#39;Deatil Review Image Progress Tracker does not exist. Starting Fresh Extraction.&#39;)

            # set list or range of product indices to crawl
            if list_of_index:
                indices = list_of_index
            elif start_idx and end_idx is None:
                indices = range(start_idx, len(self.meta))
            elif start_idx is None and end_idx:
                indices = range(0, end_idx)
            elif start_idx is not None and end_idx is not None:
                indices = range(start_idx, end_idx)
            else:
                indices = range(len(self.meta))
            print(indices)

            if list_of_index:
                self.crawl_page(indices=list_of_index, incremental=incremental,
                                open_headless=open_headless,
                                open_with_proxy_server=open_with_proxy_server,
                                randomize_proxy_usage=randomize_proxy_usage)
            else:  # By default the code will with 5 concurrent threads. you can change this behaviour by changing n_workers
                if start_idx:
                    lst_of_lst = ranges(
                        indices[-1]+1, n_workers, start_idx=start_idx)
                else:
                    lst_of_lst = ranges(len(indices), n_workers)
                print(lst_of_lst)
                # detail_Data and item_data are lists of empty lists so that each namepace of function call will have its separate detail_data
                # list to strore scraped dictionaries. will save memory(ram/hard-disk) consumption. will stop data duplication
                headless = [open_headless for i in lst_of_lst]
                proxy = [open_with_proxy_server for i in lst_of_lst]
                rand_proxy = [randomize_proxy_usage for i in lst_of_lst]
                detail_data = [[] for i in lst_of_lst]  # type: List
                # item_data=[[] for i in lst_of_lst]  # type: List
                item_df = [pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                                 &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;])
                           for i in lst_of_lst]
                review_data = [[] for i in lst_of_lst]  # type: list
                inc_list = [incremental for i in lst_of_lst]  # type: list
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    # but each of the function namespace will be modifying only one metadata tracing file so that progress saving
                    # is tracked correctly. else multiple progress tracker file will be created with difficulty to combine correct
                    # progress information
                    print(&#39;inside executor&#39;)
                    executor.map(self.crawl_page, lst_of_lst,
                                 headless, proxy, rand_proxy,
                                 detail_data, item_df, review_data,
                                 inc_list)
        try:
            if compile_progress_files:
                self.logger.info(&#39;Creating Combined Detail and Item File&#39;)
                if datetime.now().day &lt; 15:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
                else:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;

                det_li = []
                self.bad_det_li = []
                detail_files = [f for f in self.detail_current_progress_path.glob(
                    &#34;sph_prod_detail_extract_progress_*&#34;)]
                for file in detail_files:
                    try:
                        df = pd.read_csv(file)
                    except Exception:
                        self.bad_det_li.append(file)
                    else:
                        det_li.append(df)

                detail_df = pd.concat(det_li, axis=0, ignore_index=True)
                detail_df.drop_duplicates(inplace=True)
                detail_df.reset_index(inplace=True, drop=True)
                detail_df[&#39;meta_date&#39;] = meta_date
                detail_filename = f&#39;sph_product_detail_all_{meta_date}.csv&#39;
                detail_df.to_csv(self.detail_path/detail_filename, index=None)
                # detail_df.to_feather(self.detail_path/detail_filename)

                item_li = []
                self.bad_item_li = []
                item_files = [f for f in self.current_progress_path.glob(
                    &#34;sph_prod_item_extract_progress_*&#34;)]
                for file in item_files:
                    try:
                        idf = pd.read_csv(file)
                    except Exception:
                        self.bad_item_li.append(file)
                    else:
                        item_li.append(idf)

                item_dataframe = pd.concat(item_li, axis=0, ignore_index=True)
                item_dataframe.drop_duplicates(inplace=True)
                item_dataframe.reset_index(inplace=True, drop=True)
                item_dataframe[&#39;meta_date&#39;] = meta_date
                item_filename = f&#39;sph_product_item_all_{meta_date}.csv&#39;
                item_dataframe.to_csv(
                    self.detail_path/item_filename, index=None)
                # item_df.to_feather(self.detail_path/item_filename)

                self.logger.info(
                    f&#39;Detail and Item files created. Please look for file sph_product_detail_all and\
                        sph_product_item_all in path {self.detail_path}&#39;)
                print(
                    f&#39;Detail and Item files created. Please look for file sph_product_detail_all and\
                        sph_product_item_all in path {self.detail_path}&#39;)

                self.logger.info(&#39;Creating Combined Review File&#39;)
                if datetime.now().day &lt; 15:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
                else:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;
                rev_li = []
                self.bad_rev_li = []
                review_files = [f for f in self.current_progress_path.glob(
                    &#34;sph_prod_review_extract_progress_*&#34;)]
                for file in review_files:
                    try:
                        df = pd.read_csv(file)
                    except Exception:
                        self.bad_rev_li.append(file)
                    else:
                        rev_li.append(df)
                rev_df = pd.concat(rev_li, axis=0, ignore_index=True)
                rev_df.drop_duplicates(inplace=True)
                rev_df.reset_index(inplace=True, drop=True)
                rev_df[&#39;meta_date&#39;] = pd.to_datetime(meta_date).date()
                review_filename = f&#39;sph_product_review_all_{pd.to_datetime(meta_date).date()}&#39;
                # , index=None)
                rev_df.to_feather(self.review_path/review_filename)

                self.logger.info(
                    f&#39;Review file created. Please look for file sph_product_review_all in path {self.review_path}&#39;)
                print(
                    f&#39;Review file created. Please look for file sph_product_review_all in path {self.review_path}&#39;)

                if clean:
                    detail_cleaner = Cleaner(path=self.path)
                    self.detail_clean_df = detail_cleaner.clean(
                        self.detail_path/detail_filename)
                    item_cleaner = Cleaner(path=self.path)
                    self.item_clean_df, self.ing_clean_df = item_cleaner.clean(
                        self.detail_path/item_filename)

                    review_cleaner = Cleaner(path=self.path)
                    self.review_clean_df = review_cleaner.clean(
                        self.review_path/review_filename)

                    file_creation_status = True
            else:
                file_creation_status = False
        except Exception as ex:
            log_exception(
                self.logger, additional_information=f&#39;Detail Item Review Combined File Creation Failed.&#39;)
            file_creation_status = False

        if delete_progress and file_creation_status:
            shutil.rmtree(
                f&#39;{self.detail_path}\\current_progress&#39;, ignore_errors=True)
            shutil.rmtree(
                f&#39;{self.review_path}\\current_progress&#39;, ignore_errors=True)
            self.logger.info(&#39;Progress files deleted&#39;)

    def terminate_logging(self):
        &#34;&#34;&#34;terminate_logging.&#34;&#34;&#34;
        self.logger.handlers.clear()
        self.prod_detail_review_image_log.stop_log()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="meiyume.sph.crawler.Detail"><code class="flex name class">
<span>class <span class="ident">Detail</span></span>
<span>(</span><span>path: pathlib.Path = WindowsPath('D:/Amit/Meiyume/meiyume_master_source_codes'), log: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Detail extracts product details such as ingredients, price, size, color etc. from Sephora website.</p>
<p>The Detail module utilizes product urls scraped by Metadata by opening the pages one by one and getting
details for each individual products.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>Sephora</code></strong> :&ensp;<code>[type]</code></dt>
<dd>Class that initializes folder paths and selenium webdriver for data scraping.</dd>
</dl>
<p><strong>init</strong> Deatil class instace initializer.</p>
<p>This method sets all the folder paths required for Detail crawler to work.
If the paths does not exist the paths get automatically created depending on
current directory or provided directory.</p>
<h2 id="args_1">Args</h2>
<dl>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to create crawling exception and progess log. Defaults to True.</dd>
<dt><strong><code>path</code></strong> :&ensp;<code>Path</code>, optional</dt>
<dd>Folder path where the Detail will be extracted. Defaults to current directory(Path.cwd()).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Detail(Sephora):
    &#34;&#34;&#34;Detail extracts product details such as ingredients, price, size, color etc. from Sephora website.

    The Detail module utilizes product urls scraped by Metadata by opening the pages one by one and getting
    details for each individual products.

    Args:
        Sephora ([type]): Class that initializes folder paths and selenium webdriver for data scraping.
    &#34;&#34;&#34;

    def __init__(self, path: Path = Path.cwd(), log: bool = True):
        &#34;&#34;&#34;__init__ Deatil class instace initializer.

        This method sets all the folder paths required for Detail crawler to work.
        If the paths does not exist the paths get automatically created depending on
        current directory or provided directory.

        Args:
            log (bool, optional): Whether to create crawling exception and progess log. Defaults to True.
            path (Path, optional): Folder path where the Detail will be extracted. Defaults to current directory(Path.cwd()).
        &#34;&#34;&#34;
        super().__init__(path=path, data_def=&#39;detail&#39;)
        self.path = path
        self.current_progress_path = self.detail_path/&#39;current_progress&#39;
        self.current_progress_path.mkdir(parents=True, exist_ok=True)

        old_detail_files = list(self.detail_path.glob(
            &#39;sph_product_detail_all*&#39;)) + list(self.detail_path.glob(
                &#39;sph_product_item_all*&#39;))
        for f in old_detail_files:
            shutil.move(str(f), str(self.old_detail_files_path))

        old_clean_detail_files = files = os.listdir(self.detail_clean_path)
        for f in old_clean_detail_files:
            shutil.move(str(self.detail_clean_path/f),
                        str(self.old_detail_clean_files_path))
        # set logger
        if log:
            self.prod_detail_log = Logger(&#34;sph_prod_detail_extraction&#34;,
                                          path=self.crawl_log_path)
            self.logger, _ = self.prod_detail_log.start_log()

    def get_detail(self, indices: Union[list, range], open_headless: bool, open_with_proxy_server: bool,
                   randomize_proxy_usage: bool, detail_data: list = [],
                   item_df=pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                                 &#39;item_size&#39;, &#39;item_price&#39;,
                                                 &#39;item_ingredients&#39;])) -&gt; None:
        &#34;&#34;&#34;get_detail scrapes individual product pages for price, ingredients, color etc.

        Get Detail crawls product specific page and scrapes data such as ingredients, review rating distribution,
        size specific prices, color, product claims and other information pertaining to one individual product.

        Args:
            indices (Union[list, range]): list of indices or range of indices of product urls to scrape.
            open_headless (bool): Whether to open browser headless.
            open_with_proxy_server (bool): Whether to use ip rotation service.
            randomize_proxy_usage (bool): Whether to use both proxy and native network in tandem to decrease proxy requests.
            detail_data (list, optional): Empty intermediate list to store data during parallel crawl. Defaults to [].
            item_df ([type], optional): Empty intermediate dataframe to store data during parallel crawl.
                                        Defaults to []. Defaults to pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                                                                         &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;]).
        &#34;&#34;&#34;
        def store_data_refresh_mem(detail_data: list, item_df: pd.DataFrame) -&gt; Tuple[list, pd.DataFrame]:
            &#34;&#34;&#34;store_data_refresh_mem method stores crawled data in regular interval to free up system memory.

            Store data after ten products are extracted every time to free up RAM.

            Args:
                detail_data (list): List containing crawled detail data to store.
                item_df (pd.DataFrame): Dataframe containing scraped item data to store.

            Returns:
                Tuple[list, pd.DataFrame]: Empty list and dataframe to accumulate data from next ten products scrape.
            &#34;&#34;&#34;
            pd.DataFrame(detail_data).to_csv(self.current_progress_path /
                                             f&#39;sph_prod_detail_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                                             index=None)
            item_df.reset_index(inplace=True, drop=True)
            item_df.to_csv(self.current_progress_path /
                           f&#39;sph_prod_item_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                           index=None)
            item_df = pd.DataFrame(columns=[
                                   &#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;, &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;])
            self.meta.to_feather(
                self.detail_path/&#39;sph_detail_progress_tracker&#39;)
            return [], item_df

        def get_item_attributes(drv: webdriver.Firefox, product_name: str, prod_id: str, use_button: bool = False,
                                multi_variety: bool = False, typ=None, ) -&gt; Tuple[str, str, str, str]:
            &#34;&#34;&#34;get_item_attributes scrapes product item specific data such as item_name, size, price, and ingredients.

            Args:
                drv (webdriver.Firefox): Selenium webdriver with opened product page.
                product_name (str): Name of the product from metadata.
                prod_id (str): Id of the product from metdata.
                use_button (bool, optional): Whether to use buttons to extract item name. Defaults to False.
                multi_variety (bool, optional): Whether product has multiple color/size varieties. Defaults to False.
                typ ([type], optional): The product the currently selected. Defaults to None.

            Returns:
                Tuple[str, str, str, str]: item_name, size, price, ingredients.
            &#34;&#34;&#34;
            # drv # type: webdriver.Chrome
            # close popup windows
            close_popups(drv)
            accept_alert(drv, 1)

            try:
                item_price = drv.find_element_by_class_name(&#39;css-1865ad6&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                item_price = &#39;&#39;
            # print(item_price)

            if multi_variety:
                try:
                    if use_button:
                        item_name = typ.find_element_by_tag_name(
                            &#39;button&#39;).get_attribute(&#39;aria-label&#39;)
                    else:
                        item_name = typ.get_attribute(&#39;aria-label&#39;)
                    # print(item_name)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    item_name = &#34;&#34;
                    self.logger.info(str.encode(
                        f&#39;product: {product_name} (prod_id: {prod_id}) item_name does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            else:
                item_name = &#34;&#34;

            try:
                item_size = drv.find_element_by_class_name(&#39;css-128n72s&#39;).text
                # print(item_size)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                item_size = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) item_size does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            # get all tabs
            first_tab = drv.find_element_by_id(f&#39;tab{0}&#39;)
            self.scroll_to_element(drv, first_tab)
            ActionChains(drv).move_to_element(
                first_tab).click(first_tab).perform()
            prod_tabs = []
            prod_tabs = drv.find_elements_by_class_name(&#39;css-1wugx5m&#39;)
            prod_tabs.extend(drv.find_elements_by_class_name(&#39;css-12vae0p&#39;))

            tab_names = []
            for t in prod_tabs:
                tab_names.append(t.text.lower())
            # print(tab_names)

            if &#39;ingredients&#39; in tab_names:
                close_popups(drv)
                accept_alert(drv, 1)
                if len(tab_names) == 5:
                    try:
                        tab_num = 2
                        ing_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                        self.scroll_to_element(drv, ing_button)
                        ActionChains(drv).move_to_element(
                            ing_button).click(ing_button).perform()
                        item_ing = drv.find_element_by_xpath(
                            f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        print(&#39;cant get ingredient but tab exists&#39;)
                        item_ing = &#34;&#34;
                        self.logger.info(str.encode(
                            f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients extraction failed&#39;,
                            &#39;utf-8&#39;, &#39;ignore&#39;))
                elif len(tab_names) == 4:
                    try:
                        tab_num = 1
                        ing_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                        self.scroll_to_element(drv, ing_button)
                        ActionChains(drv).move_to_element(
                            ing_button).click(ing_button).perform()
                        item_ing = drv.find_element_by_xpath(
                            f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        print(&#39;cant get ingredient but tab exists&#39;)
                        item_ing = &#34;&#34;
                        self.logger.info(str.encode(
                            f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients extraction failed.&#39;,
                            &#39;utf-8&#39;, &#39;ignore&#39;))
                elif len(tab_names) &lt; 4:
                    try:
                        tab_num = 0
                        ing_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                        self.scroll_to_element(drv, ing_button)
                        ActionChains(drv).move_to_element(
                            ing_button).click(ing_button).perform()
                        item_ing = drv.find_element_by_xpath(
                            f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        print(&#39;cant get ingredient but tab exists&#39;)
                        item_ing = &#34;&#34;
                        self.logger.info(str.encode(
                            f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients extraction failed.&#39;,
                            &#39;utf-8&#39;, &#39;ignore&#39;))
            else:
                item_ing = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            # print(item_ing)
            return item_name, item_size, item_price, item_ing

        def get_product_attributes(drv: webdriver.Firefox, product_name: str, prod_id: str) -&gt; list:
            &#34;&#34;&#34;get_product_attributes uses get_item_attribute method to scrape item details and stores in a list
            which is returned to get_detail method for storing in a product specific dataframe.

            Args:
                drv (webdriver.Firefox): Selenium webdriver with opened product page.
                product_name (str): Name of the product from metadata.
                prod_id (str): Id of the product from metdata.

            Returns:
                list: List containing all product item attributes of multiple varieties with name and id.
            &#34;&#34;&#34;
            # get all the variation of product
            # close popup windows
            close_popups(drv)
            accept_alert(drv, 1)

            product_variety = []
            try:
                product_variety = drv.find_elements_by_class_name(
                    &#39;css-1j1jwa4&#39;)
                product_variety.extend(
                    drv.find_elements_by_class_name(&#39;css-cl742e&#39;))
                use_button = False
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
            try:
                if len(product_variety) &lt; 1:
                    product_variety = drv.find_elements_by_class_name(
                        &#39;css-5jqxch&#39;)
                    use_button = True
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)

            product_attributes = []

            if len(product_variety) &gt; 0:
                for typ in product_variety:
                    close_popups(drv)
                    accept_alert(drv, 1)
                    try:
                        self.scroll_to_element(drv, typ)
                        ActionChains(drv).move_to_element(
                            typ).click(typ).perform()
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    time.sleep(4)  # 8
                    item_name, item_size, item_price, item_ingredients = get_item_attributes(drv, product_name, prod_id,
                                                                                             multi_variety=True, typ=typ,
                                                                                             use_button=use_button)
                    product_attributes.append({&#34;prod_id&#34;: prod_id, &#34;product_name&#34;: product_name,
                                               &#34;item_name&#34;: item_name, &#34;item_size&#34;: item_size,
                                               &#34;item_price&#34;: item_price, &#34;item_ingredients&#34;: item_ingredients})
            else:
                item_name, item_size, item_price, item_ingredients = get_item_attributes(drv,
                                                                                         product_name, prod_id)
                product_attributes.append({&#34;prod_id&#34;: prod_id, &#34;product_name&#34;: product_name, &#34;item_name&#34;: item_name,
                                           &#34;item_size&#34;: item_size, &#34;item_price&#34;: item_price, &#34;item_ingredients&#34;: item_ingredients})

            return product_attributes

        def get_first_review_date(drv: webdriver.Firefox) -&gt; str:
            &#34;&#34;&#34;get_first_review_date scraped the first review data of the product.

            Args:
                drv (webdriver.Firefox): Selenium webdriver with opened product page.

            Returns:
                str: first review date.
            &#34;&#34;&#34;
            # close popup windows
            close_popups(drv)
            accept_alert(drv, 1)

            try:
                review_sort_trigger = drv.find_element_by_id(
                    &#39;review_filter_sort_trigger&#39;)
                self.scroll_to_element(drv, review_sort_trigger)
                ActionChains(drv).move_to_element(
                    review_sort_trigger).click(review_sort_trigger).perform()
                for btn in drv.find_elements_by_class_name(&#39;css-rfz1gg&#39;):
                    if btn.text.lower() == &#39;oldest&#39;:
                        ActionChains(drv).move_to_element(
                            btn).click(btn).perform()
                        break
                time.sleep(6)
                close_popups(drv)
                accept_alert(drv, 1)
                rev = drv.find_elements_by_class_name(&#39;css-1kk8dps&#39;)[2:]
                try:
                    first_review_date = convert_ago_to_date(
                        rev[0].find_element_by_class_name(&#39;css-h2vfi1&#39;).text)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    try:
                        first_review_date = convert_ago_to_date(
                            rev[1].find_element_by_class_name(&#39;css-h2vfi1&#39;).text)
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        print(&#39;sorted but cant get first review date value&#39;)
                        first_review_date = &#39;&#39;
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                first_review_date = &#39;&#39;
            return first_review_date

        for prod in self.meta.index[self.meta.index.isin(indices)]:
            #  ignore already extracted products
            if self.meta.loc[prod, &#39;detail_scraped&#39;] in [&#39;Y&#39;, &#39;NA&#39;]:
                continue
            # print(prod, self.meta.loc[prod, &#39;detail_scraped&#39;])
            prod_id = self.meta.loc[prod, &#39;prod_id&#39;]
            product_name = self.meta.loc[prod, &#39;product_name&#39;]
            product_page = self.meta.loc[prod, &#39;product_page&#39;]

            # create webdriver
            if randomize_proxy_usage:
                use_proxy = np.random.choice([True, False])
            else:
                use_proxy = True
            if open_with_proxy_server:
                # print(use_proxy)
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
                                        path=self.detail_path)
            else:
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
                                        path=self.detail_path)
            # open product page
            drv.get(product_page)
            time.sleep(20)  # 30
            accept_alert(drv, 10)
            close_popups(drv)

            # check product page is valid and exists
            try:
                close_popups(drv)
                accept_alert(drv, 2)
                price = drv.find_element_by_class_name(&#39;css-1865ad6&#39;)
                self.scroll_to_element(drv, price)
                price = price.text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                drv.quit()
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) no longer exists in the previously fetched link.\
                        (link:{product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                self.meta.loc[prod, &#39;detail_scraped&#39;] = &#39;NA&#39;
                continue

            try:
                chat_popup_button = WebDriverWait(drv, 3).until(
                    EC.presence_of_element_located((By.XPATH, &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)))
                chat_popup_button = drv.find_element_by_xpath(
                    &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)
                self.scroll_to_element(drv, chat_popup_button)
                ActionChains(drv).move_to_element(
                    chat_popup_button).click(chat_popup_button).perform()
            except TimeoutException:
                pass

            # get all product info tabs such as how-to-use, about-brand, ingredients
            prod_tabs = []
            prod_tabs = drv.find_elements_by_class_name(&#39;css-1wugx5m&#39;)
            prod_tabs.extend(drv.find_elements_by_class_name(&#39;css-12vae0p&#39;))

            tab_names = []
            for t in prod_tabs:
                tab_names.append(t.text.lower())

            # no. of votes
            try:
                votes = drv.find_elements_by_class_name(&#39;css-2rg6q7&#39;)[-1].text
                # print(votes)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                votes = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) votes does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            # product details
            if &#39;details&#39; in tab_names:
                try:
                    close_popups(drv)
                    accept_alert(drv, 1)
                    tab_num = tab_names.index(&#39;details&#39;)
                    detail_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                    try:
                        time.sleep(1)
                        self.scroll_to_element(drv, detail_button)
                        ActionChains(drv).move_to_element(
                            detail_button).click(detail_button).perform()
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        details = &#34;&#34;
                    else:
                        try:
                            details = drv.find_element_by_xpath(
                                f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                        except Exception as ex:
                            log_exception(self.logger,
                                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
                            details = &#34;&#34;
                            self.logger.info(str.encode(
                                f&#39;product: {product_name} (prod_id: {prod_id}) product detail text\
                                        does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                        # print(details)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    details = &#34;&#34;
                    self.logger.info(str.encode(
                        f&#39;product: {product_name} (prod_id: {prod_id}) product detail extraction failed&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            else:
                details = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) product detail does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            # how to use
            if &#39;how to use&#39; in tab_names:
                try:
                    close_popups(drv)
                    accept_alert(drv, 1)

                    tab_num = tab_names.index(&#39;how to use&#39;)
                    how_to_use_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                    try:
                        time.sleep(1)
                        self.scroll_to_element(drv, how_to_use_button)
                        ActionChains(drv).move_to_element(
                            how_to_use_button).click(how_to_use_button).perform()
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        how_to_use = &#34;&#34;
                    else:
                        try:
                            how_to_use = drv.find_element_by_xpath(
                                f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                        except Exception as ex:
                            log_exception(self.logger,
                                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
                            how_to_use = &#34;&#34;
                            self.logger.info(str.encode(
                                f&#39;product: {product_name} (prod_id: {prod_id}) how_to_use text\
                                     does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                        # print(how_to_use)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    how_to_use = &#34;&#34;
                    self.logger.info(str.encode(
                        f&#39;product: {product_name} (prod_id: {prod_id}) how_to_use extraction failed&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            else:
                how_to_use = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) how_to_use does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            # about the brand
            if &#39;about the brand&#39; in tab_names:
                try:
                    close_popups(drv)
                    accept_alert(drv, 1)

                    tab_num = tab_names.index(&#39;about the brand&#39;)
                    about_the_brand_button = drv.find_element_by_id(
                        f&#39;tab{tab_num}&#39;)
                    try:
                        time.sleep(1)
                        self.scroll_to_element(drv, about_the_brand_button)
                        ActionChains(drv).move_to_element(
                            about_the_brand_button).click(about_the_brand_button).perform()
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        about_the_brand = &#34;&#34;
                    else:
                        try:
                            about_the_brand = drv.find_element_by_xpath(
                                f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                        except Exception as ex:
                            log_exception(self.logger,
                                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
                            about_the_brand = &#34;&#34;
                            self.logger.info(str.encode(
                                f&#39;product: {product_name} (prod_id: {prod_id}) about_the_brand text\
                                    does not exist&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                        # print(about_the_brand)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    about_the_brand = &#34;&#34;
                    self.logger.info(str.encode(
                        f&#39;product: {product_name} (prod_id: {prod_id}) about_the_brand extraction failed&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            else:
                about_the_brand = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) about_the_brand does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            self.scroll_down_page(drv, h2=0.4, speed=5)
            time.sleep(5)
            try:
                chat_popup_button = WebDriverWait(drv, 3).until(
                    EC.presence_of_element_located((By.XPATH, &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)))
                chat_popup_button = drv.find_element_by_xpath(
                    &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)
                self.scroll_to_element(drv, chat_popup_button)
                ActionChains(drv).move_to_element(
                    chat_popup_button).click(chat_popup_button).perform()
            except TimeoutException:
                pass
            # click no. of reviews
            try:
                review_button = drv.find_element_by_class_name(&#39;css-1pjru6n&#39;)
                self.scroll_to_element(drv, review_button)
                ActionChains(drv).move_to_element(
                    review_button).click(review_button).perform()
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)

            try:
                first_review_date = get_first_review_date(drv)
                # print(first_review_date)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                first_review_date = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) first_review_date scrape failed.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            try:
                close_popups(drv)
                accept_alert(drv, 1)
                reviews = int(drv.find_element_by_class_name(
                    &#39;css-ils4e4&#39;).text.split()[0])
                # print(reviews)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                reviews = 0
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) reviews does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            try:
                close_popups(drv)
                accept_alert(drv, 1)
                rating_distribution = drv.find_element_by_class_name(
                    &#39;css-960eb6&#39;).text.split(&#39;\n&#39;)
                # print(rating_distribution)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                rating_distribution = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) rating_distribution does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            try:
                close_popups(drv)
                accept_alert(drv, 1)
                would_recommend = drv.find_element_by_class_name(
                    &#39;css-k9ne19&#39;).text
                # print(would_recommend)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                would_recommend = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) would_recommend does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            product_attributes = pd.DataFrame(
                get_product_attributes(drv, product_name, prod_id))
            item_df = pd.concat(
                [item_df, pd.DataFrame(product_attributes)], axis=0)

            detail_data.append({&#39;prod_id&#39;: prod_id, &#39;product_name&#39;: product_name, &#39;abt_product&#39;: details,
                                &#39;how_to_use&#39;: how_to_use, &#39;abt_brand&#39;: about_the_brand,
                                &#39;reviews&#39;: reviews, &#39;votes&#39;: votes, &#39;rating_dist&#39;: rating_distribution,
                                &#39;would_recommend&#39;: would_recommend, &#39;first_review_date&#39;: first_review_date})
            # item_data.append(product_attributes)
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) details extracted successfully&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            self.meta.loc[prod, &#39;detail_scraped&#39;] = &#39;Y&#39;

            if prod != 0 and prod % 10 == 0:
                if len(detail_data) &gt; 0:
                    detail_data, item_df = store_data_refresh_mem(
                        detail_data, item_df)
            drv.quit()

        detail_data, item_df = store_data_refresh_mem(
            detail_data, item_df)
        self.logger.info(
            f&#39;Detail Extraction Complete for start_idx: (indices[0]) to end_idx: {indices[-1]}. Or for list of values.&#39;)

    def extract(self, metadata: pd.DataFrame, download: bool = True, n_workers: int = 5,
                fresh_start: bool = False, auto_fresh_start: bool = False,
                open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = False,
                start_idx: Optional[int] = None, end_idx: Optional[int] = None, list_of_index=None,
                compile_progress_files: bool = False, clean: bool = True, delete_progress: bool = False):
        &#34;&#34;&#34;extract method controls all properties of the spiders and runs multi-threaded web crawling.

        Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
        This method has four major functionality:
        * 1. Run the spider
        * 2. Store data in regular intervals to free up ram
        * 3. Compile all crawled data into one file.
        * 4. Clean and push cleaned data to S3 storage for further algorithmic processing.

        Args:
            metadata (pd.DataFrame): Dataframe containing product specific url, name and id of the products to be scraped.
            download (bool, optional): Whether to crawl data from or compile crawled data into one file. Defaults to True.
            n_workers (int, optional): No. of parallel threads to run. Defaults to 5.
            fresh_start (bool, optional): Whether to continue last crawl job or start new one. Defaults to False.
            auto_fresh_start (bool, optional): Whether to automatically start a new crawl job if last job was finished. Defaults to False.
            open_headless (bool, optional):  Whether to open browser headless. Defaults to False.
            open_with_proxy_server (bool, optional): Whether to use ip rotation service. Defaults to True.
            randomize_proxy_usage (bool, optional): Whether to use both proxy and native network in tandem to decrease proxy requests.
                                                    Defaults to False.
            start_idx (Optional[int], optional): Starting index of the links to crawl. Defaults to None.
            end_idx (Optional[int], optional): Ending index of the links to crawl. Defaults to None.
            list_of_index ([type], optional): List of indices or range of indices of product urls to scrape. Defaults to None.
            compile_progress_files (bool, optional): Whether to combine crawled data into one file. Defaults to False.
            clean (bool, optional): Whether to clean the compiled data. Defaults to True.
            delete_progress (bool, optional): Whether to delete intermediate data after compilation into one file. Defaults to False.
        &#34;&#34;&#34;
        &#39;&#39;&#39;
        change metadata read logic.add logic to look for metadata in a folder path. if metadata is found in the folder path
        detail data crawler is triggered
        &#39;&#39;&#39;
        # list_of_files = self.metadata_clean_path.glob(
        #     &#39;no_cat_cleaned_sph_product_metadata_all*&#39;)
        # self.meta = pd.read_feather(max(list_of_files, key=os.path.getctime))[
        #     [&#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;]]

        def fresh():
            &#34;&#34;&#34; If fresh_start is True, this function sets initial parameters for a fresh data crawl.
            &#34;&#34;&#34;
            if not isinstance(metadata, pd.core.frame.DataFrame):
                list_of_files = self.metadata_clean_path.glob(
                    &#39;no_cat_cleaned_sph_product_metadata_all*&#39;)
                self.meta = pd.read_feather(max(list_of_files, key=os.path.getctime))[
                    [&#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;]]
            else:
                self.meta = metadata[[
                    &#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;]]

            self.meta[&#39;detail_scraped&#39;] = &#39;N&#39;

        if download:
            if fresh_start:
                fresh()
                self.logger.info(
                    &#39;Starting Fresh Detail Extraction.&#39;)
            else:
                if Path(self.detail_path/&#39;sph_detail_progress_tracker&#39;).exists():
                    self.meta = pd.read_feather(
                        self.detail_path/&#39;sph_detail_progress_tracker&#39;)
                    if sum(self.meta.detail_scraped == &#39;N&#39;) == 0:
                        if auto_fresh_start:
                            fresh()
                            self.logger.info(
                                &#39;Last Run was Completed. Starting Fresh Extraction.&#39;)
                        else:
                            self.logger.info(
                                &#39;Detail extraction for this cycle is complete.&#39;)
                    else:
                        self.logger.info(
                            &#39;Continuing Detail Extraction From Last Run.&#39;)
                else:
                    fresh()
                    self.logger.info(
                        &#39;Detail Progress Tracker does not exist. Starting Fresh Extraction.&#39;)

            # set list or range of product indices to crawl
            if list_of_index:
                indices = list_of_index
            elif start_idx and end_idx is None:
                indices = range(start_idx, len(self.meta))
            elif start_idx is None and end_idx:
                indices = range(0, end_idx)
            elif start_idx is not None and end_idx is not None:
                indices = range(start_idx, end_idx)
            else:
                indices = range(len(self.meta))
            print(indices)

            if list_of_index:
                self.get_detail(indices=list_of_index,
                                open_headless=open_headless,
                                open_with_proxy_server=open_with_proxy_server,
                                randomize_proxy_usage=randomize_proxy_usage)
            else:  # By default the code will with 5 concurrent threads. you can change this behaviour by changing n_workers
                if start_idx:
                    lst_of_lst = ranges(
                        indices[-1]+1, n_workers, start_idx=start_idx)
                else:
                    lst_of_lst = ranges(len(indices), n_workers)
                print(lst_of_lst)
                # detail_Data and item_data are lists of empty lists so that each namepace of function call will have its separate detail_data
                # list to strore scraped dictionaries. will save memory(ram/hard-disk) consumption. will stop data duplication
                headless = [open_headless for i in lst_of_lst]
                proxy = [open_with_proxy_server for i in lst_of_lst]
                rand_proxy = [randomize_proxy_usage for i in lst_of_lst]
                detail_data = [[] for i in lst_of_lst]  # type: List
                # item_data=[[] for i in lst_of_lst]  # type: List
                item_df = [pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                                 &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;])
                           for i in lst_of_lst]
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    # but each of the function namespace will be modifying only one metadata tracing file so that progress saving
                    # is tracked correctly. else multiple progress tracker file will be created with difficulty to combine correct
                    # progress information
                    print(&#39;inside executor&#39;)
                    executor.map(self.get_detail, lst_of_lst,
                                 headless, proxy, rand_proxy,
                                 detail_data, item_df)
        try:
            if compile_progress_files:
                self.logger.info(&#39;Creating Combined Detail and Item File&#39;)
                if datetime.now().day &lt; 15:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
                else:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;

                det_li = []
                self.bad_det_li = []
                detail_files = [f for f in self.current_progress_path.glob(
                    &#34;sph_prod_detail_extract_progress_*&#34;)]
                for file in detail_files:
                    try:
                        df = pd.read_csv(file)
                    except Exception:
                        self.bad_det_li.append(file)
                    else:
                        det_li.append(df)

                detail_df = pd.concat(det_li, axis=0, ignore_index=True)
                detail_df.drop_duplicates(inplace=True)
                detail_df.reset_index(inplace=True, drop=True)
                detail_df[&#39;meta_date&#39;] = meta_date
                detail_filename = f&#39;sph_product_detail_all_{meta_date}.csv&#39;
                detail_df.to_csv(self.detail_path/detail_filename, index=None)
                # detail_df.to_feather(self.detail_path/detail_filename)

                item_li = []
                self.bad_item_li = []
                item_files = [f for f in self.current_progress_path.glob(
                    &#34;sph_prod_item_extract_progress_*&#34;)]
                for file in item_files:
                    try:
                        idf = pd.read_csv(file)
                    except Exception:
                        self.bad_item_li.append(file)
                    else:
                        item_li.append(idf)

                item_dataframe = pd.concat(item_li, axis=0, ignore_index=True)
                item_dataframe.drop_duplicates(inplace=True)
                item_dataframe.reset_index(inplace=True, drop=True)
                item_dataframe[&#39;meta_date&#39;] = meta_date
                item_filename = f&#39;sph_product_item_all_{meta_date}.csv&#39;
                item_dataframe.to_csv(
                    self.detail_path/item_filename, index=None)
                # item_df.to_feather(self.detail_path/item_filename)

                self.logger.info(
                    f&#39;Detail and Item files created. Please look for file sph_product_detail_all and\
                        sph_product_item_all in path {self.detail_path}&#39;)
                print(
                    f&#39;Detail and Item files created. Please look for file sph_product_detail_all and\
                        sph_product_item_all in path {self.detail_path}&#39;)

                if clean:
                    detail_cleaner = Cleaner(path=self.path)
                    self.detail_clean_df = detail_cleaner.clean(
                        self.detail_path/detail_filename)
                    del detail_cleaner
                    gc.collect()

                    item_cleaner = Cleaner(path=self.path)
                    self.item_clean_df, self.ing_clean_df = item_cleaner.clean(
                        self.detail_path/item_filename)
                    del item_cleaner
                    gc.collect()

                    file_creation_status = True
            else:
                file_creation_status = False
        except Exception as ex:
            log_exception(
                self.logger, additional_information=f&#39;Detail Item Combined File Creation Failed.&#39;)
            file_creation_status = False

        if delete_progress and file_creation_status:
            shutil.rmtree(
                f&#39;{self.detail_path}\\current_progress&#39;, ignore_errors=True)
            self.logger.info(&#39;Progress files deleted&#39;)

    def terminate_logging(self):
        &#34;&#34;&#34;terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.
        &#34;&#34;&#34;
        self.logger.handlers.clear()
        self.prod_detail_log.stop_log()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="meiyume.utils.Sephora" href="../utils.html#meiyume.utils.Sephora">Sephora</a></li>
<li><a title="meiyume.utils.Browser" href="../utils.html#meiyume.utils.Browser">Browser</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="meiyume.sph.crawler.Detail.extract"><code class="name flex">
<span>def <span class="ident">extract</span></span>(<span>self, metadata: pandas.core.frame.DataFrame, download: bool = True, n_workers: int = 5, fresh_start: bool = False, auto_fresh_start: bool = False, open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = False, start_idx: Union[int, NoneType] = None, end_idx: Union[int, NoneType] = None, list_of_index=None, compile_progress_files: bool = False, clean: bool = True, delete_progress: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>extract method controls all properties of the spiders and runs multi-threaded web crawling.</p>
<p>Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
This method has four major functionality:
* 1. Run the spider
* 2. Store data in regular intervals to free up ram
* 3. Compile all crawled data into one file.
* 4. Clean and push cleaned data to S3 storage for further algorithmic processing.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>metadata</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Dataframe containing product specific url, name and id of the products to be scraped.</dd>
<dt><strong><code>download</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to crawl data from or compile crawled data into one file. Defaults to True.</dd>
<dt><strong><code>n_workers</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>No. of parallel threads to run. Defaults to 5.</dd>
<dt><strong><code>fresh_start</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to continue last crawl job or start new one. Defaults to False.</dd>
<dt><strong><code>auto_fresh_start</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to automatically start a new crawl job if last job was finished. Defaults to False.</dd>
<dt><strong><code>open_headless</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to open browser headless. Defaults to False.</dd>
<dt><strong><code>open_with_proxy_server</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use ip rotation service. Defaults to True.</dd>
<dt><strong><code>randomize_proxy_usage</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use both proxy and native network in tandem to decrease proxy requests.
Defaults to False.</dd>
<dt><strong><code>start_idx</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>Starting index of the links to crawl. Defaults to None.</dd>
<dt><strong><code>end_idx</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>Ending index of the links to crawl. Defaults to None.</dd>
<dt><strong><code>list_of_index</code></strong> :&ensp;<code>[type]</code>, optional</dt>
<dd>List of indices or range of indices of product urls to scrape. Defaults to None.</dd>
<dt><strong><code>compile_progress_files</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to combine crawled data into one file. Defaults to False.</dd>
<dt><strong><code>clean</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to clean the compiled data. Defaults to True.</dd>
<dt><strong><code>delete_progress</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to delete intermediate data after compilation into one file. Defaults to False.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract(self, metadata: pd.DataFrame, download: bool = True, n_workers: int = 5,
            fresh_start: bool = False, auto_fresh_start: bool = False,
            open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = False,
            start_idx: Optional[int] = None, end_idx: Optional[int] = None, list_of_index=None,
            compile_progress_files: bool = False, clean: bool = True, delete_progress: bool = False):
    &#34;&#34;&#34;extract method controls all properties of the spiders and runs multi-threaded web crawling.

    Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
    This method has four major functionality:
    * 1. Run the spider
    * 2. Store data in regular intervals to free up ram
    * 3. Compile all crawled data into one file.
    * 4. Clean and push cleaned data to S3 storage for further algorithmic processing.

    Args:
        metadata (pd.DataFrame): Dataframe containing product specific url, name and id of the products to be scraped.
        download (bool, optional): Whether to crawl data from or compile crawled data into one file. Defaults to True.
        n_workers (int, optional): No. of parallel threads to run. Defaults to 5.
        fresh_start (bool, optional): Whether to continue last crawl job or start new one. Defaults to False.
        auto_fresh_start (bool, optional): Whether to automatically start a new crawl job if last job was finished. Defaults to False.
        open_headless (bool, optional):  Whether to open browser headless. Defaults to False.
        open_with_proxy_server (bool, optional): Whether to use ip rotation service. Defaults to True.
        randomize_proxy_usage (bool, optional): Whether to use both proxy and native network in tandem to decrease proxy requests.
                                                Defaults to False.
        start_idx (Optional[int], optional): Starting index of the links to crawl. Defaults to None.
        end_idx (Optional[int], optional): Ending index of the links to crawl. Defaults to None.
        list_of_index ([type], optional): List of indices or range of indices of product urls to scrape. Defaults to None.
        compile_progress_files (bool, optional): Whether to combine crawled data into one file. Defaults to False.
        clean (bool, optional): Whether to clean the compiled data. Defaults to True.
        delete_progress (bool, optional): Whether to delete intermediate data after compilation into one file. Defaults to False.
    &#34;&#34;&#34;
    &#39;&#39;&#39;
    change metadata read logic.add logic to look for metadata in a folder path. if metadata is found in the folder path
    detail data crawler is triggered
    &#39;&#39;&#39;
    # list_of_files = self.metadata_clean_path.glob(
    #     &#39;no_cat_cleaned_sph_product_metadata_all*&#39;)
    # self.meta = pd.read_feather(max(list_of_files, key=os.path.getctime))[
    #     [&#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;]]

    def fresh():
        &#34;&#34;&#34; If fresh_start is True, this function sets initial parameters for a fresh data crawl.
        &#34;&#34;&#34;
        if not isinstance(metadata, pd.core.frame.DataFrame):
            list_of_files = self.metadata_clean_path.glob(
                &#39;no_cat_cleaned_sph_product_metadata_all*&#39;)
            self.meta = pd.read_feather(max(list_of_files, key=os.path.getctime))[
                [&#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;]]
        else:
            self.meta = metadata[[
                &#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;]]

        self.meta[&#39;detail_scraped&#39;] = &#39;N&#39;

    if download:
        if fresh_start:
            fresh()
            self.logger.info(
                &#39;Starting Fresh Detail Extraction.&#39;)
        else:
            if Path(self.detail_path/&#39;sph_detail_progress_tracker&#39;).exists():
                self.meta = pd.read_feather(
                    self.detail_path/&#39;sph_detail_progress_tracker&#39;)
                if sum(self.meta.detail_scraped == &#39;N&#39;) == 0:
                    if auto_fresh_start:
                        fresh()
                        self.logger.info(
                            &#39;Last Run was Completed. Starting Fresh Extraction.&#39;)
                    else:
                        self.logger.info(
                            &#39;Detail extraction for this cycle is complete.&#39;)
                else:
                    self.logger.info(
                        &#39;Continuing Detail Extraction From Last Run.&#39;)
            else:
                fresh()
                self.logger.info(
                    &#39;Detail Progress Tracker does not exist. Starting Fresh Extraction.&#39;)

        # set list or range of product indices to crawl
        if list_of_index:
            indices = list_of_index
        elif start_idx and end_idx is None:
            indices = range(start_idx, len(self.meta))
        elif start_idx is None and end_idx:
            indices = range(0, end_idx)
        elif start_idx is not None and end_idx is not None:
            indices = range(start_idx, end_idx)
        else:
            indices = range(len(self.meta))
        print(indices)

        if list_of_index:
            self.get_detail(indices=list_of_index,
                            open_headless=open_headless,
                            open_with_proxy_server=open_with_proxy_server,
                            randomize_proxy_usage=randomize_proxy_usage)
        else:  # By default the code will with 5 concurrent threads. you can change this behaviour by changing n_workers
            if start_idx:
                lst_of_lst = ranges(
                    indices[-1]+1, n_workers, start_idx=start_idx)
            else:
                lst_of_lst = ranges(len(indices), n_workers)
            print(lst_of_lst)
            # detail_Data and item_data are lists of empty lists so that each namepace of function call will have its separate detail_data
            # list to strore scraped dictionaries. will save memory(ram/hard-disk) consumption. will stop data duplication
            headless = [open_headless for i in lst_of_lst]
            proxy = [open_with_proxy_server for i in lst_of_lst]
            rand_proxy = [randomize_proxy_usage for i in lst_of_lst]
            detail_data = [[] for i in lst_of_lst]  # type: List
            # item_data=[[] for i in lst_of_lst]  # type: List
            item_df = [pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                             &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;])
                       for i in lst_of_lst]
            with concurrent.futures.ThreadPoolExecutor() as executor:
                # but each of the function namespace will be modifying only one metadata tracing file so that progress saving
                # is tracked correctly. else multiple progress tracker file will be created with difficulty to combine correct
                # progress information
                print(&#39;inside executor&#39;)
                executor.map(self.get_detail, lst_of_lst,
                             headless, proxy, rand_proxy,
                             detail_data, item_df)
    try:
        if compile_progress_files:
            self.logger.info(&#39;Creating Combined Detail and Item File&#39;)
            if datetime.now().day &lt; 15:
                meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
            else:
                meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;

            det_li = []
            self.bad_det_li = []
            detail_files = [f for f in self.current_progress_path.glob(
                &#34;sph_prod_detail_extract_progress_*&#34;)]
            for file in detail_files:
                try:
                    df = pd.read_csv(file)
                except Exception:
                    self.bad_det_li.append(file)
                else:
                    det_li.append(df)

            detail_df = pd.concat(det_li, axis=0, ignore_index=True)
            detail_df.drop_duplicates(inplace=True)
            detail_df.reset_index(inplace=True, drop=True)
            detail_df[&#39;meta_date&#39;] = meta_date
            detail_filename = f&#39;sph_product_detail_all_{meta_date}.csv&#39;
            detail_df.to_csv(self.detail_path/detail_filename, index=None)
            # detail_df.to_feather(self.detail_path/detail_filename)

            item_li = []
            self.bad_item_li = []
            item_files = [f for f in self.current_progress_path.glob(
                &#34;sph_prod_item_extract_progress_*&#34;)]
            for file in item_files:
                try:
                    idf = pd.read_csv(file)
                except Exception:
                    self.bad_item_li.append(file)
                else:
                    item_li.append(idf)

            item_dataframe = pd.concat(item_li, axis=0, ignore_index=True)
            item_dataframe.drop_duplicates(inplace=True)
            item_dataframe.reset_index(inplace=True, drop=True)
            item_dataframe[&#39;meta_date&#39;] = meta_date
            item_filename = f&#39;sph_product_item_all_{meta_date}.csv&#39;
            item_dataframe.to_csv(
                self.detail_path/item_filename, index=None)
            # item_df.to_feather(self.detail_path/item_filename)

            self.logger.info(
                f&#39;Detail and Item files created. Please look for file sph_product_detail_all and\
                    sph_product_item_all in path {self.detail_path}&#39;)
            print(
                f&#39;Detail and Item files created. Please look for file sph_product_detail_all and\
                    sph_product_item_all in path {self.detail_path}&#39;)

            if clean:
                detail_cleaner = Cleaner(path=self.path)
                self.detail_clean_df = detail_cleaner.clean(
                    self.detail_path/detail_filename)
                del detail_cleaner
                gc.collect()

                item_cleaner = Cleaner(path=self.path)
                self.item_clean_df, self.ing_clean_df = item_cleaner.clean(
                    self.detail_path/item_filename)
                del item_cleaner
                gc.collect()

                file_creation_status = True
        else:
            file_creation_status = False
    except Exception as ex:
        log_exception(
            self.logger, additional_information=f&#39;Detail Item Combined File Creation Failed.&#39;)
        file_creation_status = False

    if delete_progress and file_creation_status:
        shutil.rmtree(
            f&#39;{self.detail_path}\\current_progress&#39;, ignore_errors=True)
        self.logger.info(&#39;Progress files deleted&#39;)</code></pre>
</details>
</dd>
<dt id="meiyume.sph.crawler.Detail.get_detail"><code class="name flex">
<span>def <span class="ident">get_detail</span></span>(<span>self, indices: Union[list, range], open_headless: bool, open_with_proxy_server: bool, randomize_proxy_usage: bool, detail_data: list = [], item_df=Empty DataFrame
Columns: [prod_id, product_name, item_name, item_size, item_price, item_ingredients]
Index: []) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>get_detail scrapes individual product pages for price, ingredients, color etc.</p>
<p>Get Detail crawls product specific page and scrapes data such as ingredients, review rating distribution,
size specific prices, color, product claims and other information pertaining to one individual product.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>indices</code></strong> :&ensp;<code>Union[list, range]</code></dt>
<dd>list of indices or range of indices of product urls to scrape.</dd>
<dt><strong><code>open_headless</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to open browser headless.</dd>
<dt><strong><code>open_with_proxy_server</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use ip rotation service.</dd>
<dt><strong><code>randomize_proxy_usage</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use both proxy and native network in tandem to decrease proxy requests.</dd>
<dt><strong><code>detail_data</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Empty intermediate list to store data during parallel crawl. Defaults to [].</dd>
<dt><strong><code>item_df</code></strong> :&ensp;<code>[type]</code>, optional</dt>
<dd>Empty intermediate dataframe to store data during parallel crawl.
Defaults to []. Defaults to pd.DataFrame(columns=['prod_id', 'product_name', 'item_name',
'item_size', 'item_price', 'item_ingredients']).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_detail(self, indices: Union[list, range], open_headless: bool, open_with_proxy_server: bool,
               randomize_proxy_usage: bool, detail_data: list = [],
               item_df=pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                             &#39;item_size&#39;, &#39;item_price&#39;,
                                             &#39;item_ingredients&#39;])) -&gt; None:
    &#34;&#34;&#34;get_detail scrapes individual product pages for price, ingredients, color etc.

    Get Detail crawls product specific page and scrapes data such as ingredients, review rating distribution,
    size specific prices, color, product claims and other information pertaining to one individual product.

    Args:
        indices (Union[list, range]): list of indices or range of indices of product urls to scrape.
        open_headless (bool): Whether to open browser headless.
        open_with_proxy_server (bool): Whether to use ip rotation service.
        randomize_proxy_usage (bool): Whether to use both proxy and native network in tandem to decrease proxy requests.
        detail_data (list, optional): Empty intermediate list to store data during parallel crawl. Defaults to [].
        item_df ([type], optional): Empty intermediate dataframe to store data during parallel crawl.
                                    Defaults to []. Defaults to pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                                                                     &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;]).
    &#34;&#34;&#34;
    def store_data_refresh_mem(detail_data: list, item_df: pd.DataFrame) -&gt; Tuple[list, pd.DataFrame]:
        &#34;&#34;&#34;store_data_refresh_mem method stores crawled data in regular interval to free up system memory.

        Store data after ten products are extracted every time to free up RAM.

        Args:
            detail_data (list): List containing crawled detail data to store.
            item_df (pd.DataFrame): Dataframe containing scraped item data to store.

        Returns:
            Tuple[list, pd.DataFrame]: Empty list and dataframe to accumulate data from next ten products scrape.
        &#34;&#34;&#34;
        pd.DataFrame(detail_data).to_csv(self.current_progress_path /
                                         f&#39;sph_prod_detail_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                                         index=None)
        item_df.reset_index(inplace=True, drop=True)
        item_df.to_csv(self.current_progress_path /
                       f&#39;sph_prod_item_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                       index=None)
        item_df = pd.DataFrame(columns=[
                               &#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;, &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;])
        self.meta.to_feather(
            self.detail_path/&#39;sph_detail_progress_tracker&#39;)
        return [], item_df

    def get_item_attributes(drv: webdriver.Firefox, product_name: str, prod_id: str, use_button: bool = False,
                            multi_variety: bool = False, typ=None, ) -&gt; Tuple[str, str, str, str]:
        &#34;&#34;&#34;get_item_attributes scrapes product item specific data such as item_name, size, price, and ingredients.

        Args:
            drv (webdriver.Firefox): Selenium webdriver with opened product page.
            product_name (str): Name of the product from metadata.
            prod_id (str): Id of the product from metdata.
            use_button (bool, optional): Whether to use buttons to extract item name. Defaults to False.
            multi_variety (bool, optional): Whether product has multiple color/size varieties. Defaults to False.
            typ ([type], optional): The product the currently selected. Defaults to None.

        Returns:
            Tuple[str, str, str, str]: item_name, size, price, ingredients.
        &#34;&#34;&#34;
        # drv # type: webdriver.Chrome
        # close popup windows
        close_popups(drv)
        accept_alert(drv, 1)

        try:
            item_price = drv.find_element_by_class_name(&#39;css-1865ad6&#39;).text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            item_price = &#39;&#39;
        # print(item_price)

        if multi_variety:
            try:
                if use_button:
                    item_name = typ.find_element_by_tag_name(
                        &#39;button&#39;).get_attribute(&#39;aria-label&#39;)
                else:
                    item_name = typ.get_attribute(&#39;aria-label&#39;)
                # print(item_name)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                item_name = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) item_name does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
        else:
            item_name = &#34;&#34;

        try:
            item_size = drv.find_element_by_class_name(&#39;css-128n72s&#39;).text
            # print(item_size)
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            item_size = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) item_size does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        # get all tabs
        first_tab = drv.find_element_by_id(f&#39;tab{0}&#39;)
        self.scroll_to_element(drv, first_tab)
        ActionChains(drv).move_to_element(
            first_tab).click(first_tab).perform()
        prod_tabs = []
        prod_tabs = drv.find_elements_by_class_name(&#39;css-1wugx5m&#39;)
        prod_tabs.extend(drv.find_elements_by_class_name(&#39;css-12vae0p&#39;))

        tab_names = []
        for t in prod_tabs:
            tab_names.append(t.text.lower())
        # print(tab_names)

        if &#39;ingredients&#39; in tab_names:
            close_popups(drv)
            accept_alert(drv, 1)
            if len(tab_names) == 5:
                try:
                    tab_num = 2
                    ing_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                    self.scroll_to_element(drv, ing_button)
                    ActionChains(drv).move_to_element(
                        ing_button).click(ing_button).perform()
                    item_ing = drv.find_element_by_xpath(
                        f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    print(&#39;cant get ingredient but tab exists&#39;)
                    item_ing = &#34;&#34;
                    self.logger.info(str.encode(
                        f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients extraction failed&#39;,
                        &#39;utf-8&#39;, &#39;ignore&#39;))
            elif len(tab_names) == 4:
                try:
                    tab_num = 1
                    ing_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                    self.scroll_to_element(drv, ing_button)
                    ActionChains(drv).move_to_element(
                        ing_button).click(ing_button).perform()
                    item_ing = drv.find_element_by_xpath(
                        f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    print(&#39;cant get ingredient but tab exists&#39;)
                    item_ing = &#34;&#34;
                    self.logger.info(str.encode(
                        f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients extraction failed.&#39;,
                        &#39;utf-8&#39;, &#39;ignore&#39;))
            elif len(tab_names) &lt; 4:
                try:
                    tab_num = 0
                    ing_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                    self.scroll_to_element(drv, ing_button)
                    ActionChains(drv).move_to_element(
                        ing_button).click(ing_button).perform()
                    item_ing = drv.find_element_by_xpath(
                        f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    print(&#39;cant get ingredient but tab exists&#39;)
                    item_ing = &#34;&#34;
                    self.logger.info(str.encode(
                        f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients extraction failed.&#39;,
                        &#39;utf-8&#39;, &#39;ignore&#39;))
        else:
            item_ing = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
        # print(item_ing)
        return item_name, item_size, item_price, item_ing

    def get_product_attributes(drv: webdriver.Firefox, product_name: str, prod_id: str) -&gt; list:
        &#34;&#34;&#34;get_product_attributes uses get_item_attribute method to scrape item details and stores in a list
        which is returned to get_detail method for storing in a product specific dataframe.

        Args:
            drv (webdriver.Firefox): Selenium webdriver with opened product page.
            product_name (str): Name of the product from metadata.
            prod_id (str): Id of the product from metdata.

        Returns:
            list: List containing all product item attributes of multiple varieties with name and id.
        &#34;&#34;&#34;
        # get all the variation of product
        # close popup windows
        close_popups(drv)
        accept_alert(drv, 1)

        product_variety = []
        try:
            product_variety = drv.find_elements_by_class_name(
                &#39;css-1j1jwa4&#39;)
            product_variety.extend(
                drv.find_elements_by_class_name(&#39;css-cl742e&#39;))
            use_button = False
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
        try:
            if len(product_variety) &lt; 1:
                product_variety = drv.find_elements_by_class_name(
                    &#39;css-5jqxch&#39;)
                use_button = True
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)

        product_attributes = []

        if len(product_variety) &gt; 0:
            for typ in product_variety:
                close_popups(drv)
                accept_alert(drv, 1)
                try:
                    self.scroll_to_element(drv, typ)
                    ActionChains(drv).move_to_element(
                        typ).click(typ).perform()
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                time.sleep(4)  # 8
                item_name, item_size, item_price, item_ingredients = get_item_attributes(drv, product_name, prod_id,
                                                                                         multi_variety=True, typ=typ,
                                                                                         use_button=use_button)
                product_attributes.append({&#34;prod_id&#34;: prod_id, &#34;product_name&#34;: product_name,
                                           &#34;item_name&#34;: item_name, &#34;item_size&#34;: item_size,
                                           &#34;item_price&#34;: item_price, &#34;item_ingredients&#34;: item_ingredients})
        else:
            item_name, item_size, item_price, item_ingredients = get_item_attributes(drv,
                                                                                     product_name, prod_id)
            product_attributes.append({&#34;prod_id&#34;: prod_id, &#34;product_name&#34;: product_name, &#34;item_name&#34;: item_name,
                                       &#34;item_size&#34;: item_size, &#34;item_price&#34;: item_price, &#34;item_ingredients&#34;: item_ingredients})

        return product_attributes

    def get_first_review_date(drv: webdriver.Firefox) -&gt; str:
        &#34;&#34;&#34;get_first_review_date scraped the first review data of the product.

        Args:
            drv (webdriver.Firefox): Selenium webdriver with opened product page.

        Returns:
            str: first review date.
        &#34;&#34;&#34;
        # close popup windows
        close_popups(drv)
        accept_alert(drv, 1)

        try:
            review_sort_trigger = drv.find_element_by_id(
                &#39;review_filter_sort_trigger&#39;)
            self.scroll_to_element(drv, review_sort_trigger)
            ActionChains(drv).move_to_element(
                review_sort_trigger).click(review_sort_trigger).perform()
            for btn in drv.find_elements_by_class_name(&#39;css-rfz1gg&#39;):
                if btn.text.lower() == &#39;oldest&#39;:
                    ActionChains(drv).move_to_element(
                        btn).click(btn).perform()
                    break
            time.sleep(6)
            close_popups(drv)
            accept_alert(drv, 1)
            rev = drv.find_elements_by_class_name(&#39;css-1kk8dps&#39;)[2:]
            try:
                first_review_date = convert_ago_to_date(
                    rev[0].find_element_by_class_name(&#39;css-h2vfi1&#39;).text)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                try:
                    first_review_date = convert_ago_to_date(
                        rev[1].find_element_by_class_name(&#39;css-h2vfi1&#39;).text)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    print(&#39;sorted but cant get first review date value&#39;)
                    first_review_date = &#39;&#39;
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            first_review_date = &#39;&#39;
        return first_review_date

    for prod in self.meta.index[self.meta.index.isin(indices)]:
        #  ignore already extracted products
        if self.meta.loc[prod, &#39;detail_scraped&#39;] in [&#39;Y&#39;, &#39;NA&#39;]:
            continue
        # print(prod, self.meta.loc[prod, &#39;detail_scraped&#39;])
        prod_id = self.meta.loc[prod, &#39;prod_id&#39;]
        product_name = self.meta.loc[prod, &#39;product_name&#39;]
        product_page = self.meta.loc[prod, &#39;product_page&#39;]

        # create webdriver
        if randomize_proxy_usage:
            use_proxy = np.random.choice([True, False])
        else:
            use_proxy = True
        if open_with_proxy_server:
            # print(use_proxy)
            drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
                                    path=self.detail_path)
        else:
            drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
                                    path=self.detail_path)
        # open product page
        drv.get(product_page)
        time.sleep(20)  # 30
        accept_alert(drv, 10)
        close_popups(drv)

        # check product page is valid and exists
        try:
            close_popups(drv)
            accept_alert(drv, 2)
            price = drv.find_element_by_class_name(&#39;css-1865ad6&#39;)
            self.scroll_to_element(drv, price)
            price = price.text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            drv.quit()
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) no longer exists in the previously fetched link.\
                    (link:{product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            self.meta.loc[prod, &#39;detail_scraped&#39;] = &#39;NA&#39;
            continue

        try:
            chat_popup_button = WebDriverWait(drv, 3).until(
                EC.presence_of_element_located((By.XPATH, &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)))
            chat_popup_button = drv.find_element_by_xpath(
                &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)
            self.scroll_to_element(drv, chat_popup_button)
            ActionChains(drv).move_to_element(
                chat_popup_button).click(chat_popup_button).perform()
        except TimeoutException:
            pass

        # get all product info tabs such as how-to-use, about-brand, ingredients
        prod_tabs = []
        prod_tabs = drv.find_elements_by_class_name(&#39;css-1wugx5m&#39;)
        prod_tabs.extend(drv.find_elements_by_class_name(&#39;css-12vae0p&#39;))

        tab_names = []
        for t in prod_tabs:
            tab_names.append(t.text.lower())

        # no. of votes
        try:
            votes = drv.find_elements_by_class_name(&#39;css-2rg6q7&#39;)[-1].text
            # print(votes)
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            votes = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) votes does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        # product details
        if &#39;details&#39; in tab_names:
            try:
                close_popups(drv)
                accept_alert(drv, 1)
                tab_num = tab_names.index(&#39;details&#39;)
                detail_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                try:
                    time.sleep(1)
                    self.scroll_to_element(drv, detail_button)
                    ActionChains(drv).move_to_element(
                        detail_button).click(detail_button).perform()
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    details = &#34;&#34;
                else:
                    try:
                        details = drv.find_element_by_xpath(
                            f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        details = &#34;&#34;
                        self.logger.info(str.encode(
                            f&#39;product: {product_name} (prod_id: {prod_id}) product detail text\
                                    does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    # print(details)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                details = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) product detail extraction failed&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
        else:
            details = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) product detail does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        # how to use
        if &#39;how to use&#39; in tab_names:
            try:
                close_popups(drv)
                accept_alert(drv, 1)

                tab_num = tab_names.index(&#39;how to use&#39;)
                how_to_use_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                try:
                    time.sleep(1)
                    self.scroll_to_element(drv, how_to_use_button)
                    ActionChains(drv).move_to_element(
                        how_to_use_button).click(how_to_use_button).perform()
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    how_to_use = &#34;&#34;
                else:
                    try:
                        how_to_use = drv.find_element_by_xpath(
                            f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        how_to_use = &#34;&#34;
                        self.logger.info(str.encode(
                            f&#39;product: {product_name} (prod_id: {prod_id}) how_to_use text\
                                 does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    # print(how_to_use)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                how_to_use = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) how_to_use extraction failed&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
        else:
            how_to_use = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) how_to_use does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        # about the brand
        if &#39;about the brand&#39; in tab_names:
            try:
                close_popups(drv)
                accept_alert(drv, 1)

                tab_num = tab_names.index(&#39;about the brand&#39;)
                about_the_brand_button = drv.find_element_by_id(
                    f&#39;tab{tab_num}&#39;)
                try:
                    time.sleep(1)
                    self.scroll_to_element(drv, about_the_brand_button)
                    ActionChains(drv).move_to_element(
                        about_the_brand_button).click(about_the_brand_button).perform()
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    about_the_brand = &#34;&#34;
                else:
                    try:
                        about_the_brand = drv.find_element_by_xpath(
                            f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        about_the_brand = &#34;&#34;
                        self.logger.info(str.encode(
                            f&#39;product: {product_name} (prod_id: {prod_id}) about_the_brand text\
                                does not exist&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    # print(about_the_brand)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                about_the_brand = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) about_the_brand extraction failed&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
        else:
            about_the_brand = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) about_the_brand does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        self.scroll_down_page(drv, h2=0.4, speed=5)
        time.sleep(5)
        try:
            chat_popup_button = WebDriverWait(drv, 3).until(
                EC.presence_of_element_located((By.XPATH, &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)))
            chat_popup_button = drv.find_element_by_xpath(
                &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)
            self.scroll_to_element(drv, chat_popup_button)
            ActionChains(drv).move_to_element(
                chat_popup_button).click(chat_popup_button).perform()
        except TimeoutException:
            pass
        # click no. of reviews
        try:
            review_button = drv.find_element_by_class_name(&#39;css-1pjru6n&#39;)
            self.scroll_to_element(drv, review_button)
            ActionChains(drv).move_to_element(
                review_button).click(review_button).perform()
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)

        try:
            first_review_date = get_first_review_date(drv)
            # print(first_review_date)
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            first_review_date = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) first_review_date scrape failed.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        try:
            close_popups(drv)
            accept_alert(drv, 1)
            reviews = int(drv.find_element_by_class_name(
                &#39;css-ils4e4&#39;).text.split()[0])
            # print(reviews)
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            reviews = 0
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) reviews does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        try:
            close_popups(drv)
            accept_alert(drv, 1)
            rating_distribution = drv.find_element_by_class_name(
                &#39;css-960eb6&#39;).text.split(&#39;\n&#39;)
            # print(rating_distribution)
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            rating_distribution = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) rating_distribution does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        try:
            close_popups(drv)
            accept_alert(drv, 1)
            would_recommend = drv.find_element_by_class_name(
                &#39;css-k9ne19&#39;).text
            # print(would_recommend)
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            would_recommend = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) would_recommend does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        product_attributes = pd.DataFrame(
            get_product_attributes(drv, product_name, prod_id))
        item_df = pd.concat(
            [item_df, pd.DataFrame(product_attributes)], axis=0)

        detail_data.append({&#39;prod_id&#39;: prod_id, &#39;product_name&#39;: product_name, &#39;abt_product&#39;: details,
                            &#39;how_to_use&#39;: how_to_use, &#39;abt_brand&#39;: about_the_brand,
                            &#39;reviews&#39;: reviews, &#39;votes&#39;: votes, &#39;rating_dist&#39;: rating_distribution,
                            &#39;would_recommend&#39;: would_recommend, &#39;first_review_date&#39;: first_review_date})
        # item_data.append(product_attributes)
        self.logger.info(str.encode(
            f&#39;product: {product_name} (prod_id: {prod_id}) details extracted successfully&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
        self.meta.loc[prod, &#39;detail_scraped&#39;] = &#39;Y&#39;

        if prod != 0 and prod % 10 == 0:
            if len(detail_data) &gt; 0:
                detail_data, item_df = store_data_refresh_mem(
                    detail_data, item_df)
        drv.quit()

    detail_data, item_df = store_data_refresh_mem(
        detail_data, item_df)
    self.logger.info(
        f&#39;Detail Extraction Complete for start_idx: (indices[0]) to end_idx: {indices[-1]}. Or for list of values.&#39;)</code></pre>
</details>
</dd>
<dt id="meiyume.sph.crawler.Detail.terminate_logging"><code class="name flex">
<span>def <span class="ident">terminate_logging</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def terminate_logging(self):
    &#34;&#34;&#34;terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.
    &#34;&#34;&#34;
    self.logger.handlers.clear()
    self.prod_detail_log.stop_log()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="meiyume.utils.Sephora" href="../utils.html#meiyume.utils.Sephora">Sephora</a></b></code>:
<ul class="hlist">
<li><code><a title="meiyume.utils.Sephora.open_browser" href="../utils.html#meiyume.utils.Browser.open_browser">open_browser</a></code></li>
<li><code><a title="meiyume.utils.Sephora.open_browser_firefox" href="../utils.html#meiyume.utils.Browser.open_browser_firefox">open_browser_firefox</a></code></li>
<li><code><a title="meiyume.utils.Sephora.scroll_down_page" href="../utils.html#meiyume.utils.Browser.scroll_down_page">scroll_down_page</a></code></li>
<li><code><a title="meiyume.utils.Sephora.scroll_to_element" href="../utils.html#meiyume.utils.Browser.scroll_to_element">scroll_to_element</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="meiyume.sph.crawler.DetailReview"><code class="flex name class">
<span>class <span class="ident">DetailReview</span></span>
<span>(</span><span>log: bool = True, path: pathlib.Path = WindowsPath('D:/Amit/Meiyume/meiyume_master_source_codes'))</span>
</code></dt>
<dd>
<div class="desc"><p>DetailReview [summary]</p>
<p>[extended_summary]</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>Sephora</code></strong> :&ensp;<code>[type]</code></dt>
<dd>[description]</dd>
</dl>
<p><strong>init</strong> [summary]</p>
<p>[extended_summary]</p>
<h2 id="args_1">Args</h2>
<dl>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>[description]. Defaults to True.</dd>
<dt><strong><code>path</code></strong> :&ensp;<code>Path</code>, optional</dt>
<dd>[description]. Defaults to Path.cwd().</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DetailReview(Sephora):
    &#34;&#34;&#34;DetailReview [summary]

    [extended_summary]

    Args:
        Sephora ([type]): [description]
    &#34;&#34;&#34;

    def __init__(self, log: bool = True, path: Path = Path.cwd()):
        &#34;&#34;&#34;__init__ [summary]

        [extended_summary]

        Args:
            log (bool, optional): [description]. Defaults to True.
            path (Path, optional): [description]. Defaults to Path.cwd().
        &#34;&#34;&#34;
        super().__init__(path=path, data_def=&#39;detail_review_image&#39;)
        self.path = path
        self.detail_current_progress_path = self.detail_path/&#39;current_progress&#39;
        self.detail_current_progress_path.mkdir(parents=True, exist_ok=True)

        self.review_current_progress_path = self.review_path/&#39;current_progress&#39;
        self.review_current_progress_path.mkdir(parents=True, exist_ok=True)

        old_detail_files = list(self.detail_path.glob(
            &#39;sph_product_detail_all*&#39;)) + list(self.detail_path.glob(
                &#39;sph_product_item_all*&#39;))
        for f in old_detail_files:
            shutil.move(str(f), str(self.old_detail_files_path))

        old_clean_detail_files = files = os.listdir(self.detail_clean_path)
        for f in old_clean_detail_files:
            shutil.move(str(self.detail_clean_path/f),
                        str(self.old_detail_clean_files_path))

        old_review_files = list(self.review_path.glob(
            &#39;sph_product_review_all*&#39;))
        for f in old_review_files:
            shutil.move(str(f), str(self.old_review_files_path))

        old_clean_review_files = os.listdir(self.review_clean_path)
        for f in old_clean_review_files:
            shutil.move(str(self.review_clean_path/f),
                        str(self.old_review_clean_files_path))
        if log:
            self.prod_detail_review_image_log = Logger(
                &#34;sph_prod_review_extraction&#34;, path=self.crawl_log_path)
            self.logger, _ = self.prod_detail_review_image_log.start_log()

    def get_details(self, drv: webdriver.Firefox, prod_id: str, product_name: str) -&gt; Tuple[dict, pd.DataFrame]:
        &#34;&#34;&#34;get_detail [summary]

        [extended_summary]

        Args:
            drv (webdriver.Firefox): [description]
            prod_id (str): [description]
            product_name (str): [description]

        Returns:
            Tuple[dict, pd.DataFrame]: [description]
        &#34;&#34;&#34;

        def get_item_attributes(drv: webdriver.Firefox, product_name: str, prod_id: str, use_button: bool = False,
                                multi_variety: bool = False, typ=None, ) -&gt; Tuple[str, str, str, str]:
            &#34;&#34;&#34;get_item_attributes [summary]

            [extended_summary]

            Args:
                drv (webdriver.Chrome): [description]
                product_name (str): [description]
                prod_id (str): [description]
                use_button (bool, optional): [description]. Defaults to False.
                multi_variety (bool, optional): [description]. Defaults to False.
                typ ([type], optional): [description]. Defaults to None.

            Returns:
                Tuple[str, str, str, str]: [description]
            &#34;&#34;&#34;
            # drv # type: webdriver.Chrome
            # close popup windows
            close_popups(drv)
            accept_alert(drv, 1)

            try:
                item_price = drv.find_element_by_class_name(&#39;css-1865ad6&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                item_price = &#39;&#39;
            # print(item_price)

            if multi_variety:
                try:
                    if use_button:
                        item_name = typ.find_element_by_tag_name(
                            &#39;button&#39;).get_attribute(&#39;aria-label&#39;)
                    else:
                        item_name = typ.get_attribute(&#39;aria-label&#39;)
                    # print(item_name)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    item_name = &#34;&#34;
                    self.logger.info(str.encode(
                        f&#39;product: {product_name} (prod_id: {prod_id}) item_name does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            else:
                item_name = &#34;&#34;

            try:
                item_size = drv.find_element_by_class_name(&#39;css-128n72s&#39;).text
                # print(item_size)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                item_size = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) item_size does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            # get all tabs
            first_tab = drv.find_element_by_id(f&#39;tab{0}&#39;)
            self.scroll_to_element(drv, first_tab)
            ActionChains(drv).move_to_element(
                first_tab).click(first_tab).perform()
            prod_tabs = []
            prod_tabs = drv.find_elements_by_class_name(&#39;css-1wugx5m&#39;)
            prod_tabs.extend(drv.find_elements_by_class_name(&#39;css-12vae0p&#39;))

            tab_names = []
            for t in prod_tabs:
                tab_names.append(t.text.lower())
            # print(tab_names)

            if &#39;ingredients&#39; in tab_names:
                close_popups(drv)
                accept_alert(drv, 1)
                if len(tab_names) == 5:
                    try:
                        tab_num = 2
                        ing_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                        self.scroll_to_element(drv, ing_button)
                        ActionChains(drv).move_to_element(
                            ing_button).click(ing_button).perform()
                        item_ing = drv.find_element_by_xpath(
                            f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        print(&#39;cant get ingredient but tab exists&#39;)
                        item_ing = &#34;&#34;
                        self.logger.info(str.encode(
                            f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients extraction failed&#39;,
                            &#39;utf-8&#39;, &#39;ignore&#39;))
                elif len(tab_names) == 4:
                    try:
                        tab_num = 1
                        ing_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                        self.scroll_to_element(drv, ing_button)
                        ActionChains(drv).move_to_element(
                            ing_button).click(ing_button).perform()
                        item_ing = drv.find_element_by_xpath(
                            f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        print(&#39;cant get ingredient but tab exists&#39;)
                        item_ing = &#34;&#34;
                        self.logger.info(str.encode(
                            f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients extraction failed.&#39;,
                            &#39;utf-8&#39;, &#39;ignore&#39;))
                elif len(tab_names) &lt; 4:
                    try:
                        tab_num = 0
                        ing_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                        self.scroll_to_element(drv, ing_button)
                        ActionChains(drv).move_to_element(
                            ing_button).click(ing_button).perform()
                        item_ing = drv.find_element_by_xpath(
                            f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        print(&#39;cant get ingredient but tab exists&#39;)
                        item_ing = &#34;&#34;
                        self.logger.info(str.encode(
                            f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients extraction failed.&#39;,
                            &#39;utf-8&#39;, &#39;ignore&#39;))
            else:
                item_ing = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            # print(item_ing)
            return item_name, item_size, item_price, item_ing

        def get_product_attributes(drv: webdriver.Firefox, product_name: str, prod_id: str) -&gt; list:
            &#34;&#34;&#34;get_product_attributes [summary]

            [extended_summary]

            Args:
                drv (webdriver.Chrome): [description]
                product_name (str): [description]
                prod_id (str): [description]

            Returns:
                list: [description]
            &#34;&#34;&#34;
            # get all the variation of product
            # close popup windows
            close_popups(drv)
            accept_alert(drv, 1)

            product_variety = []
            try:
                product_variety = drv.find_elements_by_class_name(
                    &#39;css-1j1jwa4&#39;)
                product_variety.extend(
                    drv.find_elements_by_class_name(&#39;css-cl742e&#39;))
                use_button = False
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
            try:
                if len(product_variety) &lt; 1:
                    product_variety = drv.find_elements_by_class_name(
                        &#39;css-5jqxch&#39;)
                    use_button = True
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)

            product_attributes = []

            if len(product_variety) &gt; 0:
                for typ in product_variety:
                    close_popups(drv)
                    accept_alert(drv, 1)
                    try:
                        self.scroll_to_element(drv, typ)
                        ActionChains(drv).move_to_element(
                            typ).click(typ).perform()
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    time.sleep(4)  # 8
                    item_name, item_size, item_price, item_ingredients = get_item_attributes(drv, product_name, prod_id,
                                                                                             multi_variety=True, typ=typ,
                                                                                             use_button=use_button)
                    product_attributes.append({&#34;prod_id&#34;: prod_id, &#34;product_name&#34;: product_name,
                                               &#34;item_name&#34;: item_name, &#34;item_size&#34;: item_size,
                                               &#34;item_price&#34;: item_price, &#34;item_ingredients&#34;: item_ingredients})
            else:
                item_name, item_size, item_price, item_ingredients = get_item_attributes(drv,
                                                                                         product_name, prod_id)
                product_attributes.append({&#34;prod_id&#34;: prod_id, &#34;product_name&#34;: product_name, &#34;item_name&#34;: item_name,
                                           &#34;item_size&#34;: item_size, &#34;item_price&#34;: item_price, &#34;item_ingredients&#34;: item_ingredients})

            return product_attributes

        def get_first_review_date(drv: webdriver.Firefox) -&gt; str:
            &#34;&#34;&#34;get_first_review_date [summary]

            [extended_summary]

            Args:
                drv (webdriver.Chrome): [description]

            Returns:
                str: [description]
            &#34;&#34;&#34;
            # close popup windows
            close_popups(drv)
            accept_alert(drv, 1)

            try:
                review_sort_trigger = drv.find_element_by_id(
                    &#39;review_filter_sort_trigger&#39;)
                self.scroll_to_element(drv, review_sort_trigger)
                ActionChains(drv).move_to_element(
                    review_sort_trigger).click(review_sort_trigger).perform()
                for btn in drv.find_elements_by_class_name(&#39;css-rfz1gg&#39;):
                    if btn.text.lower() == &#39;oldest&#39;:
                        ActionChains(drv).move_to_element(
                            btn).click(btn).perform()
                        break
                time.sleep(6)
                close_popups(drv)
                accept_alert(drv, 1)
                rev = drv.find_elements_by_class_name(&#39;css-1kk8dps&#39;)[2:]
                try:
                    first_review_date = convert_ago_to_date(
                        rev[0].find_element_by_class_name(&#39;css-h2vfi1&#39;).text)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    try:
                        first_review_date = convert_ago_to_date(
                            rev[1].find_element_by_class_name(&#39;css-h2vfi1&#39;).text)
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        print(&#39;sorted but cant get first review date value&#39;)
                        first_review_date = &#39;&#39;
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                first_review_date = &#39;&#39;
            return first_review_date

        # get all product info tabs such as how-to-use, about-brand, ingredients
        prod_tabs = []
        prod_tabs = drv.find_elements_by_class_name(&#39;css-1wugx5m&#39;)
        prod_tabs.extend(drv.find_elements_by_class_name(&#39;css-12vae0p&#39;))

        tab_names = []
        for t in prod_tabs:
            tab_names.append(t.text.lower())

        # no. of votes
        try:
            votes = drv.find_elements_by_class_name(&#39;css-2rg6q7&#39;)[-1].text
            # print(votes)
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            votes = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) votes does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        # product details
        if &#39;details&#39; in tab_names:
            try:
                close_popups(drv)
                accept_alert(drv, 1)
                tab_num = tab_names.index(&#39;details&#39;)
                detail_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                try:
                    time.sleep(1)
                    self.scroll_to_element(drv, detail_button)
                    ActionChains(drv).move_to_element(
                        detail_button).click(detail_button).perform()
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    details = &#34;&#34;
                else:
                    try:
                        details = drv.find_element_by_xpath(
                            f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        details = &#34;&#34;
                        self.logger.info(str.encode(
                            f&#39;product: {product_name} (prod_id: {prod_id}) product detail text\
                                    does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    # print(details)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                details = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) product detail extraction failed&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
        else:
            details = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) product detail does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        # how to use
        if &#39;how to use&#39; in tab_names:
            try:
                close_popups(drv)
                accept_alert(drv, 1)

                tab_num = tab_names.index(&#39;how to use&#39;)
                how_to_use_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                try:
                    time.sleep(1)
                    self.scroll_to_element(drv, how_to_use_button)
                    ActionChains(drv).move_to_element(
                        how_to_use_button).click(how_to_use_button).perform()
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    how_to_use = &#34;&#34;
                else:
                    try:
                        how_to_use = drv.find_element_by_xpath(
                            f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        how_to_use = &#34;&#34;
                        self.logger.info(str.encode(
                            f&#39;product: {product_name} (prod_id: {prod_id}) how_to_use text\
                                    does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    # print(how_to_use)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                how_to_use = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) how_to_use extraction failed&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
        else:
            how_to_use = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) how_to_use does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        # about the brand
        if &#39;about the brand&#39; in tab_names:
            try:
                close_popups(drv)
                accept_alert(drv, 1)

                tab_num = tab_names.index(&#39;about the brand&#39;)
                about_the_brand_button = drv.find_element_by_id(
                    f&#39;tab{tab_num}&#39;)
                try:
                    time.sleep(1)
                    self.scroll_to_element(drv, about_the_brand_button)
                    ActionChains(drv).move_to_element(
                        about_the_brand_button).click(about_the_brand_button).perform()
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    about_the_brand = &#34;&#34;
                else:
                    try:
                        about_the_brand = drv.find_element_by_xpath(
                            f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        about_the_brand = &#34;&#34;
                        self.logger.info(str.encode(
                            f&#39;product: {product_name} (prod_id: {prod_id}) about_the_brand text\
                                does not exist&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    # print(about_the_brand)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                about_the_brand = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) about_the_brand extraction failed&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
        else:
            about_the_brand = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) about_the_brand does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        self.scroll_down_page(drv, h2=0.4, speed=5)
        time.sleep(5)
        try:
            chat_popup_button = WebDriverWait(drv, 3).until(
                EC.presence_of_element_located((By.XPATH, &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)))
            chat_popup_button = drv.find_element_by_xpath(
                &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)
            self.scroll_to_element(drv, chat_popup_button)
            ActionChains(drv).move_to_element(
                chat_popup_button).click(chat_popup_button).perform()
        except TimeoutException:
            pass
        # click no. of reviews
        try:
            review_button = drv.find_element_by_class_name(&#39;css-1pjru6n&#39;)
            self.scroll_to_element(drv, review_button)
            ActionChains(drv).move_to_element(
                review_button).click(review_button).perform()
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)

        try:
            first_review_date = get_first_review_date(drv)
            # print(first_review_date)
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            first_review_date = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) first_review_date scrape failed.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        try:
            close_popups(drv)
            accept_alert(drv, 1)
            reviews = int(drv.find_element_by_class_name(
                &#39;css-ils4e4&#39;).text.split()[0])
            # print(reviews)
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            reviews = 0
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) reviews does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        try:
            close_popups(drv)
            accept_alert(drv, 1)
            rating_distribution = drv.find_element_by_class_name(
                &#39;css-960eb6&#39;).text.split(&#39;\n&#39;)
            # print(rating_distribution)
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            rating_distribution = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) rating_distribution does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        try:
            close_popups(drv)
            accept_alert(drv, 1)
            would_recommend = drv.find_element_by_class_name(
                &#39;css-k9ne19&#39;).text
            # print(would_recommend)
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            would_recommend = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) would_recommend does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        detail = {&#39;prod_id&#39;: prod_id, &#39;product_name&#39;: product_name, &#39;abt_product&#39;: details,
                  &#39;how_to_use&#39;: how_to_use, &#39;abt_brand&#39;: about_the_brand,
                  &#39;reviews&#39;: reviews, &#39;votes&#39;: votes, &#39;rating_dist&#39;: rating_distribution,
                  &#39;would_recommend&#39;: would_recommend, &#39;first_review_date&#39;: first_review_date}

        item = pd.DataFrame(
            get_product_attributes(drv, product_name, prod_id))

        return detail, item

    def get_reviews(self,  drv: webdriver.Firefox, prod_id: str, product_name: str,
                    last_scraped_review_date: str, no_of_reviews: int,
                    incremental: bool = True, reviews: list = []) -&gt; list:
        &#34;&#34;&#34;get_reviews [summary]

        [extended_summary]

        Args:
            drv (webdriver.Firefox): [description]
            prod_id (str): [description]
            product_name (str): [description]
            last_scraped_review_date (str): [description]
            no_of_reviews (int): [description]
            incremental (bool, optional): [description]. Defaults to True.
            reviews (list, optional): [description]. Defaults to [].

        Returns:
            list: [description]
        &#34;&#34;&#34;
        # print(no_of_reviews)
        # drv.find_element_by_class_name(&#39;css-2rg6q7&#39;).click()
        if incremental and last_scraped_review_date != &#39;&#39;:
            for n in range(no_of_reviews//6):
                if n &gt; 400:
                    break

                time.sleep(0.4)
                revs = drv.find_elements_by_class_name(
                    &#39;css-1kk8dps&#39;)[2:]

                try:
                    if pd.to_datetime(convert_ago_to_date(revs[-1].find_element_by_class_name(&#39;css-1t84k9w&#39;).text),
                                      infer_datetime_format=True)\
                            &lt; pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                        # print(&#39;breaking incremental&#39;)
                        break
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    try:
                        if pd.to_datetime(convert_ago_to_date(revs[-2].find_element_by_class_name(&#39;css-1t84k9w&#39;).text),
                                          infer_datetime_format=True)\
                                &lt; pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                            # print(&#39;breaking incremental&#39;)
                            break
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        self.logger.info(str.encode(f&#39;Product: {product_name} prod_id: {prod_id} \
                                                        last_scraped_review_date to current review date \
                                                        comparision failed.(page: {product_page})&#39;,
                                                    &#39;utf-8&#39;, &#39;ignore&#39;))
                        # print(&#39;in second except block&#39;)
                        continue
                try:
                    show_more_review_button = drv.find_element_by_class_name(
                        &#39;css-xswy5p&#39;)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to get show more review button.&#39;)
                else:
                    try:
                        self.scroll_to_element(
                            drv, show_more_review_button)
                        ActionChains(drv).move_to_element(
                            show_more_review_button).click(show_more_review_button).perform()
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)
                        accept_alert(drv, 1)
                        close_popups(drv)
                        try:
                            self.scroll_to_element(
                                drv, show_more_review_button)
                            ActionChains(drv).move_to_element(
                                show_more_review_button).click(show_more_review_button).perform()
                        except Exception as ex:
                            log_exception(self.logger,
                                          additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)

        else:
            # print(&#39;inside get all reviews&#39;)
            # 6 because for click sephora shows 6 reviews. additional 25 no. of clicks for buffer.
            for n in range(no_of_reviews//6+10):
                &#39;&#39;&#39;
                code will stop after getting 1800 reviews of one particular product
                when crawling all reviews. By default it will get latest 1800 reviews.
                then in subsequent incremental runs it will get al new reviews on weekly basis
                &#39;&#39;&#39;
                if n &gt;= 400:  # 200:
                    break
                time.sleep(1)
                # close any opened popups by escape
                accept_alert(drv, 1)
                close_popups(drv)
                try:
                    show_more_review_button = drv.find_element_by_class_name(
                        &#39;css-xswy5p&#39;)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to get show more review button.&#39;)
                else:
                    try:
                        self.scroll_to_element(
                            drv, show_more_review_button)
                        ActionChains(drv).move_to_element(
                            show_more_review_button).click(show_more_review_button).perform()
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)
                        accept_alert(drv, 1)
                        close_popups(drv)
                        try:
                            self.scroll_to_element(
                                drv, show_more_review_button)
                            ActionChains(drv).move_to_element(
                                show_more_review_button).click(show_more_review_button).perform()
                        except Exception as ex:
                            log_exception(self.logger,
                                          additional_information=f&#39;Prod ID: {prod_id}.\
                                                Failed to click on show more review button.&#39;)
                            try:
                                self.scroll_to_element(
                                    drv, show_more_review_button)
                                ActionChains(drv).move_to_element(
                                    show_more_review_button).click(show_more_review_button).perform()
                            except Exception as ex:
                                log_exception(self.logger,
                                              additional_information=f&#39;Prod ID: {prod_id}.\
                                                Failed to click on show more review button.&#39;)
                                accept_alert(drv, 2)
                                close_popups(drv)
                                try:
                                    self.scroll_to_element(
                                        drv, show_more_review_button)
                                    ActionChains(drv).move_to_element(
                                        show_more_review_button).click(show_more_review_button).perform()
                                except Exception as ex:
                                    log_exception(self.logger,
                                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)
                                    if n &lt; (no_of_reviews//6):
                                        self.logger.info(str.encode(f&#39;Product: {product_name} - prod_id \
                                            {prod_id} breaking click next review loop.\
                                                                    [total_reviews:{no_of_reviews} loaded_reviews:{n}]\
                                                                    (page link: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                                        self.logger.info(str.encode(f&#39;Product: {product_name} - prod_id {prod_id} cant load all reviews.\
                                                                        Check click next 6 reviews\
                                                                        code section(page link: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                                    break

        accept_alert(drv, 2)
        close_popups(drv)

        product_reviews = drv.find_elements_by_class_name(
            &#39;css-1kk8dps&#39;)[2:]

        # print(&#39;starting extraction&#39;)
        r = 0
        for rev in product_reviews:
            accept_alert(drv, 0.5)
            close_popups(drv)
            self.scroll_to_element(drv, rev)
            ActionChains(drv).move_to_element(rev).perform()

            try:
                try:
                    review_text = rev.find_element_by_class_name(
                        &#39;css-1jg2pb9&#39;).text
                except NoSuchElementException:
                    review_text = rev.find_element_by_class_name(
                        &#39;css-429528&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_text. Skip review.&#39;)
                continue

            try:
                review_date = convert_ago_to_date(
                    rev.find_element_by_class_name(&#39;css-h2vfi1&#39;).text)
                if pd.to_datetime(review_date, infer_datetime_format=True) &lt;= \
                        pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                    continue
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_date.&#39;)
                review_date = &#39;&#39;

            try:
                review_title = rev.find_element_by_class_name(
                    &#39;css-1jfmule&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_title.&#39;)
                review_title = &#39;&#39;

            try:
                product_variant = rev.find_element_by_class_name(
                    &#39;css-1op1cn7&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract product_variant.&#39;)
                product_variant = &#39;&#39;

            try:
                user_rating = rev.find_element_by_class_name(
                    &#39;css-3z5ot7&#39;).get_attribute(&#39;aria-label&#39;)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract user_rating.&#39;)
                user_rating = &#39;&#39;

            try:
                user_attribute = [{&#39;_&#39;.join(u.lower().split()[0:-1]): u.lower().split()[-1]}
                                  for u in rev.find_element_by_class_name(&#39;css-ecreye&#39;).text.split(&#39;\n&#39;)]
                # user_attribute = []
                # for u in rev.find_elements_by_class_name(&#39;css-j5yt83&#39;):
                #     user_attribute.append(
                #         {&#39;_&#39;.join(u.text.lower().split()[0:-1]): u.text.lower().split()[-1]})
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract user_attribute.&#39;)
                user_attribute = []

            try:
                recommend = rev.find_element_by_class_name(
                    &#39;css-1tf5yph&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract recommend.&#39;)
                recommend = &#39;&#39;

            try:
                helpful = rev.find_element_by_class_name(&#39;css-b7zg5r&#39;).text
                # helpful = []
                # for h in rev.find_elements_by_class_name(&#39;css-39esqn&#39;):
                #     helpful.append(h.text)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract helpful.&#39;)
                helpful = &#39;&#39;

            reviews.append({&#39;prod_id&#39;: prod_id, &#39;product_name&#39;: product_name,
                            &#39;user_attribute&#39;: user_attribute, &#39;product_variant&#39;: product_variant,
                            &#39;review_title&#39;: review_title, &#39;review_text&#39;: review_text,
                            &#39;review_rating&#39;: user_rating, &#39;recommend&#39;: recommend,
                            &#39;review_date&#39;: review_date,   &#39;helpful&#39;: helpful})
        return reviews

    def crawl_page(self, indices: list, open_headless: bool, open_with_proxy_server: bool,
                   randomize_proxy_usage: bool, detail_data: list = [],
                   item_df=pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                                 &#39;item_size&#39;, &#39;item_price&#39;,
                                                 &#39;item_ingredients&#39;]),
                   review_data: list = [], incremental: bool = True):
        &#34;&#34;&#34;crawl_page

        [extended_summary]

        Args:
            indices (list): [description]
            open_headless (bool): [description]
            open_with_proxy_server (bool): [description]
            randomize_proxy_usage (bool): [description]
            detail_data (list, optional): [description]. Defaults to [].
            item_df ([type], optional): [description]. Defaults to pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
            &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;]).
            review_data (list, optional): [description]. Defaults to [].
            incremental (bool, optional): [description]. Defaults to True.
        &#34;&#34;&#34;

        def store_data_refresh_mem(detail_data: list, item_df: pd.DataFrame,
                                   review_data: list) -&gt; Tuple[list, pd.DataFrame, list]:
            &#34;&#34;&#34;store_data_refresh_mem [summary]

            [extended_summary]

            Args:
                detail_data (list): [description]
                item_df (pd.DataFrame): [description]
                review_data (list): [description]

            Returns:
                Tuple[list, pd.DataFrame, list]: [description]
            &#34;&#34;&#34;
            pd.DataFrame(detail_data).to_csv(self.detail_current_progress_path /
                                             f&#39;sph_prod_detail_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                                             index=None)
            item_df.reset_index(inplace=True, drop=True)
            item_df.to_csv(self.detail_current_progress_path /
                           f&#39;sph_prod_item_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                           index=None)
            item_df = pd.DataFrame(columns=[
                                   &#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;, &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;])

            pd.DataFrame(review_data).to_csv(self.review_current_progress_path /
                                             f&#39;sph_prod_review_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                                             index=None)
            self.meta.to_csv(
                self.path/&#39;sph_detail_review_image_progress_tracker.csv&#39;, index=None)
            return [], item_df, []

        for prod in self.meta.index[self.meta.index.isin(indices)]:
            #  ignore already extracted products
            if self.meta.loc[prod, &#39;scraped&#39;] in [&#39;Y&#39;, &#39;NA&#39;]:
                continue
            # print(prod, self.meta.loc[prod, &#39;detail_scraped&#39;])
            prod_id = self.meta.loc[prod, &#39;prod_id&#39;]
            product_name = self.meta.loc[prod, &#39;product_name&#39;]
            product_page = self.meta.loc[prod, &#39;product_page&#39;]
            last_scraped_review_date = self.meta.loc[prod,
                                                     &#39;last_scraped_review_date&#39;]
            # create webdriver
            if randomize_proxy_usage:
                use_proxy = np.random.choice([True, False])
            else:
                use_proxy = True
            if open_with_proxy_server:
                # print(use_proxy)
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
                                        path=self.detail_path)
            else:
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
                                        path=self.detail_path)
            # open product page
            drv.get(product_page)
            time.sleep(20)  # 30
            accept_alert(drv, 10)
            close_popups(drv)

            self.scroll_down_page(drv, speed=6, h2=0.6)
            time.sleep(5)

            try:
                product_text = drv.find_element_by_class_name(
                    &#39;css-1wag3se&#39;).text
                if &#39;productnotcarried&#39; in product_text.lower():
                    self.logger.info(str.encode(f&#39;Product Name: {product_name}, Product ID: {prod_id} extraction failed.\
                                            Product may not be available for sell currently.(Page: {product_page})&#39;,
                                                &#39;utf-8&#39;, &#39;ignore&#39;))
                    self.meta.loc[prod, &#39;scraped&#39;] = &#39;NA&#39;
                    self.meta.to_csv(
                        self.path/&#39;sph_detail_review_image_progress_tracker.csv&#39;, index=None)
                    drv.quit()
                    continue
            except Exception as ex:
                log_exception(
                    self.logger, additional_information=f&#39;Prod ID: {prod_id}&#39;)

            # check product page is valid and exists
            try:
                close_popups(drv)
                accept_alert(drv, 2)
                price = drv.find_element_by_class_name(&#39;css-1865ad6&#39;)
                self.scroll_to_element(drv, price)
                price = price.text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                drv.quit()
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) no longer exists in the previously fetched link.\
                        (link:{product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                self.meta.loc[prod, &#39;detail_scraped&#39;] = &#39;NA&#39;
                continue

            try:
                chat_popup_button = WebDriverWait(drv, 3).until(
                    EC.presence_of_element_located((By.XPATH, &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)))
                chat_popup_button = drv.find_element_by_xpath(
                    &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)
                self.scroll_to_element(drv, chat_popup_button)
                ActionChains(drv).move_to_element(
                    chat_popup_button).click(chat_popup_button).perform()
            except TimeoutException:
                pass

            detail, item = self.get_details(drv, prod_id, product_name)

            detail_data.append(detail)
            item_df = pd.concat(
                [item_df, item], axis=0)

            # item_data.append(product_attributes)
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) details extracted successfully&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            try:
                close_popups(drv)
                accept_alert(drv, 1)
                no_of_reviews = int(drv.find_element_by_class_name(
                    &#39;css-ils4e4&#39;).text.split()[0])
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                self.logger.info(str.encode(f&#39;Product: {product_name} prod_id: {prod_id} reviews extraction failed.\
                                              Either product has no reviews or not\
                                              available for sell currently.(page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                no_of_reviews = 0
                # self.meta.loc[prod, &#39;review_scraped&#39;] = &#34;NA&#34;
                # self.meta.to_csv(
                #     self.review_path/&#39;sph_review_progress_tracker.csv&#39;, index=None)
                # drv.quit()
                # # print(&#39;in except - continue&#39;)
                # continue

            if no_of_reviews &gt; 0:
                reviews = self.get_reviews(
                    drv, prod_id, product_name, last_scraped_review_date, no_of_reviews, incremental)
                if len(reviews) &gt; 0:
                    review_data.extend(reviews)

            if not incremental:
                self.logger.info(str.encode(
                    f&#39;Product_name: {product_name} prod_id:{prod_id} reviews extracted successfully.(total_reviews: {no_of_reviews}, \
                    extracted_reviews: {len(reviews)}, page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            else:
                self.logger.info(str.encode(
                    f&#39;Product_name: {product_name} prod_id:{prod_id} new reviews extracted successfully.\
                        (no_of_new_extracted_reviews: {len(reviews)},\
                         page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

            if prod != 0 and prod % 5 == 0:
                detail_data, item_df, review_data = store_data_refresh_mem(
                    detail_data, item_df, review_data)

            self.meta.loc[prod, &#39;scraped&#39;] = &#39;Y&#39;
            drv.quit()

        detail_data, item_df, review_data = store_data_refresh_mem(
            detail_data, item_df, review_data)
        self.logger.info(
            f&#39;Extraction Complete for start_idx: (indices[0]) to end_idx: {indices[-1]}. Or for list of values.&#39;)

    def extract(self, metadata: Union[pd.DataFrame, str, Path], download: bool = True, n_workers: int = 5,
                fresh_start: bool = False, auto_fresh_start: bool = False, incremental: bool = True,
                open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = True,
                start_idx: Optional[int] = None, end_idx: Optional[int] = None, list_of_index=None,
                compile_progress_files: bool = False, clean: bool = True, delete_progress: bool = False):
        &#34;&#34;&#34;extract [summary]

        [extended_summary]

        Args:
            metadata (Union[pd.DataFrame, str, Path]): [description]
            download (bool, optional): [description]. Defaults to True.
            n_workers (int, optional): [description]. Defaults to 5.
            fresh_start (bool, optional): [description]. Defaults to False.
            auto_fresh_start (bool, optional): [description]. Defaults to False.
            incremental (bool, optional): [description]. Defaults to True.
            open_headless (bool, optional): [description]. Defaults to False.
            open_with_proxy_server (bool, optional): [description]. Defaults to True.
            randomize_proxy_usage (bool, optional): [description]. Defaults to True.
            start_idx (Optional[int], optional): [description]. Defaults to None.
            end_idx (Optional[int], optional): [description]. Defaults to None.
            list_of_index ([type], optional): [description]. Defaults to None.
            compile_progress_files (bool, optional): [description]. Defaults to False.
            clean (bool, optional): [description]. Defaults to True.
            delete_progress (bool, optional): [description]. Defaults to False.
        &#34;&#34;&#34;
        def fresh():
            if not isinstance(metadata, pd.core.frame.DataFrame):
                list_of_files = self.metadata_clean_path.glob(
                    &#39;no_cat_cleaned_sph_product_metadata_all*&#39;)
                self.meta = pd.read_feather(max(list_of_files, key=os.path.getctime))[
                    [&#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;, &#39;last_scraped_review_date&#39;]]
            else:
                self.meta = metadata[[
                    &#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;, &#39;last_scraped_review_date&#39;]]
            self.meta.last_scraped_review_date.fillna(&#39;&#39;, inplace=True)
            self.meta[&#39;scraped&#39;] = &#39;N&#39;

        if download:
            if fresh_start:
                fresh()
                self.logger.info(
                    &#39;Starting Fresh Deatil Review Image Extraction.&#39;)
            else:
                if Path(self.path/&#39;sph_detail_review_image_progress_tracker.csv&#39;).exists():
                    self.meta = pd.read_csv(
                        self.path/&#39;sph_detail_review_image_progress_tracker.csv&#39;)
                    if sum(self.meta.scraped == &#39;N&#39;) == 0:
                        if auto_fresh_start:
                            fresh()
                            self.logger.info(
                                &#39;Last Run was Completed. Starting Fresh Extraction.&#39;)
                        else:
                            self.logger.info(
                                &#39;Deatil Review Image extraction for this cycle is complete.&#39;)
                    else:
                        self.logger.info(
                            &#39;Continuing Deatil Review Image Extraction From Last Run.&#39;)
                else:
                    fresh()
                    self.logger.info(
                        &#39;Deatil Review Image Progress Tracker does not exist. Starting Fresh Extraction.&#39;)

            # set list or range of product indices to crawl
            if list_of_index:
                indices = list_of_index
            elif start_idx and end_idx is None:
                indices = range(start_idx, len(self.meta))
            elif start_idx is None and end_idx:
                indices = range(0, end_idx)
            elif start_idx is not None and end_idx is not None:
                indices = range(start_idx, end_idx)
            else:
                indices = range(len(self.meta))
            print(indices)

            if list_of_index:
                self.crawl_page(indices=list_of_index, incremental=incremental,
                                open_headless=open_headless,
                                open_with_proxy_server=open_with_proxy_server,
                                randomize_proxy_usage=randomize_proxy_usage)
            else:  # By default the code will with 5 concurrent threads. you can change this behaviour by changing n_workers
                if start_idx:
                    lst_of_lst = ranges(
                        indices[-1]+1, n_workers, start_idx=start_idx)
                else:
                    lst_of_lst = ranges(len(indices), n_workers)
                print(lst_of_lst)
                # detail_Data and item_data are lists of empty lists so that each namepace of function call will have its separate detail_data
                # list to strore scraped dictionaries. will save memory(ram/hard-disk) consumption. will stop data duplication
                headless = [open_headless for i in lst_of_lst]
                proxy = [open_with_proxy_server for i in lst_of_lst]
                rand_proxy = [randomize_proxy_usage for i in lst_of_lst]
                detail_data = [[] for i in lst_of_lst]  # type: List
                # item_data=[[] for i in lst_of_lst]  # type: List
                item_df = [pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                                 &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;])
                           for i in lst_of_lst]
                review_data = [[] for i in lst_of_lst]  # type: list
                inc_list = [incremental for i in lst_of_lst]  # type: list
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    # but each of the function namespace will be modifying only one metadata tracing file so that progress saving
                    # is tracked correctly. else multiple progress tracker file will be created with difficulty to combine correct
                    # progress information
                    print(&#39;inside executor&#39;)
                    executor.map(self.crawl_page, lst_of_lst,
                                 headless, proxy, rand_proxy,
                                 detail_data, item_df, review_data,
                                 inc_list)
        try:
            if compile_progress_files:
                self.logger.info(&#39;Creating Combined Detail and Item File&#39;)
                if datetime.now().day &lt; 15:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
                else:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;

                det_li = []
                self.bad_det_li = []
                detail_files = [f for f in self.detail_current_progress_path.glob(
                    &#34;sph_prod_detail_extract_progress_*&#34;)]
                for file in detail_files:
                    try:
                        df = pd.read_csv(file)
                    except Exception:
                        self.bad_det_li.append(file)
                    else:
                        det_li.append(df)

                detail_df = pd.concat(det_li, axis=0, ignore_index=True)
                detail_df.drop_duplicates(inplace=True)
                detail_df.reset_index(inplace=True, drop=True)
                detail_df[&#39;meta_date&#39;] = meta_date
                detail_filename = f&#39;sph_product_detail_all_{meta_date}.csv&#39;
                detail_df.to_csv(self.detail_path/detail_filename, index=None)
                # detail_df.to_feather(self.detail_path/detail_filename)

                item_li = []
                self.bad_item_li = []
                item_files = [f for f in self.current_progress_path.glob(
                    &#34;sph_prod_item_extract_progress_*&#34;)]
                for file in item_files:
                    try:
                        idf = pd.read_csv(file)
                    except Exception:
                        self.bad_item_li.append(file)
                    else:
                        item_li.append(idf)

                item_dataframe = pd.concat(item_li, axis=0, ignore_index=True)
                item_dataframe.drop_duplicates(inplace=True)
                item_dataframe.reset_index(inplace=True, drop=True)
                item_dataframe[&#39;meta_date&#39;] = meta_date
                item_filename = f&#39;sph_product_item_all_{meta_date}.csv&#39;
                item_dataframe.to_csv(
                    self.detail_path/item_filename, index=None)
                # item_df.to_feather(self.detail_path/item_filename)

                self.logger.info(
                    f&#39;Detail and Item files created. Please look for file sph_product_detail_all and\
                        sph_product_item_all in path {self.detail_path}&#39;)
                print(
                    f&#39;Detail and Item files created. Please look for file sph_product_detail_all and\
                        sph_product_item_all in path {self.detail_path}&#39;)

                self.logger.info(&#39;Creating Combined Review File&#39;)
                if datetime.now().day &lt; 15:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
                else:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;
                rev_li = []
                self.bad_rev_li = []
                review_files = [f for f in self.current_progress_path.glob(
                    &#34;sph_prod_review_extract_progress_*&#34;)]
                for file in review_files:
                    try:
                        df = pd.read_csv(file)
                    except Exception:
                        self.bad_rev_li.append(file)
                    else:
                        rev_li.append(df)
                rev_df = pd.concat(rev_li, axis=0, ignore_index=True)
                rev_df.drop_duplicates(inplace=True)
                rev_df.reset_index(inplace=True, drop=True)
                rev_df[&#39;meta_date&#39;] = pd.to_datetime(meta_date).date()
                review_filename = f&#39;sph_product_review_all_{pd.to_datetime(meta_date).date()}&#39;
                # , index=None)
                rev_df.to_feather(self.review_path/review_filename)

                self.logger.info(
                    f&#39;Review file created. Please look for file sph_product_review_all in path {self.review_path}&#39;)
                print(
                    f&#39;Review file created. Please look for file sph_product_review_all in path {self.review_path}&#39;)

                if clean:
                    detail_cleaner = Cleaner(path=self.path)
                    self.detail_clean_df = detail_cleaner.clean(
                        self.detail_path/detail_filename)
                    item_cleaner = Cleaner(path=self.path)
                    self.item_clean_df, self.ing_clean_df = item_cleaner.clean(
                        self.detail_path/item_filename)

                    review_cleaner = Cleaner(path=self.path)
                    self.review_clean_df = review_cleaner.clean(
                        self.review_path/review_filename)

                    file_creation_status = True
            else:
                file_creation_status = False
        except Exception as ex:
            log_exception(
                self.logger, additional_information=f&#39;Detail Item Review Combined File Creation Failed.&#39;)
            file_creation_status = False

        if delete_progress and file_creation_status:
            shutil.rmtree(
                f&#39;{self.detail_path}\\current_progress&#39;, ignore_errors=True)
            shutil.rmtree(
                f&#39;{self.review_path}\\current_progress&#39;, ignore_errors=True)
            self.logger.info(&#39;Progress files deleted&#39;)

    def terminate_logging(self):
        &#34;&#34;&#34;terminate_logging.&#34;&#34;&#34;
        self.logger.handlers.clear()
        self.prod_detail_review_image_log.stop_log()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="meiyume.utils.Sephora" href="../utils.html#meiyume.utils.Sephora">Sephora</a></li>
<li><a title="meiyume.utils.Browser" href="../utils.html#meiyume.utils.Browser">Browser</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="meiyume.sph.crawler.DetailReview.crawl_page"><code class="name flex">
<span>def <span class="ident">crawl_page</span></span>(<span>self, indices: list, open_headless: bool, open_with_proxy_server: bool, randomize_proxy_usage: bool, detail_data: list = [], item_df=Empty DataFrame
Columns: [prod_id, product_name, item_name, item_size, item_price, item_ingredients]
Index: [], review_data: list = [], incremental: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>crawl_page</p>
<p>[extended_summary]</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>indices</code></strong> :&ensp;<code>list</code></dt>
<dd>[description]</dd>
<dt><strong><code>open_headless</code></strong> :&ensp;<code>bool</code></dt>
<dd>[description]</dd>
<dt><strong><code>open_with_proxy_server</code></strong> :&ensp;<code>bool</code></dt>
<dd>[description]</dd>
<dt><strong><code>randomize_proxy_usage</code></strong> :&ensp;<code>bool</code></dt>
<dd>[description]</dd>
<dt><strong><code>detail_data</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>[description]. Defaults to [].</dd>
<dt><strong><code>item_df</code></strong> :&ensp;<code>[type]</code>, optional</dt>
<dd>[description]. Defaults to pd.DataFrame(columns=['prod_id', 'product_name', 'item_name',</dd>
<dt>'item_size', 'item_price', 'item_ingredients']).</dt>
<dt><strong><code>review_data</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>[description]. Defaults to [].</dd>
<dt><strong><code>incremental</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>[description]. Defaults to True.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def crawl_page(self, indices: list, open_headless: bool, open_with_proxy_server: bool,
               randomize_proxy_usage: bool, detail_data: list = [],
               item_df=pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                             &#39;item_size&#39;, &#39;item_price&#39;,
                                             &#39;item_ingredients&#39;]),
               review_data: list = [], incremental: bool = True):
    &#34;&#34;&#34;crawl_page

    [extended_summary]

    Args:
        indices (list): [description]
        open_headless (bool): [description]
        open_with_proxy_server (bool): [description]
        randomize_proxy_usage (bool): [description]
        detail_data (list, optional): [description]. Defaults to [].
        item_df ([type], optional): [description]. Defaults to pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
        &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;]).
        review_data (list, optional): [description]. Defaults to [].
        incremental (bool, optional): [description]. Defaults to True.
    &#34;&#34;&#34;

    def store_data_refresh_mem(detail_data: list, item_df: pd.DataFrame,
                               review_data: list) -&gt; Tuple[list, pd.DataFrame, list]:
        &#34;&#34;&#34;store_data_refresh_mem [summary]

        [extended_summary]

        Args:
            detail_data (list): [description]
            item_df (pd.DataFrame): [description]
            review_data (list): [description]

        Returns:
            Tuple[list, pd.DataFrame, list]: [description]
        &#34;&#34;&#34;
        pd.DataFrame(detail_data).to_csv(self.detail_current_progress_path /
                                         f&#39;sph_prod_detail_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                                         index=None)
        item_df.reset_index(inplace=True, drop=True)
        item_df.to_csv(self.detail_current_progress_path /
                       f&#39;sph_prod_item_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                       index=None)
        item_df = pd.DataFrame(columns=[
                               &#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;, &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;])

        pd.DataFrame(review_data).to_csv(self.review_current_progress_path /
                                         f&#39;sph_prod_review_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                                         index=None)
        self.meta.to_csv(
            self.path/&#39;sph_detail_review_image_progress_tracker.csv&#39;, index=None)
        return [], item_df, []

    for prod in self.meta.index[self.meta.index.isin(indices)]:
        #  ignore already extracted products
        if self.meta.loc[prod, &#39;scraped&#39;] in [&#39;Y&#39;, &#39;NA&#39;]:
            continue
        # print(prod, self.meta.loc[prod, &#39;detail_scraped&#39;])
        prod_id = self.meta.loc[prod, &#39;prod_id&#39;]
        product_name = self.meta.loc[prod, &#39;product_name&#39;]
        product_page = self.meta.loc[prod, &#39;product_page&#39;]
        last_scraped_review_date = self.meta.loc[prod,
                                                 &#39;last_scraped_review_date&#39;]
        # create webdriver
        if randomize_proxy_usage:
            use_proxy = np.random.choice([True, False])
        else:
            use_proxy = True
        if open_with_proxy_server:
            # print(use_proxy)
            drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
                                    path=self.detail_path)
        else:
            drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
                                    path=self.detail_path)
        # open product page
        drv.get(product_page)
        time.sleep(20)  # 30
        accept_alert(drv, 10)
        close_popups(drv)

        self.scroll_down_page(drv, speed=6, h2=0.6)
        time.sleep(5)

        try:
            product_text = drv.find_element_by_class_name(
                &#39;css-1wag3se&#39;).text
            if &#39;productnotcarried&#39; in product_text.lower():
                self.logger.info(str.encode(f&#39;Product Name: {product_name}, Product ID: {prod_id} extraction failed.\
                                        Product may not be available for sell currently.(Page: {product_page})&#39;,
                                            &#39;utf-8&#39;, &#39;ignore&#39;))
                self.meta.loc[prod, &#39;scraped&#39;] = &#39;NA&#39;
                self.meta.to_csv(
                    self.path/&#39;sph_detail_review_image_progress_tracker.csv&#39;, index=None)
                drv.quit()
                continue
        except Exception as ex:
            log_exception(
                self.logger, additional_information=f&#39;Prod ID: {prod_id}&#39;)

        # check product page is valid and exists
        try:
            close_popups(drv)
            accept_alert(drv, 2)
            price = drv.find_element_by_class_name(&#39;css-1865ad6&#39;)
            self.scroll_to_element(drv, price)
            price = price.text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            drv.quit()
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) no longer exists in the previously fetched link.\
                    (link:{product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            self.meta.loc[prod, &#39;detail_scraped&#39;] = &#39;NA&#39;
            continue

        try:
            chat_popup_button = WebDriverWait(drv, 3).until(
                EC.presence_of_element_located((By.XPATH, &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)))
            chat_popup_button = drv.find_element_by_xpath(
                &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)
            self.scroll_to_element(drv, chat_popup_button)
            ActionChains(drv).move_to_element(
                chat_popup_button).click(chat_popup_button).perform()
        except TimeoutException:
            pass

        detail, item = self.get_details(drv, prod_id, product_name)

        detail_data.append(detail)
        item_df = pd.concat(
            [item_df, item], axis=0)

        # item_data.append(product_attributes)
        self.logger.info(str.encode(
            f&#39;product: {product_name} (prod_id: {prod_id}) details extracted successfully&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        try:
            close_popups(drv)
            accept_alert(drv, 1)
            no_of_reviews = int(drv.find_element_by_class_name(
                &#39;css-ils4e4&#39;).text.split()[0])
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            self.logger.info(str.encode(f&#39;Product: {product_name} prod_id: {prod_id} reviews extraction failed.\
                                          Either product has no reviews or not\
                                          available for sell currently.(page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            no_of_reviews = 0
            # self.meta.loc[prod, &#39;review_scraped&#39;] = &#34;NA&#34;
            # self.meta.to_csv(
            #     self.review_path/&#39;sph_review_progress_tracker.csv&#39;, index=None)
            # drv.quit()
            # # print(&#39;in except - continue&#39;)
            # continue

        if no_of_reviews &gt; 0:
            reviews = self.get_reviews(
                drv, prod_id, product_name, last_scraped_review_date, no_of_reviews, incremental)
            if len(reviews) &gt; 0:
                review_data.extend(reviews)

        if not incremental:
            self.logger.info(str.encode(
                f&#39;Product_name: {product_name} prod_id:{prod_id} reviews extracted successfully.(total_reviews: {no_of_reviews}, \
                extracted_reviews: {len(reviews)}, page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
        else:
            self.logger.info(str.encode(
                f&#39;Product_name: {product_name} prod_id:{prod_id} new reviews extracted successfully.\
                    (no_of_new_extracted_reviews: {len(reviews)},\
                     page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        if prod != 0 and prod % 5 == 0:
            detail_data, item_df, review_data = store_data_refresh_mem(
                detail_data, item_df, review_data)

        self.meta.loc[prod, &#39;scraped&#39;] = &#39;Y&#39;
        drv.quit()

    detail_data, item_df, review_data = store_data_refresh_mem(
        detail_data, item_df, review_data)
    self.logger.info(
        f&#39;Extraction Complete for start_idx: (indices[0]) to end_idx: {indices[-1]}. Or for list of values.&#39;)</code></pre>
</details>
</dd>
<dt id="meiyume.sph.crawler.DetailReview.extract"><code class="name flex">
<span>def <span class="ident">extract</span></span>(<span>self, metadata: Union[pandas.core.frame.DataFrame, str, pathlib.Path], download: bool = True, n_workers: int = 5, fresh_start: bool = False, auto_fresh_start: bool = False, incremental: bool = True, open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = True, start_idx: Union[int, NoneType] = None, end_idx: Union[int, NoneType] = None, list_of_index=None, compile_progress_files: bool = False, clean: bool = True, delete_progress: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>extract [summary]</p>
<p>[extended_summary]</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>metadata</code></strong> :&ensp;<code>Union[pd.DataFrame, str, Path]</code></dt>
<dd>[description]</dd>
<dt><strong><code>download</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>[description]. Defaults to True.</dd>
<dt><strong><code>n_workers</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>[description]. Defaults to 5.</dd>
<dt><strong><code>fresh_start</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>[description]. Defaults to False.</dd>
<dt><strong><code>auto_fresh_start</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>[description]. Defaults to False.</dd>
<dt><strong><code>incremental</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>[description]. Defaults to True.</dd>
<dt><strong><code>open_headless</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>[description]. Defaults to False.</dd>
<dt><strong><code>open_with_proxy_server</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>[description]. Defaults to True.</dd>
<dt><strong><code>randomize_proxy_usage</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>[description]. Defaults to True.</dd>
<dt><strong><code>start_idx</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>[description]. Defaults to None.</dd>
<dt><strong><code>end_idx</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>[description]. Defaults to None.</dd>
<dt><strong><code>list_of_index</code></strong> :&ensp;<code>[type]</code>, optional</dt>
<dd>[description]. Defaults to None.</dd>
<dt><strong><code>compile_progress_files</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>[description]. Defaults to False.</dd>
<dt><strong><code>clean</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>[description]. Defaults to True.</dd>
<dt><strong><code>delete_progress</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>[description]. Defaults to False.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract(self, metadata: Union[pd.DataFrame, str, Path], download: bool = True, n_workers: int = 5,
            fresh_start: bool = False, auto_fresh_start: bool = False, incremental: bool = True,
            open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = True,
            start_idx: Optional[int] = None, end_idx: Optional[int] = None, list_of_index=None,
            compile_progress_files: bool = False, clean: bool = True, delete_progress: bool = False):
    &#34;&#34;&#34;extract [summary]

    [extended_summary]

    Args:
        metadata (Union[pd.DataFrame, str, Path]): [description]
        download (bool, optional): [description]. Defaults to True.
        n_workers (int, optional): [description]. Defaults to 5.
        fresh_start (bool, optional): [description]. Defaults to False.
        auto_fresh_start (bool, optional): [description]. Defaults to False.
        incremental (bool, optional): [description]. Defaults to True.
        open_headless (bool, optional): [description]. Defaults to False.
        open_with_proxy_server (bool, optional): [description]. Defaults to True.
        randomize_proxy_usage (bool, optional): [description]. Defaults to True.
        start_idx (Optional[int], optional): [description]. Defaults to None.
        end_idx (Optional[int], optional): [description]. Defaults to None.
        list_of_index ([type], optional): [description]. Defaults to None.
        compile_progress_files (bool, optional): [description]. Defaults to False.
        clean (bool, optional): [description]. Defaults to True.
        delete_progress (bool, optional): [description]. Defaults to False.
    &#34;&#34;&#34;
    def fresh():
        if not isinstance(metadata, pd.core.frame.DataFrame):
            list_of_files = self.metadata_clean_path.glob(
                &#39;no_cat_cleaned_sph_product_metadata_all*&#39;)
            self.meta = pd.read_feather(max(list_of_files, key=os.path.getctime))[
                [&#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;, &#39;last_scraped_review_date&#39;]]
        else:
            self.meta = metadata[[
                &#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;, &#39;last_scraped_review_date&#39;]]
        self.meta.last_scraped_review_date.fillna(&#39;&#39;, inplace=True)
        self.meta[&#39;scraped&#39;] = &#39;N&#39;

    if download:
        if fresh_start:
            fresh()
            self.logger.info(
                &#39;Starting Fresh Deatil Review Image Extraction.&#39;)
        else:
            if Path(self.path/&#39;sph_detail_review_image_progress_tracker.csv&#39;).exists():
                self.meta = pd.read_csv(
                    self.path/&#39;sph_detail_review_image_progress_tracker.csv&#39;)
                if sum(self.meta.scraped == &#39;N&#39;) == 0:
                    if auto_fresh_start:
                        fresh()
                        self.logger.info(
                            &#39;Last Run was Completed. Starting Fresh Extraction.&#39;)
                    else:
                        self.logger.info(
                            &#39;Deatil Review Image extraction for this cycle is complete.&#39;)
                else:
                    self.logger.info(
                        &#39;Continuing Deatil Review Image Extraction From Last Run.&#39;)
            else:
                fresh()
                self.logger.info(
                    &#39;Deatil Review Image Progress Tracker does not exist. Starting Fresh Extraction.&#39;)

        # set list or range of product indices to crawl
        if list_of_index:
            indices = list_of_index
        elif start_idx and end_idx is None:
            indices = range(start_idx, len(self.meta))
        elif start_idx is None and end_idx:
            indices = range(0, end_idx)
        elif start_idx is not None and end_idx is not None:
            indices = range(start_idx, end_idx)
        else:
            indices = range(len(self.meta))
        print(indices)

        if list_of_index:
            self.crawl_page(indices=list_of_index, incremental=incremental,
                            open_headless=open_headless,
                            open_with_proxy_server=open_with_proxy_server,
                            randomize_proxy_usage=randomize_proxy_usage)
        else:  # By default the code will with 5 concurrent threads. you can change this behaviour by changing n_workers
            if start_idx:
                lst_of_lst = ranges(
                    indices[-1]+1, n_workers, start_idx=start_idx)
            else:
                lst_of_lst = ranges(len(indices), n_workers)
            print(lst_of_lst)
            # detail_Data and item_data are lists of empty lists so that each namepace of function call will have its separate detail_data
            # list to strore scraped dictionaries. will save memory(ram/hard-disk) consumption. will stop data duplication
            headless = [open_headless for i in lst_of_lst]
            proxy = [open_with_proxy_server for i in lst_of_lst]
            rand_proxy = [randomize_proxy_usage for i in lst_of_lst]
            detail_data = [[] for i in lst_of_lst]  # type: List
            # item_data=[[] for i in lst_of_lst]  # type: List
            item_df = [pd.DataFrame(columns=[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;item_name&#39;,
                                             &#39;item_size&#39;, &#39;item_price&#39;, &#39;item_ingredients&#39;])
                       for i in lst_of_lst]
            review_data = [[] for i in lst_of_lst]  # type: list
            inc_list = [incremental for i in lst_of_lst]  # type: list
            with concurrent.futures.ThreadPoolExecutor() as executor:
                # but each of the function namespace will be modifying only one metadata tracing file so that progress saving
                # is tracked correctly. else multiple progress tracker file will be created with difficulty to combine correct
                # progress information
                print(&#39;inside executor&#39;)
                executor.map(self.crawl_page, lst_of_lst,
                             headless, proxy, rand_proxy,
                             detail_data, item_df, review_data,
                             inc_list)
    try:
        if compile_progress_files:
            self.logger.info(&#39;Creating Combined Detail and Item File&#39;)
            if datetime.now().day &lt; 15:
                meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
            else:
                meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;

            det_li = []
            self.bad_det_li = []
            detail_files = [f for f in self.detail_current_progress_path.glob(
                &#34;sph_prod_detail_extract_progress_*&#34;)]
            for file in detail_files:
                try:
                    df = pd.read_csv(file)
                except Exception:
                    self.bad_det_li.append(file)
                else:
                    det_li.append(df)

            detail_df = pd.concat(det_li, axis=0, ignore_index=True)
            detail_df.drop_duplicates(inplace=True)
            detail_df.reset_index(inplace=True, drop=True)
            detail_df[&#39;meta_date&#39;] = meta_date
            detail_filename = f&#39;sph_product_detail_all_{meta_date}.csv&#39;
            detail_df.to_csv(self.detail_path/detail_filename, index=None)
            # detail_df.to_feather(self.detail_path/detail_filename)

            item_li = []
            self.bad_item_li = []
            item_files = [f for f in self.current_progress_path.glob(
                &#34;sph_prod_item_extract_progress_*&#34;)]
            for file in item_files:
                try:
                    idf = pd.read_csv(file)
                except Exception:
                    self.bad_item_li.append(file)
                else:
                    item_li.append(idf)

            item_dataframe = pd.concat(item_li, axis=0, ignore_index=True)
            item_dataframe.drop_duplicates(inplace=True)
            item_dataframe.reset_index(inplace=True, drop=True)
            item_dataframe[&#39;meta_date&#39;] = meta_date
            item_filename = f&#39;sph_product_item_all_{meta_date}.csv&#39;
            item_dataframe.to_csv(
                self.detail_path/item_filename, index=None)
            # item_df.to_feather(self.detail_path/item_filename)

            self.logger.info(
                f&#39;Detail and Item files created. Please look for file sph_product_detail_all and\
                    sph_product_item_all in path {self.detail_path}&#39;)
            print(
                f&#39;Detail and Item files created. Please look for file sph_product_detail_all and\
                    sph_product_item_all in path {self.detail_path}&#39;)

            self.logger.info(&#39;Creating Combined Review File&#39;)
            if datetime.now().day &lt; 15:
                meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
            else:
                meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;
            rev_li = []
            self.bad_rev_li = []
            review_files = [f for f in self.current_progress_path.glob(
                &#34;sph_prod_review_extract_progress_*&#34;)]
            for file in review_files:
                try:
                    df = pd.read_csv(file)
                except Exception:
                    self.bad_rev_li.append(file)
                else:
                    rev_li.append(df)
            rev_df = pd.concat(rev_li, axis=0, ignore_index=True)
            rev_df.drop_duplicates(inplace=True)
            rev_df.reset_index(inplace=True, drop=True)
            rev_df[&#39;meta_date&#39;] = pd.to_datetime(meta_date).date()
            review_filename = f&#39;sph_product_review_all_{pd.to_datetime(meta_date).date()}&#39;
            # , index=None)
            rev_df.to_feather(self.review_path/review_filename)

            self.logger.info(
                f&#39;Review file created. Please look for file sph_product_review_all in path {self.review_path}&#39;)
            print(
                f&#39;Review file created. Please look for file sph_product_review_all in path {self.review_path}&#39;)

            if clean:
                detail_cleaner = Cleaner(path=self.path)
                self.detail_clean_df = detail_cleaner.clean(
                    self.detail_path/detail_filename)
                item_cleaner = Cleaner(path=self.path)
                self.item_clean_df, self.ing_clean_df = item_cleaner.clean(
                    self.detail_path/item_filename)

                review_cleaner = Cleaner(path=self.path)
                self.review_clean_df = review_cleaner.clean(
                    self.review_path/review_filename)

                file_creation_status = True
        else:
            file_creation_status = False
    except Exception as ex:
        log_exception(
            self.logger, additional_information=f&#39;Detail Item Review Combined File Creation Failed.&#39;)
        file_creation_status = False

    if delete_progress and file_creation_status:
        shutil.rmtree(
            f&#39;{self.detail_path}\\current_progress&#39;, ignore_errors=True)
        shutil.rmtree(
            f&#39;{self.review_path}\\current_progress&#39;, ignore_errors=True)
        self.logger.info(&#39;Progress files deleted&#39;)</code></pre>
</details>
</dd>
<dt id="meiyume.sph.crawler.DetailReview.get_details"><code class="name flex">
<span>def <span class="ident">get_details</span></span>(<span>self, drv: selenium.webdriver.firefox.webdriver.WebDriver, prod_id: str, product_name: str) ‑> Tuple[dict, pandas.core.frame.DataFrame]</span>
</code></dt>
<dd>
<div class="desc"><p>get_detail [summary]</p>
<p>[extended_summary]</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>drv</code></strong> :&ensp;<code>webdriver.Firefox</code></dt>
<dd>[description]</dd>
<dt><strong><code>prod_id</code></strong> :&ensp;<code>str</code></dt>
<dd>[description]</dd>
<dt><strong><code>product_name</code></strong> :&ensp;<code>str</code></dt>
<dd>[description]</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[dict, pd.DataFrame]</code></dt>
<dd>[description]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_details(self, drv: webdriver.Firefox, prod_id: str, product_name: str) -&gt; Tuple[dict, pd.DataFrame]:
    &#34;&#34;&#34;get_detail [summary]

    [extended_summary]

    Args:
        drv (webdriver.Firefox): [description]
        prod_id (str): [description]
        product_name (str): [description]

    Returns:
        Tuple[dict, pd.DataFrame]: [description]
    &#34;&#34;&#34;

    def get_item_attributes(drv: webdriver.Firefox, product_name: str, prod_id: str, use_button: bool = False,
                            multi_variety: bool = False, typ=None, ) -&gt; Tuple[str, str, str, str]:
        &#34;&#34;&#34;get_item_attributes [summary]

        [extended_summary]

        Args:
            drv (webdriver.Chrome): [description]
            product_name (str): [description]
            prod_id (str): [description]
            use_button (bool, optional): [description]. Defaults to False.
            multi_variety (bool, optional): [description]. Defaults to False.
            typ ([type], optional): [description]. Defaults to None.

        Returns:
            Tuple[str, str, str, str]: [description]
        &#34;&#34;&#34;
        # drv # type: webdriver.Chrome
        # close popup windows
        close_popups(drv)
        accept_alert(drv, 1)

        try:
            item_price = drv.find_element_by_class_name(&#39;css-1865ad6&#39;).text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            item_price = &#39;&#39;
        # print(item_price)

        if multi_variety:
            try:
                if use_button:
                    item_name = typ.find_element_by_tag_name(
                        &#39;button&#39;).get_attribute(&#39;aria-label&#39;)
                else:
                    item_name = typ.get_attribute(&#39;aria-label&#39;)
                # print(item_name)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                item_name = &#34;&#34;
                self.logger.info(str.encode(
                    f&#39;product: {product_name} (prod_id: {prod_id}) item_name does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
        else:
            item_name = &#34;&#34;

        try:
            item_size = drv.find_element_by_class_name(&#39;css-128n72s&#39;).text
            # print(item_size)
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            item_size = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) item_size does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

        # get all tabs
        first_tab = drv.find_element_by_id(f&#39;tab{0}&#39;)
        self.scroll_to_element(drv, first_tab)
        ActionChains(drv).move_to_element(
            first_tab).click(first_tab).perform()
        prod_tabs = []
        prod_tabs = drv.find_elements_by_class_name(&#39;css-1wugx5m&#39;)
        prod_tabs.extend(drv.find_elements_by_class_name(&#39;css-12vae0p&#39;))

        tab_names = []
        for t in prod_tabs:
            tab_names.append(t.text.lower())
        # print(tab_names)

        if &#39;ingredients&#39; in tab_names:
            close_popups(drv)
            accept_alert(drv, 1)
            if len(tab_names) == 5:
                try:
                    tab_num = 2
                    ing_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                    self.scroll_to_element(drv, ing_button)
                    ActionChains(drv).move_to_element(
                        ing_button).click(ing_button).perform()
                    item_ing = drv.find_element_by_xpath(
                        f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    print(&#39;cant get ingredient but tab exists&#39;)
                    item_ing = &#34;&#34;
                    self.logger.info(str.encode(
                        f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients extraction failed&#39;,
                        &#39;utf-8&#39;, &#39;ignore&#39;))
            elif len(tab_names) == 4:
                try:
                    tab_num = 1
                    ing_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                    self.scroll_to_element(drv, ing_button)
                    ActionChains(drv).move_to_element(
                        ing_button).click(ing_button).perform()
                    item_ing = drv.find_element_by_xpath(
                        f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    print(&#39;cant get ingredient but tab exists&#39;)
                    item_ing = &#34;&#34;
                    self.logger.info(str.encode(
                        f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients extraction failed.&#39;,
                        &#39;utf-8&#39;, &#39;ignore&#39;))
            elif len(tab_names) &lt; 4:
                try:
                    tab_num = 0
                    ing_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
                    self.scroll_to_element(drv, ing_button)
                    ActionChains(drv).move_to_element(
                        ing_button).click(ing_button).perform()
                    item_ing = drv.find_element_by_xpath(
                        f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    print(&#39;cant get ingredient but tab exists&#39;)
                    item_ing = &#34;&#34;
                    self.logger.info(str.encode(
                        f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients extraction failed.&#39;,
                        &#39;utf-8&#39;, &#39;ignore&#39;))
        else:
            item_ing = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) item_ingredients does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
        # print(item_ing)
        return item_name, item_size, item_price, item_ing

    def get_product_attributes(drv: webdriver.Firefox, product_name: str, prod_id: str) -&gt; list:
        &#34;&#34;&#34;get_product_attributes [summary]

        [extended_summary]

        Args:
            drv (webdriver.Chrome): [description]
            product_name (str): [description]
            prod_id (str): [description]

        Returns:
            list: [description]
        &#34;&#34;&#34;
        # get all the variation of product
        # close popup windows
        close_popups(drv)
        accept_alert(drv, 1)

        product_variety = []
        try:
            product_variety = drv.find_elements_by_class_name(
                &#39;css-1j1jwa4&#39;)
            product_variety.extend(
                drv.find_elements_by_class_name(&#39;css-cl742e&#39;))
            use_button = False
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
        try:
            if len(product_variety) &lt; 1:
                product_variety = drv.find_elements_by_class_name(
                    &#39;css-5jqxch&#39;)
                use_button = True
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)

        product_attributes = []

        if len(product_variety) &gt; 0:
            for typ in product_variety:
                close_popups(drv)
                accept_alert(drv, 1)
                try:
                    self.scroll_to_element(drv, typ)
                    ActionChains(drv).move_to_element(
                        typ).click(typ).perform()
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                time.sleep(4)  # 8
                item_name, item_size, item_price, item_ingredients = get_item_attributes(drv, product_name, prod_id,
                                                                                         multi_variety=True, typ=typ,
                                                                                         use_button=use_button)
                product_attributes.append({&#34;prod_id&#34;: prod_id, &#34;product_name&#34;: product_name,
                                           &#34;item_name&#34;: item_name, &#34;item_size&#34;: item_size,
                                           &#34;item_price&#34;: item_price, &#34;item_ingredients&#34;: item_ingredients})
        else:
            item_name, item_size, item_price, item_ingredients = get_item_attributes(drv,
                                                                                     product_name, prod_id)
            product_attributes.append({&#34;prod_id&#34;: prod_id, &#34;product_name&#34;: product_name, &#34;item_name&#34;: item_name,
                                       &#34;item_size&#34;: item_size, &#34;item_price&#34;: item_price, &#34;item_ingredients&#34;: item_ingredients})

        return product_attributes

    def get_first_review_date(drv: webdriver.Firefox) -&gt; str:
        &#34;&#34;&#34;get_first_review_date [summary]

        [extended_summary]

        Args:
            drv (webdriver.Chrome): [description]

        Returns:
            str: [description]
        &#34;&#34;&#34;
        # close popup windows
        close_popups(drv)
        accept_alert(drv, 1)

        try:
            review_sort_trigger = drv.find_element_by_id(
                &#39;review_filter_sort_trigger&#39;)
            self.scroll_to_element(drv, review_sort_trigger)
            ActionChains(drv).move_to_element(
                review_sort_trigger).click(review_sort_trigger).perform()
            for btn in drv.find_elements_by_class_name(&#39;css-rfz1gg&#39;):
                if btn.text.lower() == &#39;oldest&#39;:
                    ActionChains(drv).move_to_element(
                        btn).click(btn).perform()
                    break
            time.sleep(6)
            close_popups(drv)
            accept_alert(drv, 1)
            rev = drv.find_elements_by_class_name(&#39;css-1kk8dps&#39;)[2:]
            try:
                first_review_date = convert_ago_to_date(
                    rev[0].find_element_by_class_name(&#39;css-h2vfi1&#39;).text)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                try:
                    first_review_date = convert_ago_to_date(
                        rev[1].find_element_by_class_name(&#39;css-h2vfi1&#39;).text)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    print(&#39;sorted but cant get first review date value&#39;)
                    first_review_date = &#39;&#39;
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            first_review_date = &#39;&#39;
        return first_review_date

    # get all product info tabs such as how-to-use, about-brand, ingredients
    prod_tabs = []
    prod_tabs = drv.find_elements_by_class_name(&#39;css-1wugx5m&#39;)
    prod_tabs.extend(drv.find_elements_by_class_name(&#39;css-12vae0p&#39;))

    tab_names = []
    for t in prod_tabs:
        tab_names.append(t.text.lower())

    # no. of votes
    try:
        votes = drv.find_elements_by_class_name(&#39;css-2rg6q7&#39;)[-1].text
        # print(votes)
    except Exception as ex:
        log_exception(self.logger,
                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
        votes = &#34;&#34;
        self.logger.info(str.encode(
            f&#39;product: {product_name} (prod_id: {prod_id}) votes does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

    # product details
    if &#39;details&#39; in tab_names:
        try:
            close_popups(drv)
            accept_alert(drv, 1)
            tab_num = tab_names.index(&#39;details&#39;)
            detail_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
            try:
                time.sleep(1)
                self.scroll_to_element(drv, detail_button)
                ActionChains(drv).move_to_element(
                    detail_button).click(detail_button).perform()
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                details = &#34;&#34;
            else:
                try:
                    details = drv.find_element_by_xpath(
                        f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    details = &#34;&#34;
                    self.logger.info(str.encode(
                        f&#39;product: {product_name} (prod_id: {prod_id}) product detail text\
                                does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                # print(details)
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            details = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) product detail extraction failed&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
    else:
        details = &#34;&#34;
        self.logger.info(str.encode(
            f&#39;product: {product_name} (prod_id: {prod_id}) product detail does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

    # how to use
    if &#39;how to use&#39; in tab_names:
        try:
            close_popups(drv)
            accept_alert(drv, 1)

            tab_num = tab_names.index(&#39;how to use&#39;)
            how_to_use_button = drv.find_element_by_id(f&#39;tab{tab_num}&#39;)
            try:
                time.sleep(1)
                self.scroll_to_element(drv, how_to_use_button)
                ActionChains(drv).move_to_element(
                    how_to_use_button).click(how_to_use_button).perform()
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                how_to_use = &#34;&#34;
            else:
                try:
                    how_to_use = drv.find_element_by_xpath(
                        f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    how_to_use = &#34;&#34;
                    self.logger.info(str.encode(
                        f&#39;product: {product_name} (prod_id: {prod_id}) how_to_use text\
                                does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                # print(how_to_use)
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            how_to_use = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) how_to_use extraction failed&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
    else:
        how_to_use = &#34;&#34;
        self.logger.info(str.encode(
            f&#39;product: {product_name} (prod_id: {prod_id}) how_to_use does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

    # about the brand
    if &#39;about the brand&#39; in tab_names:
        try:
            close_popups(drv)
            accept_alert(drv, 1)

            tab_num = tab_names.index(&#39;about the brand&#39;)
            about_the_brand_button = drv.find_element_by_id(
                f&#39;tab{tab_num}&#39;)
            try:
                time.sleep(1)
                self.scroll_to_element(drv, about_the_brand_button)
                ActionChains(drv).move_to_element(
                    about_the_brand_button).click(about_the_brand_button).perform()
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                about_the_brand = &#34;&#34;
            else:
                try:
                    about_the_brand = drv.find_element_by_xpath(
                        f&#39;//*[@id=&#34;tabpanel{tab_num}&#34;]/div&#39;).text
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    about_the_brand = &#34;&#34;
                    self.logger.info(str.encode(
                        f&#39;product: {product_name} (prod_id: {prod_id}) about_the_brand text\
                            does not exist&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                # print(about_the_brand)
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            about_the_brand = &#34;&#34;
            self.logger.info(str.encode(
                f&#39;product: {product_name} (prod_id: {prod_id}) about_the_brand extraction failed&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
    else:
        about_the_brand = &#34;&#34;
        self.logger.info(str.encode(
            f&#39;product: {product_name} (prod_id: {prod_id}) about_the_brand does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

    self.scroll_down_page(drv, h2=0.4, speed=5)
    time.sleep(5)
    try:
        chat_popup_button = WebDriverWait(drv, 3).until(
            EC.presence_of_element_located((By.XPATH, &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)))
        chat_popup_button = drv.find_element_by_xpath(
            &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)
        self.scroll_to_element(drv, chat_popup_button)
        ActionChains(drv).move_to_element(
            chat_popup_button).click(chat_popup_button).perform()
    except TimeoutException:
        pass
    # click no. of reviews
    try:
        review_button = drv.find_element_by_class_name(&#39;css-1pjru6n&#39;)
        self.scroll_to_element(drv, review_button)
        ActionChains(drv).move_to_element(
            review_button).click(review_button).perform()
    except Exception as ex:
        log_exception(self.logger,
                      additional_information=f&#39;Prod ID: {prod_id}&#39;)

    try:
        first_review_date = get_first_review_date(drv)
        # print(first_review_date)
    except Exception as ex:
        log_exception(self.logger,
                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
        first_review_date = &#34;&#34;
        self.logger.info(str.encode(
            f&#39;product: {product_name} (prod_id: {prod_id}) first_review_date scrape failed.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

    try:
        close_popups(drv)
        accept_alert(drv, 1)
        reviews = int(drv.find_element_by_class_name(
            &#39;css-ils4e4&#39;).text.split()[0])
        # print(reviews)
    except Exception as ex:
        log_exception(self.logger,
                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
        reviews = 0
        self.logger.info(str.encode(
            f&#39;product: {product_name} (prod_id: {prod_id}) reviews does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

    try:
        close_popups(drv)
        accept_alert(drv, 1)
        rating_distribution = drv.find_element_by_class_name(
            &#39;css-960eb6&#39;).text.split(&#39;\n&#39;)
        # print(rating_distribution)
    except Exception as ex:
        log_exception(self.logger,
                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
        rating_distribution = &#34;&#34;
        self.logger.info(str.encode(
            f&#39;product: {product_name} (prod_id: {prod_id}) rating_distribution does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

    try:
        close_popups(drv)
        accept_alert(drv, 1)
        would_recommend = drv.find_element_by_class_name(
            &#39;css-k9ne19&#39;).text
        # print(would_recommend)
    except Exception as ex:
        log_exception(self.logger,
                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
        would_recommend = &#34;&#34;
        self.logger.info(str.encode(
            f&#39;product: {product_name} (prod_id: {prod_id}) would_recommend does not exist.&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

    detail = {&#39;prod_id&#39;: prod_id, &#39;product_name&#39;: product_name, &#39;abt_product&#39;: details,
              &#39;how_to_use&#39;: how_to_use, &#39;abt_brand&#39;: about_the_brand,
              &#39;reviews&#39;: reviews, &#39;votes&#39;: votes, &#39;rating_dist&#39;: rating_distribution,
              &#39;would_recommend&#39;: would_recommend, &#39;first_review_date&#39;: first_review_date}

    item = pd.DataFrame(
        get_product_attributes(drv, product_name, prod_id))

    return detail, item</code></pre>
</details>
</dd>
<dt id="meiyume.sph.crawler.DetailReview.get_reviews"><code class="name flex">
<span>def <span class="ident">get_reviews</span></span>(<span>self, drv: selenium.webdriver.firefox.webdriver.WebDriver, prod_id: str, product_name: str, last_scraped_review_date: str, no_of_reviews: int, incremental: bool = True, reviews: list = []) ‑> list</span>
</code></dt>
<dd>
<div class="desc"><p>get_reviews [summary]</p>
<p>[extended_summary]</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>drv</code></strong> :&ensp;<code>webdriver.Firefox</code></dt>
<dd>[description]</dd>
<dt><strong><code>prod_id</code></strong> :&ensp;<code>str</code></dt>
<dd>[description]</dd>
<dt><strong><code>product_name</code></strong> :&ensp;<code>str</code></dt>
<dd>[description]</dd>
<dt><strong><code>last_scraped_review_date</code></strong> :&ensp;<code>str</code></dt>
<dd>[description]</dd>
<dt><strong><code>no_of_reviews</code></strong> :&ensp;<code>int</code></dt>
<dd>[description]</dd>
<dt><strong><code>incremental</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>[description]. Defaults to True.</dd>
<dt><strong><code>reviews</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>[description]. Defaults to [].</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>[description]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_reviews(self,  drv: webdriver.Firefox, prod_id: str, product_name: str,
                last_scraped_review_date: str, no_of_reviews: int,
                incremental: bool = True, reviews: list = []) -&gt; list:
    &#34;&#34;&#34;get_reviews [summary]

    [extended_summary]

    Args:
        drv (webdriver.Firefox): [description]
        prod_id (str): [description]
        product_name (str): [description]
        last_scraped_review_date (str): [description]
        no_of_reviews (int): [description]
        incremental (bool, optional): [description]. Defaults to True.
        reviews (list, optional): [description]. Defaults to [].

    Returns:
        list: [description]
    &#34;&#34;&#34;
    # print(no_of_reviews)
    # drv.find_element_by_class_name(&#39;css-2rg6q7&#39;).click()
    if incremental and last_scraped_review_date != &#39;&#39;:
        for n in range(no_of_reviews//6):
            if n &gt; 400:
                break

            time.sleep(0.4)
            revs = drv.find_elements_by_class_name(
                &#39;css-1kk8dps&#39;)[2:]

            try:
                if pd.to_datetime(convert_ago_to_date(revs[-1].find_element_by_class_name(&#39;css-1t84k9w&#39;).text),
                                  infer_datetime_format=True)\
                        &lt; pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                    # print(&#39;breaking incremental&#39;)
                    break
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                try:
                    if pd.to_datetime(convert_ago_to_date(revs[-2].find_element_by_class_name(&#39;css-1t84k9w&#39;).text),
                                      infer_datetime_format=True)\
                            &lt; pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                        # print(&#39;breaking incremental&#39;)
                        break
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    self.logger.info(str.encode(f&#39;Product: {product_name} prod_id: {prod_id} \
                                                    last_scraped_review_date to current review date \
                                                    comparision failed.(page: {product_page})&#39;,
                                                &#39;utf-8&#39;, &#39;ignore&#39;))
                    # print(&#39;in second except block&#39;)
                    continue
            try:
                show_more_review_button = drv.find_element_by_class_name(
                    &#39;css-xswy5p&#39;)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to get show more review button.&#39;)
            else:
                try:
                    self.scroll_to_element(
                        drv, show_more_review_button)
                    ActionChains(drv).move_to_element(
                        show_more_review_button).click(show_more_review_button).perform()
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)
                    accept_alert(drv, 1)
                    close_popups(drv)
                    try:
                        self.scroll_to_element(
                            drv, show_more_review_button)
                        ActionChains(drv).move_to_element(
                            show_more_review_button).click(show_more_review_button).perform()
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)

    else:
        # print(&#39;inside get all reviews&#39;)
        # 6 because for click sephora shows 6 reviews. additional 25 no. of clicks for buffer.
        for n in range(no_of_reviews//6+10):
            &#39;&#39;&#39;
            code will stop after getting 1800 reviews of one particular product
            when crawling all reviews. By default it will get latest 1800 reviews.
            then in subsequent incremental runs it will get al new reviews on weekly basis
            &#39;&#39;&#39;
            if n &gt;= 400:  # 200:
                break
            time.sleep(1)
            # close any opened popups by escape
            accept_alert(drv, 1)
            close_popups(drv)
            try:
                show_more_review_button = drv.find_element_by_class_name(
                    &#39;css-xswy5p&#39;)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to get show more review button.&#39;)
            else:
                try:
                    self.scroll_to_element(
                        drv, show_more_review_button)
                    ActionChains(drv).move_to_element(
                        show_more_review_button).click(show_more_review_button).perform()
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)
                    accept_alert(drv, 1)
                    close_popups(drv)
                    try:
                        self.scroll_to_element(
                            drv, show_more_review_button)
                        ActionChains(drv).move_to_element(
                            show_more_review_button).click(show_more_review_button).perform()
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}.\
                                            Failed to click on show more review button.&#39;)
                        try:
                            self.scroll_to_element(
                                drv, show_more_review_button)
                            ActionChains(drv).move_to_element(
                                show_more_review_button).click(show_more_review_button).perform()
                        except Exception as ex:
                            log_exception(self.logger,
                                          additional_information=f&#39;Prod ID: {prod_id}.\
                                            Failed to click on show more review button.&#39;)
                            accept_alert(drv, 2)
                            close_popups(drv)
                            try:
                                self.scroll_to_element(
                                    drv, show_more_review_button)
                                ActionChains(drv).move_to_element(
                                    show_more_review_button).click(show_more_review_button).perform()
                            except Exception as ex:
                                log_exception(self.logger,
                                              additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)
                                if n &lt; (no_of_reviews//6):
                                    self.logger.info(str.encode(f&#39;Product: {product_name} - prod_id \
                                        {prod_id} breaking click next review loop.\
                                                                [total_reviews:{no_of_reviews} loaded_reviews:{n}]\
                                                                (page link: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                                    self.logger.info(str.encode(f&#39;Product: {product_name} - prod_id {prod_id} cant load all reviews.\
                                                                    Check click next 6 reviews\
                                                                    code section(page link: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                                break

    accept_alert(drv, 2)
    close_popups(drv)

    product_reviews = drv.find_elements_by_class_name(
        &#39;css-1kk8dps&#39;)[2:]

    # print(&#39;starting extraction&#39;)
    r = 0
    for rev in product_reviews:
        accept_alert(drv, 0.5)
        close_popups(drv)
        self.scroll_to_element(drv, rev)
        ActionChains(drv).move_to_element(rev).perform()

        try:
            try:
                review_text = rev.find_element_by_class_name(
                    &#39;css-1jg2pb9&#39;).text
            except NoSuchElementException:
                review_text = rev.find_element_by_class_name(
                    &#39;css-429528&#39;).text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_text. Skip review.&#39;)
            continue

        try:
            review_date = convert_ago_to_date(
                rev.find_element_by_class_name(&#39;css-h2vfi1&#39;).text)
            if pd.to_datetime(review_date, infer_datetime_format=True) &lt;= \
                    pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                continue
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_date.&#39;)
            review_date = &#39;&#39;

        try:
            review_title = rev.find_element_by_class_name(
                &#39;css-1jfmule&#39;).text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_title.&#39;)
            review_title = &#39;&#39;

        try:
            product_variant = rev.find_element_by_class_name(
                &#39;css-1op1cn7&#39;).text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}. Failed to extract product_variant.&#39;)
            product_variant = &#39;&#39;

        try:
            user_rating = rev.find_element_by_class_name(
                &#39;css-3z5ot7&#39;).get_attribute(&#39;aria-label&#39;)
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}. Failed to extract user_rating.&#39;)
            user_rating = &#39;&#39;

        try:
            user_attribute = [{&#39;_&#39;.join(u.lower().split()[0:-1]): u.lower().split()[-1]}
                              for u in rev.find_element_by_class_name(&#39;css-ecreye&#39;).text.split(&#39;\n&#39;)]
            # user_attribute = []
            # for u in rev.find_elements_by_class_name(&#39;css-j5yt83&#39;):
            #     user_attribute.append(
            #         {&#39;_&#39;.join(u.text.lower().split()[0:-1]): u.text.lower().split()[-1]})
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}. Failed to extract user_attribute.&#39;)
            user_attribute = []

        try:
            recommend = rev.find_element_by_class_name(
                &#39;css-1tf5yph&#39;).text
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}. Failed to extract recommend.&#39;)
            recommend = &#39;&#39;

        try:
            helpful = rev.find_element_by_class_name(&#39;css-b7zg5r&#39;).text
            # helpful = []
            # for h in rev.find_elements_by_class_name(&#39;css-39esqn&#39;):
            #     helpful.append(h.text)
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}. Failed to extract helpful.&#39;)
            helpful = &#39;&#39;

        reviews.append({&#39;prod_id&#39;: prod_id, &#39;product_name&#39;: product_name,
                        &#39;user_attribute&#39;: user_attribute, &#39;product_variant&#39;: product_variant,
                        &#39;review_title&#39;: review_title, &#39;review_text&#39;: review_text,
                        &#39;review_rating&#39;: user_rating, &#39;recommend&#39;: recommend,
                        &#39;review_date&#39;: review_date,   &#39;helpful&#39;: helpful})
    return reviews</code></pre>
</details>
</dd>
<dt id="meiyume.sph.crawler.DetailReview.terminate_logging"><code class="name flex">
<span>def <span class="ident">terminate_logging</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>terminate_logging.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def terminate_logging(self):
    &#34;&#34;&#34;terminate_logging.&#34;&#34;&#34;
    self.logger.handlers.clear()
    self.prod_detail_review_image_log.stop_log()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="meiyume.utils.Sephora" href="../utils.html#meiyume.utils.Sephora">Sephora</a></b></code>:
<ul class="hlist">
<li><code><a title="meiyume.utils.Sephora.open_browser" href="../utils.html#meiyume.utils.Browser.open_browser">open_browser</a></code></li>
<li><code><a title="meiyume.utils.Sephora.open_browser_firefox" href="../utils.html#meiyume.utils.Browser.open_browser_firefox">open_browser_firefox</a></code></li>
<li><code><a title="meiyume.utils.Sephora.scroll_down_page" href="../utils.html#meiyume.utils.Browser.scroll_down_page">scroll_down_page</a></code></li>
<li><code><a title="meiyume.utils.Sephora.scroll_to_element" href="../utils.html#meiyume.utils.Browser.scroll_to_element">scroll_to_element</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="meiyume.sph.crawler.Image"><code class="flex name class">
<span>class <span class="ident">Image</span></span>
<span>(</span><span>path: pathlib.Path = WindowsPath('D:/Amit/Meiyume/meiyume_master_source_codes'), log: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Image [summary]</p>
<p>[extended_summary]</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>Sephora</code></strong> :&ensp;<code>[type]</code></dt>
<dd>[description]</dd>
</dl>
<p><strong>init</strong> [summary]</p>
<p>[extended_summary]</p>
<h2 id="args_1">Args</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>Path</code>, optional</dt>
<dd>[description]. Defaults to Path.cwd().</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>[description]. Defaults to True.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Image(Sephora):

    &#34;&#34;&#34;Image [summary]

    [extended_summary]

    Args:
        Sephora ([type]): [description]

    &#34;&#34;&#34;

    def __init__(self, path: Path = Path.cwd(), log: bool = True):
        &#34;&#34;&#34;__init__ [summary]

        [extended_summary]

        Args:
            path (Path, optional): [description]. Defaults to Path.cwd().
            log (bool, optional): [description]. Defaults to True.
        &#34;&#34;&#34;
        super().__init__(path=path, data_def=&#39;image&#39;)

        if log:
            self.prod_image_log = Logger(
                &#34;sph_prod_image_extraction&#34;, path=self.crawl_log_path)
            self.logger, _ = self.prod_image_log.start_log()

    def get_images(self, indices: Union[list, range], open_headless: bool,
                   open_with_proxy_server: bool,
                   randomize_proxy_usage: bool) -&gt; None:
        &#34;&#34;&#34;get_images scrapes individual product images.

        Get Images download up to four images for on product.

        Args:
            indices (Union[list, range]): list of indices or range of indices of product urls to scrape.
            open_headless (bool): Whether to open browser headless.
            open_with_proxy_server (bool): Whether to use ip rotation service.
            randomize_proxy_usage (bool): Whether to use both proxy and native network in tandem to decrease proxy requests.
        &#34;&#34;&#34;

        for prod in self.meta.index[self.meta.index.isin(indices)]:
            if self.meta.loc[prod, &#39;image_scraped&#39;] in [&#39;Y&#39;, &#39;NA&#39;]:
                continue
            prod_id = self.meta.loc[prod, &#39;prod_id&#39;]
            product_page = self.meta.loc[prod, &#39;product_page&#39;]

            # create webdriver
            if randomize_proxy_usage:
                use_proxy = np.random.choice([True, False])
            else:
                use_proxy = True
            if open_with_proxy_server:
                # print(use_proxy)
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
                                        open_for_screenshot=True, path=self.image_path)
            else:
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
                                        open_for_screenshot=True, path=self.image_path)
            # open product page
            drv.get(product_page)
            time.sleep(15)  # 30
            accept_alert(drv, 10)
            close_popups(drv)

            try:
                product_text = drv.find_element_by_class_name(
                    &#39;css-1wag3se&#39;).text
                if &#39;productnotcarried&#39; in product_text.lower():
                    self.logger.info(str.encode(f&#39;prod_id: {prod_id} image extraction failed.\
                                            Product may not be available for sell currently.(page: {product_page})&#39;,
                                                &#39;utf-8&#39;, &#39;ignore&#39;))
                    self.meta.loc[prod, &#39;image_scraped&#39;] = &#39;NA&#39;
                    self.meta.to_csv(
                        self.image_path/&#39;sph_image_progress_tracker.csv&#39;, index=None)
                    drv.quit()
                    continue
            except Exception as ex:
                log_exception(
                    self.logger, additional_information=f&#39;Prod ID: {prod_id}&#39;)

            # get image elements
            try:
                accept_alert(drv, 1)
                close_popups(drv)
                images = drv.find_elements_by_class_name(&#39;css-11rgy2w&#39;)
                if len(images) == 0:
                    try:
                        accept_alert(drv, 1)
                        close_popups(drv)
                        images = drv.find_elements_by_class_name(&#39;css-11rgy2w&#39;)
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        self.logger.info(str.encode(f&#39;prod_id: {prod_id} failed to get image sources.\
                                                    (page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                        self.meta.loc[prod, &#39;image_scraped&#39;] = &#39;NA&#39;
                        self.meta.to_csv(
                            self.image_path/&#39;sph_image_progress_tracker.csv&#39;, index=None)
                        drv.quit()
                        continue
            except Exception:
                continue
            else:
                if len(images) == 0:
                    self.logger.info(str.encode(f&#39;{prod_id} image extraction failed.\
                                                    (page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    self.meta.loc[prod, &#39;image_scraped&#39;] = &#39;NA&#39;
                    self.meta.to_csv(
                        self.image_path/&#39;sph_image_progress_tracker.csv&#39;, index=None)
                    drv.quit()
                    continue
            # get image urls
            sources = [i.find_element_by_tag_name(
                &#39;img&#39;).get_attribute(&#39;src&#39;) for i in images]
            sources = [i.split(&#39;?&#39;)[0] for i in sources]

            if len(sources) &gt; 4:
                sources = sources[:3]

            self.current_image_path = self.image_path/prod_id
            if not self.current_image_path.exists():
                self.current_image_path.mkdir(parents=True, exist_ok=True)

            # download images
            image_count = 0
            try:
                for src in sources:
                    drv.get(src)
                    time.sleep(2)
                    accept_alert(drv, 1)
                    close_popups(drv)
                    image_count += 1
                    image_name = f&#39;{prod_id}_image_{image_count}.jpg&#39;
                    drv.save_screenshot(
                        str(self.current_image_path/image_name))
                self.meta.loc[prod, &#39;image_scraped&#39;] = &#39;Y&#39;
                drv.quit()
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                if image_count &lt;= 1:
                    self.logger.info(str.encode(f&#39;prod_id: {prod_id} image extraction failed.\
                                                    (page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    self.meta.loc[prod, &#39;image_scraped&#39;] = &#39;NA&#39;
                    self.meta.to_csv(
                        self.image_path/&#39;sph_image_progress_tracker.csv&#39;, index=None)
                drv.quit()
                continue

            if prod % 10 == 0 and prod != 0:
                self.meta.to_csv(
                    self.image_path/&#39;sph_image_progress_tracker.csv&#39;, index=None)
        self.meta.to_csv(
            self.image_path/&#39;sph_image_progress_tracker.csv&#39;, index=None)

    def extract(self, metadata: Union[pd.DataFrame, str, Path], download: bool = True,
                n_workers: int = 5, fresh_start: bool = False, auto_fresh_start: bool = False,
                open_headless: bool = False, open_with_proxy_server: bool = True,
                randomize_proxy_usage: bool = True,
                start_idx: Optional[int] = None, end_idx: Optional[int] = None, list_of_index=None,
                ):
        &#34;&#34;&#34;extract method controls all properties of the spiders and runs multi-threaded web crawling.

        Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
        This method has four major functionality:
        * 1. Run the spider
        * 2. Store data in regular intervals to free up ram
        * 3. Compile all crawled data into one file.
        * 4. Clean and push cleaned data to S3 storage for further algorithmic processing.

        Args:
            metadata (pd.DataFrame): Dataframe containing product specific url, name and id of the products to be scraped.
            download (bool, optional): Whether to crawl data from or compile crawled data into one file. Defaults to True.
            n_workers (int, optional): No. of parallel threads to run. Defaults to 5.
            fresh_start (bool, optional): Whether to continue last crawl job or start new one. Defaults to False.
            auto_fresh_start (bool, optional): Whether to automatically start a new crawl job if last job was finished. Defaults to False.
            open_headless (bool, optional):  Whether to open browser headless. Defaults to False.
            open_with_proxy_server (bool, optional): Whether to use ip rotation service. Defaults to True.
            randomize_proxy_usage (bool, optional): Whether to use both proxy and native network in tandem to decrease proxy requests.
                                                    Defaults to False.
            start_idx (Optional[int], optional): Starting index of the links to crawl. Defaults to None.
            end_idx (Optional[int], optional): Ending index of the links to crawl. Defaults to None.
            list_of_index ([type], optional): List of indices or range of indices of product urls to scrape. Defaults to None.
            compile_progress_files (bool, optional): Whether to combine crawled data into one file. Defaults to False.
            clean (bool, optional): Whether to clean the compiled data. Defaults to True.
            delete_progress (bool, optional): Whether to delete intermediate data after compilation into one file. Defaults to False.
        &#34;&#34;&#34;
        def fresh():
            self.meta = metadata[[&#39;prod_id&#39;, &#39;product_page&#39;]]
            self.meta[&#39;image_scraped&#39;] = &#39;N&#39;

        if download:
            if fresh_start:
                fresh()
            else:
                if Path(self.image_path/&#39;sph_image_progress_tracker.csv&#39;).exists():
                    self.meta = pd.read_csv(
                        self.image_path/&#39;sph_image_progress_tracker.csv&#39;)
                    if sum(self.meta.image_scraped == &#39;N&#39;) == 0:
                        if auto_fresh_start:
                            fresh()
                            self.logger.info(
                                &#39;Last Run was Completed. Starting Fresh Extraction.&#39;)
                        else:
                            self.logger.info(
                                f&#39;Image extraction for this cycle is complete. Please check files in path: {self.image_path}&#39;)
                            print(
                                f&#39;Image extraction for this cycle is complete. Please check files in path: {self.image_path}&#39;)
                    else:
                        self.logger.info(
                            &#39;Continuing Image Extraction From Last Run.&#39;)

            self.meta.to_csv(
                self.image_path/&#39;sph_image_progress_tracker.csv&#39;, index=None)
            self.meta.reset_index(inplace=True, drop=True)

            # set list or range of product indices to crawl
            if list_of_index:
                indices = list_of_index
            elif start_idx and end_idx is None:
                indices = range(start_idx, len(self.meta))
            elif start_idx is None and end_idx:
                indices = range(0, end_idx)
            elif start_idx is not None and end_idx is not None:
                indices = range(start_idx, end_idx)
            else:
                indices = range(len(self.meta))

            if list_of_index:
                self.get_images(
                    indices=list_of_index, open_headless=open_headless,
                    open_with_proxy_server=open_with_proxy_server,
                    randomize_proxy_usage=randomize_proxy_usage)
            else:  # By default the code will with 5 concurrent threads. you can change this behaviour by changing n_workers
                if start_idx:
                    lst_of_lst = ranges(
                        indices[-1]+1, n_workers, start_idx=start_idx)
                else:
                    lst_of_lst = ranges(len(indices), n_workers)
                print(lst_of_lst)

                headless = [open_headless for i in lst_of_lst]
                proxy = [open_with_proxy_server for i in lst_of_lst]
                rand_proxy = [randomize_proxy_usage for i in lst_of_lst]

                with concurrent.futures.ThreadPoolExecutor() as executor:
                    executor.map(self.get_images, lst_of_lst, headless,
                                 proxy, rand_proxy)

        self.logger.info(
            f&#39;Image files are downloaded to product specific folders. \
            Please look for file sph_product_review_all in path {self.image_path}&#39;)
        print(
            f&#39;Image files are downloaded to product specific folders. \
            Please look for file sph_product_review_all in path {self.image_path}&#39;)

    def terminate_logging(self) -&gt; None:
        &#34;&#34;&#34;terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.
        &#34;&#34;&#34;
        self.logger.handlers.clear()
        self.prod_image_log.stop_log()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="meiyume.utils.Sephora" href="../utils.html#meiyume.utils.Sephora">Sephora</a></li>
<li><a title="meiyume.utils.Browser" href="../utils.html#meiyume.utils.Browser">Browser</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="meiyume.sph.crawler.Image.extract"><code class="name flex">
<span>def <span class="ident">extract</span></span>(<span>self, metadata: Union[pandas.core.frame.DataFrame, str, pathlib.Path], download: bool = True, n_workers: int = 5, fresh_start: bool = False, auto_fresh_start: bool = False, open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = True, start_idx: Union[int, NoneType] = None, end_idx: Union[int, NoneType] = None, list_of_index=None)</span>
</code></dt>
<dd>
<div class="desc"><p>extract method controls all properties of the spiders and runs multi-threaded web crawling.</p>
<p>Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
This method has four major functionality:
* 1. Run the spider
* 2. Store data in regular intervals to free up ram
* 3. Compile all crawled data into one file.
* 4. Clean and push cleaned data to S3 storage for further algorithmic processing.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>metadata</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Dataframe containing product specific url, name and id of the products to be scraped.</dd>
<dt><strong><code>download</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to crawl data from or compile crawled data into one file. Defaults to True.</dd>
<dt><strong><code>n_workers</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>No. of parallel threads to run. Defaults to 5.</dd>
<dt><strong><code>fresh_start</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to continue last crawl job or start new one. Defaults to False.</dd>
<dt><strong><code>auto_fresh_start</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to automatically start a new crawl job if last job was finished. Defaults to False.</dd>
<dt><strong><code>open_headless</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to open browser headless. Defaults to False.</dd>
<dt><strong><code>open_with_proxy_server</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use ip rotation service. Defaults to True.</dd>
<dt><strong><code>randomize_proxy_usage</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use both proxy and native network in tandem to decrease proxy requests.
Defaults to False.</dd>
<dt><strong><code>start_idx</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>Starting index of the links to crawl. Defaults to None.</dd>
<dt><strong><code>end_idx</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>Ending index of the links to crawl. Defaults to None.</dd>
<dt><strong><code>list_of_index</code></strong> :&ensp;<code>[type]</code>, optional</dt>
<dd>List of indices or range of indices of product urls to scrape. Defaults to None.</dd>
<dt><strong><code>compile_progress_files</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to combine crawled data into one file. Defaults to False.</dd>
<dt><strong><code>clean</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to clean the compiled data. Defaults to True.</dd>
<dt><strong><code>delete_progress</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to delete intermediate data after compilation into one file. Defaults to False.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract(self, metadata: Union[pd.DataFrame, str, Path], download: bool = True,
            n_workers: int = 5, fresh_start: bool = False, auto_fresh_start: bool = False,
            open_headless: bool = False, open_with_proxy_server: bool = True,
            randomize_proxy_usage: bool = True,
            start_idx: Optional[int] = None, end_idx: Optional[int] = None, list_of_index=None,
            ):
    &#34;&#34;&#34;extract method controls all properties of the spiders and runs multi-threaded web crawling.

    Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
    This method has four major functionality:
    * 1. Run the spider
    * 2. Store data in regular intervals to free up ram
    * 3. Compile all crawled data into one file.
    * 4. Clean and push cleaned data to S3 storage for further algorithmic processing.

    Args:
        metadata (pd.DataFrame): Dataframe containing product specific url, name and id of the products to be scraped.
        download (bool, optional): Whether to crawl data from or compile crawled data into one file. Defaults to True.
        n_workers (int, optional): No. of parallel threads to run. Defaults to 5.
        fresh_start (bool, optional): Whether to continue last crawl job or start new one. Defaults to False.
        auto_fresh_start (bool, optional): Whether to automatically start a new crawl job if last job was finished. Defaults to False.
        open_headless (bool, optional):  Whether to open browser headless. Defaults to False.
        open_with_proxy_server (bool, optional): Whether to use ip rotation service. Defaults to True.
        randomize_proxy_usage (bool, optional): Whether to use both proxy and native network in tandem to decrease proxy requests.
                                                Defaults to False.
        start_idx (Optional[int], optional): Starting index of the links to crawl. Defaults to None.
        end_idx (Optional[int], optional): Ending index of the links to crawl. Defaults to None.
        list_of_index ([type], optional): List of indices or range of indices of product urls to scrape. Defaults to None.
        compile_progress_files (bool, optional): Whether to combine crawled data into one file. Defaults to False.
        clean (bool, optional): Whether to clean the compiled data. Defaults to True.
        delete_progress (bool, optional): Whether to delete intermediate data after compilation into one file. Defaults to False.
    &#34;&#34;&#34;
    def fresh():
        self.meta = metadata[[&#39;prod_id&#39;, &#39;product_page&#39;]]
        self.meta[&#39;image_scraped&#39;] = &#39;N&#39;

    if download:
        if fresh_start:
            fresh()
        else:
            if Path(self.image_path/&#39;sph_image_progress_tracker.csv&#39;).exists():
                self.meta = pd.read_csv(
                    self.image_path/&#39;sph_image_progress_tracker.csv&#39;)
                if sum(self.meta.image_scraped == &#39;N&#39;) == 0:
                    if auto_fresh_start:
                        fresh()
                        self.logger.info(
                            &#39;Last Run was Completed. Starting Fresh Extraction.&#39;)
                    else:
                        self.logger.info(
                            f&#39;Image extraction for this cycle is complete. Please check files in path: {self.image_path}&#39;)
                        print(
                            f&#39;Image extraction for this cycle is complete. Please check files in path: {self.image_path}&#39;)
                else:
                    self.logger.info(
                        &#39;Continuing Image Extraction From Last Run.&#39;)

        self.meta.to_csv(
            self.image_path/&#39;sph_image_progress_tracker.csv&#39;, index=None)
        self.meta.reset_index(inplace=True, drop=True)

        # set list or range of product indices to crawl
        if list_of_index:
            indices = list_of_index
        elif start_idx and end_idx is None:
            indices = range(start_idx, len(self.meta))
        elif start_idx is None and end_idx:
            indices = range(0, end_idx)
        elif start_idx is not None and end_idx is not None:
            indices = range(start_idx, end_idx)
        else:
            indices = range(len(self.meta))

        if list_of_index:
            self.get_images(
                indices=list_of_index, open_headless=open_headless,
                open_with_proxy_server=open_with_proxy_server,
                randomize_proxy_usage=randomize_proxy_usage)
        else:  # By default the code will with 5 concurrent threads. you can change this behaviour by changing n_workers
            if start_idx:
                lst_of_lst = ranges(
                    indices[-1]+1, n_workers, start_idx=start_idx)
            else:
                lst_of_lst = ranges(len(indices), n_workers)
            print(lst_of_lst)

            headless = [open_headless for i in lst_of_lst]
            proxy = [open_with_proxy_server for i in lst_of_lst]
            rand_proxy = [randomize_proxy_usage for i in lst_of_lst]

            with concurrent.futures.ThreadPoolExecutor() as executor:
                executor.map(self.get_images, lst_of_lst, headless,
                             proxy, rand_proxy)

    self.logger.info(
        f&#39;Image files are downloaded to product specific folders. \
        Please look for file sph_product_review_all in path {self.image_path}&#39;)
    print(
        f&#39;Image files are downloaded to product specific folders. \
        Please look for file sph_product_review_all in path {self.image_path}&#39;)</code></pre>
</details>
</dd>
<dt id="meiyume.sph.crawler.Image.get_images"><code class="name flex">
<span>def <span class="ident">get_images</span></span>(<span>self, indices: Union[list, range], open_headless: bool, open_with_proxy_server: bool, randomize_proxy_usage: bool) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>get_images scrapes individual product images.</p>
<p>Get Images download up to four images for on product.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>indices</code></strong> :&ensp;<code>Union[list, range]</code></dt>
<dd>list of indices or range of indices of product urls to scrape.</dd>
<dt><strong><code>open_headless</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to open browser headless.</dd>
<dt><strong><code>open_with_proxy_server</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use ip rotation service.</dd>
<dt><strong><code>randomize_proxy_usage</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use both proxy and native network in tandem to decrease proxy requests.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_images(self, indices: Union[list, range], open_headless: bool,
               open_with_proxy_server: bool,
               randomize_proxy_usage: bool) -&gt; None:
    &#34;&#34;&#34;get_images scrapes individual product images.

    Get Images download up to four images for on product.

    Args:
        indices (Union[list, range]): list of indices or range of indices of product urls to scrape.
        open_headless (bool): Whether to open browser headless.
        open_with_proxy_server (bool): Whether to use ip rotation service.
        randomize_proxy_usage (bool): Whether to use both proxy and native network in tandem to decrease proxy requests.
    &#34;&#34;&#34;

    for prod in self.meta.index[self.meta.index.isin(indices)]:
        if self.meta.loc[prod, &#39;image_scraped&#39;] in [&#39;Y&#39;, &#39;NA&#39;]:
            continue
        prod_id = self.meta.loc[prod, &#39;prod_id&#39;]
        product_page = self.meta.loc[prod, &#39;product_page&#39;]

        # create webdriver
        if randomize_proxy_usage:
            use_proxy = np.random.choice([True, False])
        else:
            use_proxy = True
        if open_with_proxy_server:
            # print(use_proxy)
            drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
                                    open_for_screenshot=True, path=self.image_path)
        else:
            drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
                                    open_for_screenshot=True, path=self.image_path)
        # open product page
        drv.get(product_page)
        time.sleep(15)  # 30
        accept_alert(drv, 10)
        close_popups(drv)

        try:
            product_text = drv.find_element_by_class_name(
                &#39;css-1wag3se&#39;).text
            if &#39;productnotcarried&#39; in product_text.lower():
                self.logger.info(str.encode(f&#39;prod_id: {prod_id} image extraction failed.\
                                        Product may not be available for sell currently.(page: {product_page})&#39;,
                                            &#39;utf-8&#39;, &#39;ignore&#39;))
                self.meta.loc[prod, &#39;image_scraped&#39;] = &#39;NA&#39;
                self.meta.to_csv(
                    self.image_path/&#39;sph_image_progress_tracker.csv&#39;, index=None)
                drv.quit()
                continue
        except Exception as ex:
            log_exception(
                self.logger, additional_information=f&#39;Prod ID: {prod_id}&#39;)

        # get image elements
        try:
            accept_alert(drv, 1)
            close_popups(drv)
            images = drv.find_elements_by_class_name(&#39;css-11rgy2w&#39;)
            if len(images) == 0:
                try:
                    accept_alert(drv, 1)
                    close_popups(drv)
                    images = drv.find_elements_by_class_name(&#39;css-11rgy2w&#39;)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    self.logger.info(str.encode(f&#39;prod_id: {prod_id} failed to get image sources.\
                                                (page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    self.meta.loc[prod, &#39;image_scraped&#39;] = &#39;NA&#39;
                    self.meta.to_csv(
                        self.image_path/&#39;sph_image_progress_tracker.csv&#39;, index=None)
                    drv.quit()
                    continue
        except Exception:
            continue
        else:
            if len(images) == 0:
                self.logger.info(str.encode(f&#39;{prod_id} image extraction failed.\
                                                (page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                self.meta.loc[prod, &#39;image_scraped&#39;] = &#39;NA&#39;
                self.meta.to_csv(
                    self.image_path/&#39;sph_image_progress_tracker.csv&#39;, index=None)
                drv.quit()
                continue
        # get image urls
        sources = [i.find_element_by_tag_name(
            &#39;img&#39;).get_attribute(&#39;src&#39;) for i in images]
        sources = [i.split(&#39;?&#39;)[0] for i in sources]

        if len(sources) &gt; 4:
            sources = sources[:3]

        self.current_image_path = self.image_path/prod_id
        if not self.current_image_path.exists():
            self.current_image_path.mkdir(parents=True, exist_ok=True)

        # download images
        image_count = 0
        try:
            for src in sources:
                drv.get(src)
                time.sleep(2)
                accept_alert(drv, 1)
                close_popups(drv)
                image_count += 1
                image_name = f&#39;{prod_id}_image_{image_count}.jpg&#39;
                drv.save_screenshot(
                    str(self.current_image_path/image_name))
            self.meta.loc[prod, &#39;image_scraped&#39;] = &#39;Y&#39;
            drv.quit()
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            if image_count &lt;= 1:
                self.logger.info(str.encode(f&#39;prod_id: {prod_id} image extraction failed.\
                                                (page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                self.meta.loc[prod, &#39;image_scraped&#39;] = &#39;NA&#39;
                self.meta.to_csv(
                    self.image_path/&#39;sph_image_progress_tracker.csv&#39;, index=None)
            drv.quit()
            continue

        if prod % 10 == 0 and prod != 0:
            self.meta.to_csv(
                self.image_path/&#39;sph_image_progress_tracker.csv&#39;, index=None)
    self.meta.to_csv(
        self.image_path/&#39;sph_image_progress_tracker.csv&#39;, index=None)</code></pre>
</details>
</dd>
<dt id="meiyume.sph.crawler.Image.terminate_logging"><code class="name flex">
<span>def <span class="ident">terminate_logging</span></span>(<span>self) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def terminate_logging(self) -&gt; None:
    &#34;&#34;&#34;terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.
    &#34;&#34;&#34;
    self.logger.handlers.clear()
    self.prod_image_log.stop_log()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="meiyume.utils.Sephora" href="../utils.html#meiyume.utils.Sephora">Sephora</a></b></code>:
<ul class="hlist">
<li><code><a title="meiyume.utils.Sephora.open_browser" href="../utils.html#meiyume.utils.Browser.open_browser">open_browser</a></code></li>
<li><code><a title="meiyume.utils.Sephora.open_browser_firefox" href="../utils.html#meiyume.utils.Browser.open_browser_firefox">open_browser_firefox</a></code></li>
<li><code><a title="meiyume.utils.Sephora.scroll_down_page" href="../utils.html#meiyume.utils.Browser.scroll_down_page">scroll_down_page</a></code></li>
<li><code><a title="meiyume.utils.Sephora.scroll_to_element" href="../utils.html#meiyume.utils.Browser.scroll_to_element">scroll_to_element</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="meiyume.sph.crawler.Metadata"><code class="flex name class">
<span>class <span class="ident">Metadata</span></span>
<span>(</span><span>log: bool = True, path: pathlib.Path = WindowsPath('D:/Amit/Meiyume/meiyume_master_source_codes'))</span>
</code></dt>
<dd>
<div class="desc"><p>Metadata extracts product metadata such as product page url, prices and brand from Sephora website.</p>
<p>The Metadata class begins the data crawling process and all other stages depend on the product urls extracted by Metadata class.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>Sephora</code></strong> :&ensp;<code>Browser</code></dt>
<dd>Class that initializes folder paths and selenium webdriver for data scraping.</dd>
</dl>
<p><strong>init</strong> Metadata class instace initializer.</p>
<p>This method sets all the folder paths required for Metadata crawler to work.
If the paths does not exist the paths get automatically created depending on current directory
or provided directory.</p>
<h2 id="args_1">Args</h2>
<dl>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to create crawling exception and progess log. Defaults to True.</dd>
<dt><strong><code>path</code></strong> :&ensp;<code>Path</code>, optional</dt>
<dd>Folder path where the Metadata will be extracted. Defaults to current directory(Path.cwd()).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Metadata(Sephora):
    &#34;&#34;&#34;Metadata extracts product metadata such as product page url, prices and brand from Sephora website.

    The Metadata class begins the data crawling process and all other stages depend on the product urls extracted by Metadata class.

    Args:
        Sephora (Browser): Class that initializes folder paths and selenium webdriver for data scraping.

    &#34;&#34;&#34;

    base_url = &#34;https://www.sephora.com&#34;
    info = tldextract.extract(base_url)
    source = info.registered_domain

    @classmethod
    def update_base_url(cls, url: str) -&gt; None:
        &#34;&#34;&#34;update_base_url defines the parent url from where the data scraping process will begin.

        Args:
            url (str): The URL from which the spider will enter the website.

        &#34;&#34;&#34;
        cls.base_url = url
        cls.info = tldextract.extract(cls.base_url)
        cls.source = cls.info.registered_domain

    def __init__(self, log: bool = True, path: Path = Path.cwd()):
        &#34;&#34;&#34;__init__ Metadata class instace initializer.

        This method sets all the folder paths required for Metadata crawler to work.
        If the paths does not exist the paths get automatically created depending on current directory
        or provided directory.

        Args:
            log (bool, optional): Whether to create crawling exception and progess log. Defaults to True.
            path (Path, optional): Folder path where the Metadata will be extracted. Defaults to current directory(Path.cwd()).

        &#34;&#34;&#34;

        super().__init__(path=path, data_def=&#39;meta&#39;)
        self.path = path
        self.current_progress_path = self.metadata_path/&#39;current_progress&#39;
        self.current_progress_path.mkdir(parents=True, exist_ok=True)

        # move old raw and clean files to old folder
        old_metadata_files = list(self.metadata_path.glob(
            &#39;sph_product_metadata_all*&#39;))
        for f in old_metadata_files:
            shutil.move(str(f), str(self.old_metadata_files_path))

        old_clean_metadata_files = os.listdir(self.metadata_clean_path)
        for f in old_clean_metadata_files:
            shutil.move(str(self.metadata_clean_path/f),
                        str(self.old_metadata_clean_files_path))
        # set logger
        if log:
            self.prod_meta_log = Logger(
                &#34;sph_prod_metadata_extraction&#34;, path=self.crawl_log_path)
            self.logger, _ = self.prod_meta_log.start_log()

    def get_product_type_urls(self, open_headless: bool, open_with_proxy_server: bool) -&gt; pd.DataFrame:
        &#34;&#34;&#34;get_product_type_urls Extract the category/subcategory structure and urls to extract the products of those category/subcategory.

        Extracts the links of pages containing the list of all products structured into
        category/subcategory/product type to effectively stored in relational database.
        Defines the structure of data extraction that helps store unstructured data in a structured manner.

        Args:
            open_headless (bool): Whether to open browser headless.
            open_with_proxy_server (bool): Whether to use proxy server.

        Returns:
            pd.DataFrame: returns pandas dataframe containing urls for getting list of products, category, subcategory etc.

        &#34;&#34;&#34;
        # create webdriver instance
        drv = self.open_browser(
            open_headless=open_headless, open_with_proxy_server=open_with_proxy_server, path=self.metadata_path)

        drv.get(self.base_url)
        time.sleep(15)
        # click and close welcome forms
        accept_alert(drv, 10)
        close_popups(drv)

        cats = drv.find_elements_by_css_selector(&#39;a[id^=&#34;top_nav_drop_&#34;]&#39;)
        cat_urls = []
        for c in cats:
            if c.get_attribute(&#39;href&#39;) is not None:
                cat_name, url = (c.get_attribute(&#34;href&#34;).split(&#34;/&#34;)
                                 [-1], c.get_attribute(&#34;href&#34;))
                cat_urls.append((cat_name, url))
                self.logger.info(str.encode(f&#39;Category:- name:{cat_name}, \
                                          url:{url}&#39;, &#34;utf-8&#34;, &#34;ignore&#34;))

        sub_cat_urls = []
        for cu in cat_urls:
            cat_name = cu[0]
            if cat_name in [&#39;brands-list&#39;, &#39;new-beauty-products&#39;, &#39;sephora-collection&#39;]:
                continue
            cat_url = cu[1]
            drv.get(cat_url)

            time.sleep(10)
            accept_alert(drv, 10)
            close_popups(drv)

            sub_cats = drv.find_elements_by_css_selector(
                &#39;a[data-at*=&#34;top_level_category&#34;]&#39;)
            # sub_cats.extend(drv.find_elements_by_class_name(&#34;css-or7ouu&#34;))
            if len(sub_cats) &gt; 0:
                for s in sub_cats:
                    sub_cat_urls.append((cat_name, s.get_attribute(
                        &#34;href&#34;).split(&#34;/&#34;)[-1], s.get_attribute(&#34;href&#34;)))
                    self.logger.info(str.encode(f&#39;SubCategory:- name:{s.get_attribute(&#34;href&#34;).split(&#34;/&#34;)[-1]},\
                                                  url:{s.get_attribute(&#34;href&#34;)}&#39;, &#34;utf-8&#34;, &#34;ignore&#34;))
            else:
                sub_cat_urls.append(
                    (cat_name, cat_url.split(&#39;/&#39;)[-1], cat_url))

        product_type_urls = []
        for su in sub_cat_urls:
            cat_name = su[0]
            sub_cat_name = su[1]
            if any(name in sub_cat_name for name in [&#39;best-selling&#39;, &#39;new&#39;, &#39;mini&#39;]):
                continue
            sub_cat_url = su[2]
            drv.get(sub_cat_url)

            time.sleep(10)
            accept_alert(drv, 10)
            close_popups(drv)

            product_types = drv.find_elements_by_css_selector(
                &#39;a[data-at*=&#34;nth_level&#34;]&#39;)
            if len(product_types) &gt; 0:
                for item in product_types:
                    product_type_urls.append((cat_name, sub_cat_name, item.get_attribute(&#34;href&#34;).split(&#34;/&#34;)[-1],
                                              item.get_attribute(&#34;href&#34;)))
                    self.logger.info(str.encode(f&#39;ProductType:- name:{item.get_attribute(&#34;href&#34;).split(&#34;/&#34;)[-1]},\
                                                  url:{item.get_attribute(&#34;href&#34;)}&#39;, &#34;utf-8&#34;, &#34;ignore&#34;))
            else:
                product_type_urls.append(
                    (cat_name, sub_cat_name, sub_cat_url.split(&#39;/&#39;)[-1], sub_cat_url))

        df = pd.DataFrame(product_type_urls, columns=[
                          &#39;category_raw&#39;, &#39;sub_category_raw&#39;, &#39;product_type&#39;, &#39;url&#39;])

        df_clean = pd.DataFrame(sub_cat_urls, columns=[
                                &#39;category_raw&#39;, &#39;product_type&#39;, &#39;url&#39;])
        df_clean[&#39;sub_category_raw&#39;] = &#39;CLEAN&#39;
        df_clean = df_clean[(df_clean.url.apply(
            lambda x: True if &#39;clean&#39; in x else False)) &amp; (df_clean.product_type != &#39;cleanser&#39;)]

        df_vegan = pd.DataFrame(sub_cat_urls, columns=[
                                &#39;category_raw&#39;, &#39;product_type&#39;, &#39;url&#39;])
        df_vegan[&#39;sub_category_raw&#39;] = &#39;VEGAN&#39;
        df_vegan = df_vegan[df_vegan.url.apply(
            lambda x: True if &#39;vegan&#39; in x.lower() else False)]

        df = pd.concat([df, df_clean, df_vegan], axis=0)
        df.reset_index(inplace=True, drop=True)
        df.to_feather(self.metadata_path/&#39;sph_product_cat_subcat_structure&#39;)
        drv.quit()

        df.drop_duplicates(subset=&#39;url&#39;, inplace=True)
        df.drop(columns=&#39;sub_category_raw&#39;, inplace=True)
        df.reset_index(inplace=True, drop=True)
        df[&#39;scraped&#39;] = &#39;N&#39;
        df.to_feather(self.metadata_path/f&#39;sph_product_type_urls_to_extract&#39;)
        return df

    def get_metadata(self, indices: Union[list, range],
                     open_headless: bool, open_with_proxy_server: bool,
                     randomize_proxy_usage: bool,
                     product_meta_data: list = []) -&gt; None:
        &#34;&#34;&#34;get_metadata Crawls product listing pages for price, name, brand etc.

        Get Metadata crawls a product type page for example lipstick.
        The function gets individual product urls, names, brands and prices etc. and stores
        in a relational table structure to use later to download product images, scrape reviews and
        other specific information.

        Args:
            indices (Union[list, range]): list of indices or range of indices of product urls to scrape.
            open_headless (bool): Whether to open browser headless.
            open_with_proxy_server (bool): Whether to use proxy server.
            randomize_proxy_usage (bool): Whether to use both proxy and native network in tandem to decrease proxy requests.
            product_meta_data (list, optional): Empty intermediate list to store product metadata during parallel crawl. Defaults to [].

        &#34;&#34;&#34;
        for pt in self.product_type_urls.index[self.product_type_urls.index.isin(indices)]:
            cat_name = self.product_type_urls.loc[pt, &#39;category_raw&#39;]
            product_type = self.product_type_urls.loc[pt, &#39;product_type&#39;]
            product_type_link = self.product_type_urls.loc[pt, &#39;url&#39;]

            self.progress_tracker.loc[pt, &#39;product_type&#39;] = product_type
            # print(product_type_link)
            if &#39;best-selling&#39; in product_type.lower() or &#39;new&#39; in product_type.lower():
                self.progress_tracker.loc[pt, &#39;scraped&#39;] = &#39;NA&#39;
                continue

            if randomize_proxy_usage:
                use_proxy = np.random.choice([True, False])
            else:
                use_proxy = True
            if open_with_proxy_server:
                # print(use_proxy)
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
                                        path=self.metadata_path)
            else:
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
                                        path=self.metadata_path)

            drv.get(product_type_link)
            time.sleep(15)  # 30
            accept_alert(drv, 10)
            close_popups(drv)

            try:
                chat_popup_button = WebDriverWait(drv, 3).until(
                    EC.presence_of_element_located((By.XPATH, &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)))
                chat_popup_button = drv.find_element_by_xpath(
                    &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)
                self.scroll_to_element(drv, chat_popup_button)
                ActionChains(drv).move_to_element(
                    chat_popup_button).click(chat_popup_button).perform()
            except TimeoutException:
                pass

            # sort by new products (required to get all new products properly)
            try:
                sort_dropdown = drv.find_element_by_css_selector(
                    &#39;button[id=&#34;cat_sort_menu_trigger&#34;]&#39;)
                Browser().scroll_to_element(drv, sort_dropdown)
                ActionChains(drv).move_to_element(
                    sort_dropdown).click(sort_dropdown).perform()
                drv.find_elements_by_css_selector(
                    &#39;div[id=&#34;cat_sort_menu&#34;]&gt;button&#39;)[2].click()
                time.sleep(10)
                # sort_dropdown = drv.find_element_by_class_name(&#39;css-16tfpwn&#39;)
                # self.scroll_to_element(drv, sort_dropdown)
                # ActionChains(drv).move_to_element(
                #     sort_dropdown).click(sort_dropdown).perform()
                # button = drv.find_element_by_xpath(
                #     &#39;//*[@id=&#34;cat_sort_menu&#34;]/button[3]&#39;)
                # drv.implicitly_wait(4)
                # self.scroll_to_element(drv, button)
                # ActionChains(drv).move_to_element(
                #     button).click(button).perform()
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod Type: {product_type}&#39;)
                self.logger.info(str.encode(
                    f&#39;Category: {cat_name} - ProductType {product_type} cannot sort by NEW.(page link: {product_type_link})&#39;,
                    &#39;utf-8&#39;, &#39;ignore&#39;))
                pass

            # load all the products
            self.scroll_down_page(drv, h2=0.8, speed=3)
            time.sleep(8)

            # check whether on the first page of product type
            try:
                close_popups(drv)
                accept_alert(drv, 2)
                current_page = drv.find_element_by_class_name(
                    &#39;css-g48inl&#39;).text
            except NoSuchElementException as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod Type: {product_type}&#39;)
                self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType {product_type} has\
                only one page of products.(page link: {product_type_link})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                one_page = True
                current_page = 1
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod Type: {product_type}&#39;)
                self.progress_tracker.loc[pt, &#39;scraped&#39;] = &#39;NA&#39;
                self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType {product_type}\
                     page not found.(page link: {product_type_link})&#39;,
                                            &#39;utf-8&#39;, &#39;ignore&#39;))
            else:
                # get a list of all available pages
                one_page = False
                # get next page button
                next_page_button = drv.find_element_by_css_selector(
                    &#39;div&gt;nav&gt;ul&gt;button[aria-label=&#34;Next&#34;]&#39;)
                pages = []
                for page in drv.find_elements_by_class_name(&#39;css-1lk9n5p&#39;):
                    pages.append(page.text)

            # start getting product form each page
            while True:
                cp = 0
                self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type}\
                                  getting product from page {current_page}.(page link: {product_type_link})&#39;,
                                            &#39;utf-8&#39;, &#39;ignore&#39;))
                time.sleep(5)
                close_popups(drv)
                accept_alert(drv, 2)
                products = drv.find_elements_by_css_selector(
                    &#39;div[data-comp=&#34;ProductGrid &#34;]&gt;div&gt;div&gt;a&#39;)
                # print(len(products))

                for p in products:
                    time.sleep(0.5)

                    close_popups(drv)
                    accept_alert(drv, 0.5)

                    self.scroll_to_element(drv, p)
                    ActionChains(drv).move_to_element(p).perform()

                    try:
                        product_name = p.get_attribute(&#39;aria-label&#39;)
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                     product {products.index(p)} metadata extraction failed.\
                                                (page_link: {product_type_link} - page_no: {current_page})&#39;,
                                                    &#39;utf-8&#39;, &#39;ignore&#39;))
                        continue

                    try:
                        new_f = p.find_element_by_css_selector(
                            &#39;div[data-at=&#34;product_badges&#34;]&#39;).text
                        product_new_flag = &#39;NEW&#39;
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        product_new_flag = &#39;&#39;
                        # self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                        #                              product {products.index(p)} product_new_flag extraction failed.\
                        #                         (page_link: {product_type_link} - page_no: {current_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    try:
                        product_page = p.get_attribute(&#39;href&#39;)
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        product_page = &#39;&#39;
                        self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                     product {products.index(p)} product_page extraction failed.\
                                                (page_link: {product_type_link} - page_no: {current_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    try:
                        brand = p.find_element_by_css_selector(
                            &#39;span[data-at=&#34;sku_item_brand&#34;]&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        brand = &#39;&#39;
                        self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                     product {products.index(p)} brand extraction failed.\
                                                (page_link: {product_type_link} - page_no: {current_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    try:
                        rating = p.find_element_by_css_selector(
                            &#39;div[data-comp=&#34;StarRating &#34;]&#39;).get_attribute(&#39;aria-label&#39;)
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        rating = &#39;&#39;
                        self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                     product {products.index(p)} rating extraction failed.\
                                                (page_link: {product_type_link} - page_no: {current_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    try:
                        price = p.find_element_by_css_selector(
                            &#39;span[data-at=&#34;sku_item_price_list&#34;]&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        price = &#39;&#39;
                        self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                      product {products.index(p)} price extraction failed.\
                                                (page_link: {product_type_link} - page_no: {current_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

                    if datetime.now().day &lt; 15:
                        meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
                    else:
                        meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;

                    product_data_dict = {&#34;product_name&#34;: product_name, &#34;product_page&#34;: product_page, &#34;brand&#34;: brand, &#34;price&#34;: price,
                                         &#34;rating&#34;: rating, &#34;category&#34;: cat_name, &#34;product_type&#34;: product_type, &#34;new_flag&#34;: product_new_flag,
                                         &#34;complete_scrape_flag&#34;: &#34;N&#34;, &#34;meta_date&#34;: meta_date}
                    cp += 1
                    self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                 Product: {product_name} - {cp} extracted successfully.\
                                                (page_link: {product_type_link} - page_no: {current_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    product_meta_data.append(product_data_dict)

                if one_page:
                    break
                elif int(current_page) == int(pages[-1]):
                    self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} extraction complete.\
                                                (page_link: {product_type_link} - page_no: {current_page})&#39;,
                                                &#39;utf-8&#39;, &#39;ignore&#39;))
                    break
                else:
                    # go to next page
                    try:
                        self.scroll_to_element(drv, next_page_button)
                        ActionChains(drv).move_to_element(
                            next_page_button).click(next_page_button).perform()
                        time.sleep(15)
                        accept_alert(drv, 10)
                        close_popups(drv)
                        self.scroll_down_page(drv, h2=0.8, speed=3)
                        time.sleep(10)
                        current_page = drv.find_element_by_class_name(
                            &#39;css-g48inl&#39;).text
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod Type: {product_type}&#39;)
                        self.logger.info(str.encode(f&#39;Page navigation issue occurred for Category: {cat_name} - \
                                                        ProductType: {product_type} (page_link: {product_type_link} \
                                                        - page_no: {current_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                        break
            drv.quit()

            if len(product_meta_data) &gt; 0:
                product_meta_df = pd.DataFrame(product_meta_data)
                product_meta_df.to_feather(
                    self.current_progress_path/f&#39;sph_prod_meta_extract_progress_{product_type}_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}&#39;)
                self.logger.info(
                    f&#39;Completed till IndexPosition: {pt} - ProductType: {product_type}. (URL:{product_type_link})&#39;)
                self.progress_tracker.loc[pt, &#39;scraped&#39;] = &#39;Y&#39;
                self.progress_tracker.to_feather(
                    self.metadata_path/&#39;sph_metadata_progress_tracker&#39;)
                product_meta_data = []
        self.logger.info(&#39;Metadata Extraction Complete&#39;)
        print(&#39;Metadata Extraction Complete&#39;)

    def extract(self, download: bool = True, fresh_start: bool = False, auto_fresh_start: bool = False, n_workers: int = 5,
                open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = True,
                start_idx: Optional[int] = None, end_idx: Optional[int] = None, list_of_index=None,
                compile_progress_files: bool = False, clean: bool = True, delete_progress: bool = False) -&gt; None:
        &#34;&#34;&#34;extract method controls all properties of the spiders and runs multi-threaded web crawling.

        Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
        This method has four major functionality:
        * 1. Run the spider
        * 2. Store data in regular intervals to free up ram
        * 3. Compile all crawled data into one file.
        * 4. Clean and push cleaned data to S3 storage for further algorithmic processing.

        Args:
            download (bool, optional): Whether to crawl data from or compile crawled data into one file. Defaults to True.
            fresh_start (bool, optional): Whether to continue last crawl job or start new one. Defaults to False.
            auto_fresh_start (bool, optional): Whether to automatically start a new crawl job if last job was finished.
                                               Defaults to False.
            n_workers (int, optional): No. of parallel threads to run. Defaults to 5.
            open_headless (bool, optional): Whether to open browser headless. Defaults to False.
            open_with_proxy_server (bool, optional): Whether to use ip rotation service. Defaults to True.
            randomize_proxy_usage (bool, optional): Whether to use both proxy and native network in tandem to decrease proxy requests.
                                                    Defaults to True.
            start_idx (Optional[int], optional): Starting index of the links to crawl. Defaults to None.
            end_idx (Optional[int], optional): Ending index of the links to crawl. Defaults to None.
            list_of_index ([type], optional): List of indices or range of indices of product urls to scrape. Defaults to None.
            compile_progress_files (bool, optional): Whether to combine crawled data into one file. Defaults to False.
            clean (bool, optional): Whether to clean the compiled data. Defaults to True.
            delete_progress (bool, optional): Whether to delete intermediate data after compilation into one file. Defaults to False.
        &#34;&#34;&#34;
        def fresh():
            &#34;&#34;&#34; If fresh_start is True, this function sets initial parameters for a fresh data crawl.
            &#34;&#34;&#34;
            self.product_type_urls = self.get_product_type_urls(open_headless=open_headless,
                                                                open_with_proxy_server=open_with_proxy_server)
            # progress tracker: captures scraped and error desc
            self.progress_tracker = pd.DataFrame(index=self.product_type_urls.index, columns=[
                &#39;product_type&#39;, &#39;scraped&#39;, &#39;error_desc&#39;])
            self.progress_tracker.scraped = &#39;N&#39;

        if fresh_start:
            self.logger.info(&#39;Starting Fresh Extraction.&#39;)
            fresh()
        else:
            if Path(self.metadata_path/&#39;sph_product_type_urls_to_extract&#39;).exists():
                self.product_type_urls = pd.read_feather(
                    self.metadata_path/&#39;sph_product_type_urls_to_extract&#39;)
                if Path(self.metadata_path/&#39;sph_metadata_progress_tracker&#39;).exists():
                    self.progress_tracker = pd.read_feather(
                        self.metadata_path/&#39;sph_metadata_progress_tracker&#39;)
                else:
                    self.progress_tracker = pd.DataFrame(index=self.product_type_urls.index, columns=[
                        &#39;product_type&#39;, &#39;scraped&#39;, &#39;error_desc&#39;])
                    self.progress_tracker.scraped = &#39;N&#39;
                    self.progress_tracker.to_feather(
                        self.metadata_path/&#39;sph_metadata_progress_tracker&#39;)
                if sum(self.progress_tracker.scraped == &#39;N&#39;) &gt; 0:
                    self.logger.info(
                        &#39;Continuing Metadata Extraction From Last Run.&#39;)
                    self.product_type_urls = self.product_type_urls[self.product_type_urls.index.isin(
                        self.progress_tracker.index[self.progress_tracker.scraped == &#39;N&#39;].values.tolist())]
                else:
                    if auto_fresh_start:
                        self.logger.info(
                            &#39;Previous Run Was Complete. Starting Fresh Extraction.&#39;)
                        fresh()
                    else:
                        self.logger.info(
                            &#39;Previous Run is Complete.&#39;)
            else:
                self.logger.info(
                    &#39;URL File Not Found. Start Fresh Extraction.&#39;)
        # print(self.progress_tracker)
        if download:
            # set list or range of product indices to crawl
            if list_of_index:
                indices = list_of_index
            elif start_idx and end_idx is None:
                indices = range(start_idx, len(self.product_type_urls))
            elif start_idx is None and end_idx:
                indices = range(0, end_idx)
            elif start_idx is not None and end_idx is not None:
                indices = range(start_idx, end_idx)
            else:
                indices = range(len(self.product_type_urls))
            # print(indices)
            if list_of_index:
                self.get_metadata(indices=list_of_index,
                                  open_headless=open_headless,
                                  open_with_proxy_server=open_with_proxy_server,
                                  randomize_proxy_usage=randomize_proxy_usage,
                                  product_meta_data=[])
            else:
                &#39;&#39;&#39;
                # review_Data and item_data are lists of empty lists so that each namepace of function call will
                # have its separate detail_data
                # list to strore scraped dictionaries. will save memory(ram/hard-disk) consumption. will stop data duplication
                &#39;&#39;&#39;
                if start_idx:
                    lst_of_lst = ranges(
                        indices[-1]+1, n_workers, start_idx=start_idx)
                else:
                    lst_of_lst = ranges(len(indices), n_workers)
                print(lst_of_lst)
                headless = [open_headless for i in lst_of_lst]
                proxy = [open_with_proxy_server for i in lst_of_lst]
                rand_proxy = [randomize_proxy_usage for i in lst_of_lst]
                product_meta_data = [[] for i in lst_of_lst]
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    &#39;&#39;&#39;
                    # but each of the function namespace will be modifying only one metadata tracing file so that progress saving
                    # is tracked correctly. else multiple progress tracker file will be created with difficulty to combine correct
                    # progress information
                    &#39;&#39;&#39;
                    executor.map(self.get_metadata, lst_of_lst,
                                 headless, proxy, rand_proxy, product_meta_data)

        if compile_progress_files:
            self.logger.info(&#39;Creating Combined Metadata File&#39;)
            files = [f for f in self.current_progress_path.glob(
                &#34;sph_prod_meta_extract_progress_*&#34;)]
            li = [pd.read_feather(file) for file in files]
            metadata_df = pd.concat(li, axis=0, ignore_index=True)
            metadata_df.reset_index(inplace=True, drop=True)
            metadata_df[&#39;source&#39;] = self.source

            if datetime.now().day &lt; 15:
                meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
            else:
                meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;
            filename = f&#39;sph_product_metadata_all_{meta_date}&#39;
            metadata_df.to_feather(self.metadata_path/filename)

            self.logger.info(
                f&#39;Metadata file created. Please look for file {filename} in path {self.metadata_path}&#39;)
            print(
                f&#39;Metadata file created. Please look for file {filename} in path {self.metadata_path}&#39;)

            if clean:
                cleaner = Cleaner(path=self.path)
                _ = cleaner.clean(
                    data=self.metadata_path/filename)
                self.logger.info(
                    &#39;Metadata Cleaned and Removed Duplicates for Details/Review/Image Extraction.&#39;)

            if delete_progress:
                shutil.rmtree(
                    f&#39;{self.metadata_path}\\current_progress&#39;, ignore_errors=True)
                self.logger.info(&#39;Progress files deleted&#39;)

    def terminate_logging(self):
        &#34;&#34;&#34;terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.
        &#34;&#34;&#34;
        self.logger.handlers.clear()
        self.prod_meta_log.stop_log()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="meiyume.utils.Sephora" href="../utils.html#meiyume.utils.Sephora">Sephora</a></li>
<li><a title="meiyume.utils.Browser" href="../utils.html#meiyume.utils.Browser">Browser</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="meiyume.sph.crawler.Metadata.base_url"><code class="name">var <span class="ident">base_url</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="meiyume.sph.crawler.Metadata.info"><code class="name">var <span class="ident">info</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="meiyume.sph.crawler.Metadata.source"><code class="name">var <span class="ident">source</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="meiyume.sph.crawler.Metadata.update_base_url"><code class="name flex">
<span>def <span class="ident">update_base_url</span></span>(<span>url: str) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>update_base_url defines the parent url from where the data scraping process will begin.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>url</code></strong> :&ensp;<code>str</code></dt>
<dd>The URL from which the spider will enter the website.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def update_base_url(cls, url: str) -&gt; None:
    &#34;&#34;&#34;update_base_url defines the parent url from where the data scraping process will begin.

    Args:
        url (str): The URL from which the spider will enter the website.

    &#34;&#34;&#34;
    cls.base_url = url
    cls.info = tldextract.extract(cls.base_url)
    cls.source = cls.info.registered_domain</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="meiyume.sph.crawler.Metadata.extract"><code class="name flex">
<span>def <span class="ident">extract</span></span>(<span>self, download: bool = True, fresh_start: bool = False, auto_fresh_start: bool = False, n_workers: int = 5, open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = True, start_idx: Union[int, NoneType] = None, end_idx: Union[int, NoneType] = None, list_of_index=None, compile_progress_files: bool = False, clean: bool = True, delete_progress: bool = False) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>extract method controls all properties of the spiders and runs multi-threaded web crawling.</p>
<p>Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
This method has four major functionality:
* 1. Run the spider
* 2. Store data in regular intervals to free up ram
* 3. Compile all crawled data into one file.
* 4. Clean and push cleaned data to S3 storage for further algorithmic processing.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>download</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to crawl data from or compile crawled data into one file. Defaults to True.</dd>
<dt><strong><code>fresh_start</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to continue last crawl job or start new one. Defaults to False.</dd>
<dt><strong><code>auto_fresh_start</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to automatically start a new crawl job if last job was finished.
Defaults to False.</dd>
<dt><strong><code>n_workers</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>No. of parallel threads to run. Defaults to 5.</dd>
<dt><strong><code>open_headless</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to open browser headless. Defaults to False.</dd>
<dt><strong><code>open_with_proxy_server</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use ip rotation service. Defaults to True.</dd>
<dt><strong><code>randomize_proxy_usage</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use both proxy and native network in tandem to decrease proxy requests.
Defaults to True.</dd>
<dt><strong><code>start_idx</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>Starting index of the links to crawl. Defaults to None.</dd>
<dt><strong><code>end_idx</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>Ending index of the links to crawl. Defaults to None.</dd>
<dt><strong><code>list_of_index</code></strong> :&ensp;<code>[type]</code>, optional</dt>
<dd>List of indices or range of indices of product urls to scrape. Defaults to None.</dd>
<dt><strong><code>compile_progress_files</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to combine crawled data into one file. Defaults to False.</dd>
<dt><strong><code>clean</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to clean the compiled data. Defaults to True.</dd>
<dt><strong><code>delete_progress</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to delete intermediate data after compilation into one file. Defaults to False.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract(self, download: bool = True, fresh_start: bool = False, auto_fresh_start: bool = False, n_workers: int = 5,
            open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = True,
            start_idx: Optional[int] = None, end_idx: Optional[int] = None, list_of_index=None,
            compile_progress_files: bool = False, clean: bool = True, delete_progress: bool = False) -&gt; None:
    &#34;&#34;&#34;extract method controls all properties of the spiders and runs multi-threaded web crawling.

    Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
    This method has four major functionality:
    * 1. Run the spider
    * 2. Store data in regular intervals to free up ram
    * 3. Compile all crawled data into one file.
    * 4. Clean and push cleaned data to S3 storage for further algorithmic processing.

    Args:
        download (bool, optional): Whether to crawl data from or compile crawled data into one file. Defaults to True.
        fresh_start (bool, optional): Whether to continue last crawl job or start new one. Defaults to False.
        auto_fresh_start (bool, optional): Whether to automatically start a new crawl job if last job was finished.
                                           Defaults to False.
        n_workers (int, optional): No. of parallel threads to run. Defaults to 5.
        open_headless (bool, optional): Whether to open browser headless. Defaults to False.
        open_with_proxy_server (bool, optional): Whether to use ip rotation service. Defaults to True.
        randomize_proxy_usage (bool, optional): Whether to use both proxy and native network in tandem to decrease proxy requests.
                                                Defaults to True.
        start_idx (Optional[int], optional): Starting index of the links to crawl. Defaults to None.
        end_idx (Optional[int], optional): Ending index of the links to crawl. Defaults to None.
        list_of_index ([type], optional): List of indices or range of indices of product urls to scrape. Defaults to None.
        compile_progress_files (bool, optional): Whether to combine crawled data into one file. Defaults to False.
        clean (bool, optional): Whether to clean the compiled data. Defaults to True.
        delete_progress (bool, optional): Whether to delete intermediate data after compilation into one file. Defaults to False.
    &#34;&#34;&#34;
    def fresh():
        &#34;&#34;&#34; If fresh_start is True, this function sets initial parameters for a fresh data crawl.
        &#34;&#34;&#34;
        self.product_type_urls = self.get_product_type_urls(open_headless=open_headless,
                                                            open_with_proxy_server=open_with_proxy_server)
        # progress tracker: captures scraped and error desc
        self.progress_tracker = pd.DataFrame(index=self.product_type_urls.index, columns=[
            &#39;product_type&#39;, &#39;scraped&#39;, &#39;error_desc&#39;])
        self.progress_tracker.scraped = &#39;N&#39;

    if fresh_start:
        self.logger.info(&#39;Starting Fresh Extraction.&#39;)
        fresh()
    else:
        if Path(self.metadata_path/&#39;sph_product_type_urls_to_extract&#39;).exists():
            self.product_type_urls = pd.read_feather(
                self.metadata_path/&#39;sph_product_type_urls_to_extract&#39;)
            if Path(self.metadata_path/&#39;sph_metadata_progress_tracker&#39;).exists():
                self.progress_tracker = pd.read_feather(
                    self.metadata_path/&#39;sph_metadata_progress_tracker&#39;)
            else:
                self.progress_tracker = pd.DataFrame(index=self.product_type_urls.index, columns=[
                    &#39;product_type&#39;, &#39;scraped&#39;, &#39;error_desc&#39;])
                self.progress_tracker.scraped = &#39;N&#39;
                self.progress_tracker.to_feather(
                    self.metadata_path/&#39;sph_metadata_progress_tracker&#39;)
            if sum(self.progress_tracker.scraped == &#39;N&#39;) &gt; 0:
                self.logger.info(
                    &#39;Continuing Metadata Extraction From Last Run.&#39;)
                self.product_type_urls = self.product_type_urls[self.product_type_urls.index.isin(
                    self.progress_tracker.index[self.progress_tracker.scraped == &#39;N&#39;].values.tolist())]
            else:
                if auto_fresh_start:
                    self.logger.info(
                        &#39;Previous Run Was Complete. Starting Fresh Extraction.&#39;)
                    fresh()
                else:
                    self.logger.info(
                        &#39;Previous Run is Complete.&#39;)
        else:
            self.logger.info(
                &#39;URL File Not Found. Start Fresh Extraction.&#39;)
    # print(self.progress_tracker)
    if download:
        # set list or range of product indices to crawl
        if list_of_index:
            indices = list_of_index
        elif start_idx and end_idx is None:
            indices = range(start_idx, len(self.product_type_urls))
        elif start_idx is None and end_idx:
            indices = range(0, end_idx)
        elif start_idx is not None and end_idx is not None:
            indices = range(start_idx, end_idx)
        else:
            indices = range(len(self.product_type_urls))
        # print(indices)
        if list_of_index:
            self.get_metadata(indices=list_of_index,
                              open_headless=open_headless,
                              open_with_proxy_server=open_with_proxy_server,
                              randomize_proxy_usage=randomize_proxy_usage,
                              product_meta_data=[])
        else:
            &#39;&#39;&#39;
            # review_Data and item_data are lists of empty lists so that each namepace of function call will
            # have its separate detail_data
            # list to strore scraped dictionaries. will save memory(ram/hard-disk) consumption. will stop data duplication
            &#39;&#39;&#39;
            if start_idx:
                lst_of_lst = ranges(
                    indices[-1]+1, n_workers, start_idx=start_idx)
            else:
                lst_of_lst = ranges(len(indices), n_workers)
            print(lst_of_lst)
            headless = [open_headless for i in lst_of_lst]
            proxy = [open_with_proxy_server for i in lst_of_lst]
            rand_proxy = [randomize_proxy_usage for i in lst_of_lst]
            product_meta_data = [[] for i in lst_of_lst]
            with concurrent.futures.ThreadPoolExecutor() as executor:
                &#39;&#39;&#39;
                # but each of the function namespace will be modifying only one metadata tracing file so that progress saving
                # is tracked correctly. else multiple progress tracker file will be created with difficulty to combine correct
                # progress information
                &#39;&#39;&#39;
                executor.map(self.get_metadata, lst_of_lst,
                             headless, proxy, rand_proxy, product_meta_data)

    if compile_progress_files:
        self.logger.info(&#39;Creating Combined Metadata File&#39;)
        files = [f for f in self.current_progress_path.glob(
            &#34;sph_prod_meta_extract_progress_*&#34;)]
        li = [pd.read_feather(file) for file in files]
        metadata_df = pd.concat(li, axis=0, ignore_index=True)
        metadata_df.reset_index(inplace=True, drop=True)
        metadata_df[&#39;source&#39;] = self.source

        if datetime.now().day &lt; 15:
            meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
        else:
            meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;
        filename = f&#39;sph_product_metadata_all_{meta_date}&#39;
        metadata_df.to_feather(self.metadata_path/filename)

        self.logger.info(
            f&#39;Metadata file created. Please look for file {filename} in path {self.metadata_path}&#39;)
        print(
            f&#39;Metadata file created. Please look for file {filename} in path {self.metadata_path}&#39;)

        if clean:
            cleaner = Cleaner(path=self.path)
            _ = cleaner.clean(
                data=self.metadata_path/filename)
            self.logger.info(
                &#39;Metadata Cleaned and Removed Duplicates for Details/Review/Image Extraction.&#39;)

        if delete_progress:
            shutil.rmtree(
                f&#39;{self.metadata_path}\\current_progress&#39;, ignore_errors=True)
            self.logger.info(&#39;Progress files deleted&#39;)</code></pre>
</details>
</dd>
<dt id="meiyume.sph.crawler.Metadata.get_metadata"><code class="name flex">
<span>def <span class="ident">get_metadata</span></span>(<span>self, indices: Union[list, range], open_headless: bool, open_with_proxy_server: bool, randomize_proxy_usage: bool, product_meta_data: list = []) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>get_metadata Crawls product listing pages for price, name, brand etc.</p>
<p>Get Metadata crawls a product type page for example lipstick.
The function gets individual product urls, names, brands and prices etc. and stores
in a relational table structure to use later to download product images, scrape reviews and
other specific information.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>indices</code></strong> :&ensp;<code>Union[list, range]</code></dt>
<dd>list of indices or range of indices of product urls to scrape.</dd>
<dt><strong><code>open_headless</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to open browser headless.</dd>
<dt><strong><code>open_with_proxy_server</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use proxy server.</dd>
<dt><strong><code>randomize_proxy_usage</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use both proxy and native network in tandem to decrease proxy requests.</dd>
<dt><strong><code>product_meta_data</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Empty intermediate list to store product metadata during parallel crawl. Defaults to [].</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_metadata(self, indices: Union[list, range],
                 open_headless: bool, open_with_proxy_server: bool,
                 randomize_proxy_usage: bool,
                 product_meta_data: list = []) -&gt; None:
    &#34;&#34;&#34;get_metadata Crawls product listing pages for price, name, brand etc.

    Get Metadata crawls a product type page for example lipstick.
    The function gets individual product urls, names, brands and prices etc. and stores
    in a relational table structure to use later to download product images, scrape reviews and
    other specific information.

    Args:
        indices (Union[list, range]): list of indices or range of indices of product urls to scrape.
        open_headless (bool): Whether to open browser headless.
        open_with_proxy_server (bool): Whether to use proxy server.
        randomize_proxy_usage (bool): Whether to use both proxy and native network in tandem to decrease proxy requests.
        product_meta_data (list, optional): Empty intermediate list to store product metadata during parallel crawl. Defaults to [].

    &#34;&#34;&#34;
    for pt in self.product_type_urls.index[self.product_type_urls.index.isin(indices)]:
        cat_name = self.product_type_urls.loc[pt, &#39;category_raw&#39;]
        product_type = self.product_type_urls.loc[pt, &#39;product_type&#39;]
        product_type_link = self.product_type_urls.loc[pt, &#39;url&#39;]

        self.progress_tracker.loc[pt, &#39;product_type&#39;] = product_type
        # print(product_type_link)
        if &#39;best-selling&#39; in product_type.lower() or &#39;new&#39; in product_type.lower():
            self.progress_tracker.loc[pt, &#39;scraped&#39;] = &#39;NA&#39;
            continue

        if randomize_proxy_usage:
            use_proxy = np.random.choice([True, False])
        else:
            use_proxy = True
        if open_with_proxy_server:
            # print(use_proxy)
            drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
                                    path=self.metadata_path)
        else:
            drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
                                    path=self.metadata_path)

        drv.get(product_type_link)
        time.sleep(15)  # 30
        accept_alert(drv, 10)
        close_popups(drv)

        try:
            chat_popup_button = WebDriverWait(drv, 3).until(
                EC.presence_of_element_located((By.XPATH, &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)))
            chat_popup_button = drv.find_element_by_xpath(
                &#39;//*[@id=&#34;divToky&#34;]/img[3]&#39;)
            self.scroll_to_element(drv, chat_popup_button)
            ActionChains(drv).move_to_element(
                chat_popup_button).click(chat_popup_button).perform()
        except TimeoutException:
            pass

        # sort by new products (required to get all new products properly)
        try:
            sort_dropdown = drv.find_element_by_css_selector(
                &#39;button[id=&#34;cat_sort_menu_trigger&#34;]&#39;)
            Browser().scroll_to_element(drv, sort_dropdown)
            ActionChains(drv).move_to_element(
                sort_dropdown).click(sort_dropdown).perform()
            drv.find_elements_by_css_selector(
                &#39;div[id=&#34;cat_sort_menu&#34;]&gt;button&#39;)[2].click()
            time.sleep(10)
            # sort_dropdown = drv.find_element_by_class_name(&#39;css-16tfpwn&#39;)
            # self.scroll_to_element(drv, sort_dropdown)
            # ActionChains(drv).move_to_element(
            #     sort_dropdown).click(sort_dropdown).perform()
            # button = drv.find_element_by_xpath(
            #     &#39;//*[@id=&#34;cat_sort_menu&#34;]/button[3]&#39;)
            # drv.implicitly_wait(4)
            # self.scroll_to_element(drv, button)
            # ActionChains(drv).move_to_element(
            #     button).click(button).perform()
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod Type: {product_type}&#39;)
            self.logger.info(str.encode(
                f&#39;Category: {cat_name} - ProductType {product_type} cannot sort by NEW.(page link: {product_type_link})&#39;,
                &#39;utf-8&#39;, &#39;ignore&#39;))
            pass

        # load all the products
        self.scroll_down_page(drv, h2=0.8, speed=3)
        time.sleep(8)

        # check whether on the first page of product type
        try:
            close_popups(drv)
            accept_alert(drv, 2)
            current_page = drv.find_element_by_class_name(
                &#39;css-g48inl&#39;).text
        except NoSuchElementException as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod Type: {product_type}&#39;)
            self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType {product_type} has\
            only one page of products.(page link: {product_type_link})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            one_page = True
            current_page = 1
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod Type: {product_type}&#39;)
            self.progress_tracker.loc[pt, &#39;scraped&#39;] = &#39;NA&#39;
            self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType {product_type}\
                 page not found.(page link: {product_type_link})&#39;,
                                        &#39;utf-8&#39;, &#39;ignore&#39;))
        else:
            # get a list of all available pages
            one_page = False
            # get next page button
            next_page_button = drv.find_element_by_css_selector(
                &#39;div&gt;nav&gt;ul&gt;button[aria-label=&#34;Next&#34;]&#39;)
            pages = []
            for page in drv.find_elements_by_class_name(&#39;css-1lk9n5p&#39;):
                pages.append(page.text)

        # start getting product form each page
        while True:
            cp = 0
            self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type}\
                              getting product from page {current_page}.(page link: {product_type_link})&#39;,
                                        &#39;utf-8&#39;, &#39;ignore&#39;))
            time.sleep(5)
            close_popups(drv)
            accept_alert(drv, 2)
            products = drv.find_elements_by_css_selector(
                &#39;div[data-comp=&#34;ProductGrid &#34;]&gt;div&gt;div&gt;a&#39;)
            # print(len(products))

            for p in products:
                time.sleep(0.5)

                close_popups(drv)
                accept_alert(drv, 0.5)

                self.scroll_to_element(drv, p)
                ActionChains(drv).move_to_element(p).perform()

                try:
                    product_name = p.get_attribute(&#39;aria-label&#39;)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod Type: {product_type}&#39;)
                    self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                 product {products.index(p)} metadata extraction failed.\
                                            (page_link: {product_type_link} - page_no: {current_page})&#39;,
                                                &#39;utf-8&#39;, &#39;ignore&#39;))
                    continue

                try:
                    new_f = p.find_element_by_css_selector(
                        &#39;div[data-at=&#34;product_badges&#34;]&#39;).text
                    product_new_flag = &#39;NEW&#39;
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod Type: {product_type}&#39;)
                    product_new_flag = &#39;&#39;
                    # self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                    #                              product {products.index(p)} product_new_flag extraction failed.\
                    #                         (page_link: {product_type_link} - page_no: {current_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                try:
                    product_page = p.get_attribute(&#39;href&#39;)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod Type: {product_type}&#39;)
                    product_page = &#39;&#39;
                    self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                 product {products.index(p)} product_page extraction failed.\
                                            (page_link: {product_type_link} - page_no: {current_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                try:
                    brand = p.find_element_by_css_selector(
                        &#39;span[data-at=&#34;sku_item_brand&#34;]&#39;).text
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod Type: {product_type}&#39;)
                    brand = &#39;&#39;
                    self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                 product {products.index(p)} brand extraction failed.\
                                            (page_link: {product_type_link} - page_no: {current_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                try:
                    rating = p.find_element_by_css_selector(
                        &#39;div[data-comp=&#34;StarRating &#34;]&#39;).get_attribute(&#39;aria-label&#39;)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod Type: {product_type}&#39;)
                    rating = &#39;&#39;
                    self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                 product {products.index(p)} rating extraction failed.\
                                            (page_link: {product_type_link} - page_no: {current_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                try:
                    price = p.find_element_by_css_selector(
                        &#39;span[data-at=&#34;sku_item_price_list&#34;]&#39;).text
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod Type: {product_type}&#39;)
                    price = &#39;&#39;
                    self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                                  product {products.index(p)} price extraction failed.\
                                            (page_link: {product_type_link} - page_no: {current_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))

                if datetime.now().day &lt; 15:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
                else:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;

                product_data_dict = {&#34;product_name&#34;: product_name, &#34;product_page&#34;: product_page, &#34;brand&#34;: brand, &#34;price&#34;: price,
                                     &#34;rating&#34;: rating, &#34;category&#34;: cat_name, &#34;product_type&#34;: product_type, &#34;new_flag&#34;: product_new_flag,
                                     &#34;complete_scrape_flag&#34;: &#34;N&#34;, &#34;meta_date&#34;: meta_date}
                cp += 1
                self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} -\
                                             Product: {product_name} - {cp} extracted successfully.\
                                            (page_link: {product_type_link} - page_no: {current_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                product_meta_data.append(product_data_dict)

            if one_page:
                break
            elif int(current_page) == int(pages[-1]):
                self.logger.info(str.encode(f&#39;Category: {cat_name} - ProductType: {product_type} extraction complete.\
                                            (page_link: {product_type_link} - page_no: {current_page})&#39;,
                                            &#39;utf-8&#39;, &#39;ignore&#39;))
                break
            else:
                # go to next page
                try:
                    self.scroll_to_element(drv, next_page_button)
                    ActionChains(drv).move_to_element(
                        next_page_button).click(next_page_button).perform()
                    time.sleep(15)
                    accept_alert(drv, 10)
                    close_popups(drv)
                    self.scroll_down_page(drv, h2=0.8, speed=3)
                    time.sleep(10)
                    current_page = drv.find_element_by_class_name(
                        &#39;css-g48inl&#39;).text
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod Type: {product_type}&#39;)
                    self.logger.info(str.encode(f&#39;Page navigation issue occurred for Category: {cat_name} - \
                                                    ProductType: {product_type} (page_link: {product_type_link} \
                                                    - page_no: {current_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                    break
        drv.quit()

        if len(product_meta_data) &gt; 0:
            product_meta_df = pd.DataFrame(product_meta_data)
            product_meta_df.to_feather(
                self.current_progress_path/f&#39;sph_prod_meta_extract_progress_{product_type}_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}&#39;)
            self.logger.info(
                f&#39;Completed till IndexPosition: {pt} - ProductType: {product_type}. (URL:{product_type_link})&#39;)
            self.progress_tracker.loc[pt, &#39;scraped&#39;] = &#39;Y&#39;
            self.progress_tracker.to_feather(
                self.metadata_path/&#39;sph_metadata_progress_tracker&#39;)
            product_meta_data = []
    self.logger.info(&#39;Metadata Extraction Complete&#39;)
    print(&#39;Metadata Extraction Complete&#39;)</code></pre>
</details>
</dd>
<dt id="meiyume.sph.crawler.Metadata.get_product_type_urls"><code class="name flex">
<span>def <span class="ident">get_product_type_urls</span></span>(<span>self, open_headless: bool, open_with_proxy_server: bool) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>get_product_type_urls Extract the category/subcategory structure and urls to extract the products of those category/subcategory.</p>
<p>Extracts the links of pages containing the list of all products structured into
category/subcategory/product type to effectively stored in relational database.
Defines the structure of data extraction that helps store unstructured data in a structured manner.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>open_headless</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to open browser headless.</dd>
<dt><strong><code>open_with_proxy_server</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use proxy server.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>returns pandas dataframe containing urls for getting list of products, category, subcategory etc.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_product_type_urls(self, open_headless: bool, open_with_proxy_server: bool) -&gt; pd.DataFrame:
    &#34;&#34;&#34;get_product_type_urls Extract the category/subcategory structure and urls to extract the products of those category/subcategory.

    Extracts the links of pages containing the list of all products structured into
    category/subcategory/product type to effectively stored in relational database.
    Defines the structure of data extraction that helps store unstructured data in a structured manner.

    Args:
        open_headless (bool): Whether to open browser headless.
        open_with_proxy_server (bool): Whether to use proxy server.

    Returns:
        pd.DataFrame: returns pandas dataframe containing urls for getting list of products, category, subcategory etc.

    &#34;&#34;&#34;
    # create webdriver instance
    drv = self.open_browser(
        open_headless=open_headless, open_with_proxy_server=open_with_proxy_server, path=self.metadata_path)

    drv.get(self.base_url)
    time.sleep(15)
    # click and close welcome forms
    accept_alert(drv, 10)
    close_popups(drv)

    cats = drv.find_elements_by_css_selector(&#39;a[id^=&#34;top_nav_drop_&#34;]&#39;)
    cat_urls = []
    for c in cats:
        if c.get_attribute(&#39;href&#39;) is not None:
            cat_name, url = (c.get_attribute(&#34;href&#34;).split(&#34;/&#34;)
                             [-1], c.get_attribute(&#34;href&#34;))
            cat_urls.append((cat_name, url))
            self.logger.info(str.encode(f&#39;Category:- name:{cat_name}, \
                                      url:{url}&#39;, &#34;utf-8&#34;, &#34;ignore&#34;))

    sub_cat_urls = []
    for cu in cat_urls:
        cat_name = cu[0]
        if cat_name in [&#39;brands-list&#39;, &#39;new-beauty-products&#39;, &#39;sephora-collection&#39;]:
            continue
        cat_url = cu[1]
        drv.get(cat_url)

        time.sleep(10)
        accept_alert(drv, 10)
        close_popups(drv)

        sub_cats = drv.find_elements_by_css_selector(
            &#39;a[data-at*=&#34;top_level_category&#34;]&#39;)
        # sub_cats.extend(drv.find_elements_by_class_name(&#34;css-or7ouu&#34;))
        if len(sub_cats) &gt; 0:
            for s in sub_cats:
                sub_cat_urls.append((cat_name, s.get_attribute(
                    &#34;href&#34;).split(&#34;/&#34;)[-1], s.get_attribute(&#34;href&#34;)))
                self.logger.info(str.encode(f&#39;SubCategory:- name:{s.get_attribute(&#34;href&#34;).split(&#34;/&#34;)[-1]},\
                                              url:{s.get_attribute(&#34;href&#34;)}&#39;, &#34;utf-8&#34;, &#34;ignore&#34;))
        else:
            sub_cat_urls.append(
                (cat_name, cat_url.split(&#39;/&#39;)[-1], cat_url))

    product_type_urls = []
    for su in sub_cat_urls:
        cat_name = su[0]
        sub_cat_name = su[1]
        if any(name in sub_cat_name for name in [&#39;best-selling&#39;, &#39;new&#39;, &#39;mini&#39;]):
            continue
        sub_cat_url = su[2]
        drv.get(sub_cat_url)

        time.sleep(10)
        accept_alert(drv, 10)
        close_popups(drv)

        product_types = drv.find_elements_by_css_selector(
            &#39;a[data-at*=&#34;nth_level&#34;]&#39;)
        if len(product_types) &gt; 0:
            for item in product_types:
                product_type_urls.append((cat_name, sub_cat_name, item.get_attribute(&#34;href&#34;).split(&#34;/&#34;)[-1],
                                          item.get_attribute(&#34;href&#34;)))
                self.logger.info(str.encode(f&#39;ProductType:- name:{item.get_attribute(&#34;href&#34;).split(&#34;/&#34;)[-1]},\
                                              url:{item.get_attribute(&#34;href&#34;)}&#39;, &#34;utf-8&#34;, &#34;ignore&#34;))
        else:
            product_type_urls.append(
                (cat_name, sub_cat_name, sub_cat_url.split(&#39;/&#39;)[-1], sub_cat_url))

    df = pd.DataFrame(product_type_urls, columns=[
                      &#39;category_raw&#39;, &#39;sub_category_raw&#39;, &#39;product_type&#39;, &#39;url&#39;])

    df_clean = pd.DataFrame(sub_cat_urls, columns=[
                            &#39;category_raw&#39;, &#39;product_type&#39;, &#39;url&#39;])
    df_clean[&#39;sub_category_raw&#39;] = &#39;CLEAN&#39;
    df_clean = df_clean[(df_clean.url.apply(
        lambda x: True if &#39;clean&#39; in x else False)) &amp; (df_clean.product_type != &#39;cleanser&#39;)]

    df_vegan = pd.DataFrame(sub_cat_urls, columns=[
                            &#39;category_raw&#39;, &#39;product_type&#39;, &#39;url&#39;])
    df_vegan[&#39;sub_category_raw&#39;] = &#39;VEGAN&#39;
    df_vegan = df_vegan[df_vegan.url.apply(
        lambda x: True if &#39;vegan&#39; in x.lower() else False)]

    df = pd.concat([df, df_clean, df_vegan], axis=0)
    df.reset_index(inplace=True, drop=True)
    df.to_feather(self.metadata_path/&#39;sph_product_cat_subcat_structure&#39;)
    drv.quit()

    df.drop_duplicates(subset=&#39;url&#39;, inplace=True)
    df.drop(columns=&#39;sub_category_raw&#39;, inplace=True)
    df.reset_index(inplace=True, drop=True)
    df[&#39;scraped&#39;] = &#39;N&#39;
    df.to_feather(self.metadata_path/f&#39;sph_product_type_urls_to_extract&#39;)
    return df</code></pre>
</details>
</dd>
<dt id="meiyume.sph.crawler.Metadata.terminate_logging"><code class="name flex">
<span>def <span class="ident">terminate_logging</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def terminate_logging(self):
    &#34;&#34;&#34;terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.
    &#34;&#34;&#34;
    self.logger.handlers.clear()
    self.prod_meta_log.stop_log()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="meiyume.utils.Sephora" href="../utils.html#meiyume.utils.Sephora">Sephora</a></b></code>:
<ul class="hlist">
<li><code><a title="meiyume.utils.Sephora.open_browser" href="../utils.html#meiyume.utils.Browser.open_browser">open_browser</a></code></li>
<li><code><a title="meiyume.utils.Sephora.open_browser_firefox" href="../utils.html#meiyume.utils.Browser.open_browser_firefox">open_browser_firefox</a></code></li>
<li><code><a title="meiyume.utils.Sephora.scroll_down_page" href="../utils.html#meiyume.utils.Browser.scroll_down_page">scroll_down_page</a></code></li>
<li><code><a title="meiyume.utils.Sephora.scroll_to_element" href="../utils.html#meiyume.utils.Browser.scroll_to_element">scroll_to_element</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="meiyume.sph.crawler.Review"><code class="flex name class">
<span>class <span class="ident">Review</span></span>
<span>(</span><span>log: bool = True, path: pathlib.Path = WindowsPath('D:/Amit/Meiyume/meiyume_master_source_codes'))</span>
</code></dt>
<dd>
<div class="desc"><p>Review extracts product reviews, review rating and user attributes from Sephora website.</p>
<p>The Review module utilizes product urls scraped by Metadata by opening the pages one by one and getting
reviews for each individual products.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>Sephora</code></strong> :&ensp;<code>[type]</code></dt>
<dd>Class that initializes folder paths and selenium webdriver for data scraping.</dd>
</dl>
<p><strong>init</strong> Review class instace initializer.</p>
<p>This method sets all the folder paths required for Review crawler to work.
If the paths does not exist the paths get automatically created depending on
current directory or provided directory.</p>
<h2 id="args_1">Args</h2>
<dl>
<dt><strong><code>log</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to create crawling exception and progess log. Defaults to True.</dd>
<dt><strong><code>path</code></strong> :&ensp;<code>Path</code>, optional</dt>
<dd>Folder path where the Review will be extracted. Defaults to current directory(Path.cwd()).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Review(Sephora):
    &#34;&#34;&#34;Review extracts product reviews, review rating and user attributes from Sephora website.

    The Review module utilizes product urls scraped by Metadata by opening the pages one by one and getting
    reviews for each individual products.

    Args:
        Sephora ([type]): Class that initializes folder paths and selenium webdriver for data scraping.
    &#34;&#34;&#34;

    def __init__(self, log: bool = True, path: Path = Path.cwd()):
        &#34;&#34;&#34;__init__ Review class instace initializer.

        This method sets all the folder paths required for Review crawler to work.
        If the paths does not exist the paths get automatically created depending on
        current directory or provided directory.

        Args:
            log (bool, optional): Whether to create crawling exception and progess log. Defaults to True.
            path (Path, optional): Folder path where the Review will be extracted. Defaults to current directory(Path.cwd()).
        &#34;&#34;&#34;
        super().__init__(path=path, data_def=&#39;review&#39;)
        self.path = path
        self.current_progress_path = self.review_path/&#39;current_progress&#39;
        self.current_progress_path.mkdir(parents=True, exist_ok=True)

        old_review_files = list(self.review_path.glob(
            &#39;sph_product_review_all*&#39;))
        for f in old_review_files:
            shutil.move(str(f), str(self.old_review_files_path))

        old_clean_review_files = os.listdir(self.review_clean_path)
        for f in old_clean_review_files:
            shutil.move(str(self.review_clean_path/f),
                        str(self.old_review_clean_files_path))
        if log:
            self.prod_review_log = Logger(
                &#34;sph_prod_review_extraction&#34;, path=self.crawl_log_path)
            self.logger, _ = self.prod_review_log.start_log()

    def get_reviews(self, indices: list, open_headless: bool, open_with_proxy_server: bool,
                    randomize_proxy_usage: bool,
                    review_data: list = [], incremental: bool = True):
        &#34;&#34;&#34;get_reviews Crawls individual product pages for review text, title, date user attributes etc.

        Args:
            indices (list): list of indices or range of indices of product urls to scrape.
            open_headless (bool): Whether to open browser headless.
            open_with_proxy_server (bool): Whether to use ip rotation service.
            randomize_proxy_usage (bool): Whether to use both proxy and native network in tandem to decrease proxy requests.
            review_data (list, optional): Empty intermediate list to store data during parallel crawl. Defaults to [].
            incremental (bool, optional): Whether to scrape reviews incrementally from last scraped review date. Defaults to True.
        &#34;&#34;&#34;
        def store_data_refresh_mem(review_data: list) -&gt; list:
            &#34;&#34;&#34;store_data_refresh_mem method stores crawled data in regular interval to free up system memory.

            Store data after each product&#39;s reviews are extracted to free up RAM.

            Args:
                review_data (list): List containing scraped Review data to store.

            Returns:
                list: Empty list to accumulate data from next product scraping.
            &#34;&#34;&#34;
            pd.DataFrame(review_data).to_csv(self.current_progress_path /
                                             f&#39;sph_prod_review_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                                             index=None)

            self.meta.to_csv(
                self.review_path/&#39;sph_review_progress_tracker.csv&#39;, index=None)
            return []

        for prod in self.meta.index[self.meta.index.isin(indices)]:
            if self.meta.loc[prod, &#39;review_scraped&#39;] in [&#39;Y&#39;, &#39;NA&#39;] or self.meta.loc[prod, &#39;review_scraped&#39;] is np.nan:
                continue
            prod_id = self.meta.loc[prod, &#39;prod_id&#39;]
            product_name = self.meta.loc[prod, &#39;product_name&#39;]
            product_page = self.meta.loc[prod, &#39;product_page&#39;]

            last_scraped_review_date = self.meta.loc[prod,
                                                     &#39;last_scraped_review_date&#39;]
            # print(last_scraped_review_date)

            if randomize_proxy_usage:
                use_proxy = np.random.choice([True, False])
            else:
                use_proxy = True
            if open_with_proxy_server:
                # print(use_proxy)
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
                                        path=self.detail_path)
                # drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
                #                                 path=self.detail_path)
            else:
                drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
                                        path=self.detail_path)
                # drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
                #                                 path=self.detail_path)

            drv.get(product_page)
            time.sleep(10)  # 30
            accept_alert(drv, 10)
            close_popups(drv)

            self.scroll_down_page(drv, speed=6, h2=0.6)
            time.sleep(5)

            try:
                close_popups(drv)
                accept_alert(drv, 1)
                no_of_reviews = int(drv.find_element_by_class_name(
                    &#39;css-ils4e4&#39;).text.split()[0])
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}&#39;)
                self.logger.info(str.encode(f&#39;Product: {product_name} prod_id: {prod_id} reviews extraction failed.\
                                              Either product has no reviews or not\
                                              available for sell currently.(page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                no_of_reviews = 0
                self.meta.loc[prod, &#39;review_scraped&#39;] = &#34;NA&#34;
                self.meta.to_csv(
                    self.review_path/&#39;sph_review_progress_tracker.csv&#39;, index=None)
                drv.quit()
                # print(&#39;in except - continue&#39;)
                continue

            # print(no_of_reviews)
            # drv.find_element_by_class_name(&#39;css-2rg6q7&#39;).click()
            if incremental and last_scraped_review_date != &#39;&#39;:
                for n in range(no_of_reviews//6):
                    if n &gt; 400:
                        break

                    time.sleep(0.4)
                    revs = drv.find_elements_by_class_name(
                        &#39;css-1kk8dps&#39;)[2:]

                    try:
                        if pd.to_datetime(convert_ago_to_date(revs[-1].find_element_by_class_name(&#39;css-1t84k9w&#39;).text),
                                          infer_datetime_format=True)\
                                &lt; pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                            # print(&#39;breaking incremental&#39;)
                            break
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        try:
                            if pd.to_datetime(convert_ago_to_date(revs[-2].find_element_by_class_name(&#39;css-1t84k9w&#39;).text),
                                              infer_datetime_format=True)\
                                    &lt; pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                                # print(&#39;breaking incremental&#39;)
                                break
                        except Exception as ex:
                            log_exception(self.logger,
                                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
                            self.logger.info(str.encode(f&#39;Product: {product_name} prod_id: {prod_id} \
                                                         last_scraped_review_date to current review date \
                                                         comparision failed.(page: {product_page})&#39;,
                                                        &#39;utf-8&#39;, &#39;ignore&#39;))
                            # print(&#39;in second except block&#39;)
                            continue
                    try:
                        show_more_review_button = drv.find_element_by_class_name(
                            &#39;css-xswy5p&#39;)
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}. Failed to get show more review button.&#39;)
                    else:
                        try:
                            self.scroll_to_element(
                                drv, show_more_review_button)
                            ActionChains(drv).move_to_element(
                                show_more_review_button).click(show_more_review_button).perform()
                        except Exception as ex:
                            log_exception(self.logger,
                                          additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)
                            accept_alert(drv, 1)
                            close_popups(drv)
                            try:
                                self.scroll_to_element(
                                    drv, show_more_review_button)
                                ActionChains(drv).move_to_element(
                                    show_more_review_button).click(show_more_review_button).perform()
                            except Exception as ex:
                                log_exception(self.logger,
                                              additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)

            else:
                # print(&#39;inside get all reviews&#39;)
                # 6 because for click sephora shows 6 reviews. additional 25 no. of clicks for buffer.
                for n in range(no_of_reviews//6+10):
                    &#39;&#39;&#39;
                    code will stop after getting 1800 reviews of one particular product
                    when crawling all reviews. By default it will get latest 1800 reviews.
                    then in subsequent incremental runs it will get al new reviews on weekly basis
                    &#39;&#39;&#39;
                    if n &gt;= 400:  # 200:
                        break
                    time.sleep(1)
                    # close any opened popups by escape
                    accept_alert(drv, 1)
                    close_popups(drv)
                    try:
                        show_more_review_button = drv.find_element_by_class_name(
                            &#39;css-xswy5p&#39;)
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}. Failed to get show more review button.&#39;)
                    else:
                        try:
                            self.scroll_to_element(
                                drv, show_more_review_button)
                            ActionChains(drv).move_to_element(
                                show_more_review_button).click(show_more_review_button).perform()
                        except Exception as ex:
                            log_exception(self.logger,
                                          additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)
                            accept_alert(drv, 1)
                            close_popups(drv)
                            try:
                                self.scroll_to_element(
                                    drv, show_more_review_button)
                                ActionChains(drv).move_to_element(
                                    show_more_review_button).click(show_more_review_button).perform()
                            except Exception as ex:
                                log_exception(self.logger,
                                              additional_information=f&#39;Prod ID: {prod_id}.\
                                                   Failed to click on show more review button.&#39;)
                                try:
                                    self.scroll_to_element(
                                        drv, show_more_review_button)
                                    ActionChains(drv).move_to_element(
                                        show_more_review_button).click(show_more_review_button).perform()
                                except Exception as ex:
                                    log_exception(self.logger,
                                                  additional_information=f&#39;Prod ID: {prod_id}.\
                                                   Failed to click on show more review button.&#39;)
                                    accept_alert(drv, 2)
                                    close_popups(drv)
                                    try:
                                        self.scroll_to_element(
                                            drv, show_more_review_button)
                                        ActionChains(drv).move_to_element(
                                            show_more_review_button).click(show_more_review_button).perform()
                                    except Exception as ex:
                                        log_exception(self.logger,
                                                      additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)
                                        if n &lt; (no_of_reviews//6):
                                            self.logger.info(str.encode(f&#39;Product: {product_name} - prod_id \
                                                {prod_id} breaking click next review loop.\
                                                                        [total_reviews:{no_of_reviews} loaded_reviews:{n}]\
                                                                        (page link: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                                            self.logger.info(str.encode(f&#39;Product: {product_name} - prod_id {prod_id} cant load all reviews.\
                                                                          Check click next 6 reviews\
                                                                          code section(page link: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                                        break

            accept_alert(drv, 2)
            close_popups(drv)

            product_reviews = drv.find_elements_by_class_name(
                &#39;css-1kk8dps&#39;)[2:]

            # print(&#39;starting extraction&#39;)
            r = 0
            for rev in product_reviews:
                accept_alert(drv, 0.5)
                close_popups(drv)
                self.scroll_to_element(drv, rev)
                ActionChains(drv).move_to_element(rev).perform()

                try:
                    try:
                        review_text = rev.find_element_by_class_name(
                            &#39;css-1jg2pb9&#39;).text
                    except NoSuchElementException:
                        review_text = rev.find_element_by_class_name(
                            &#39;css-429528&#39;).text
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_text. Skip review.&#39;)
                    continue

                try:
                    review_date = convert_ago_to_date(
                        rev.find_element_by_class_name(&#39;css-h2vfi1&#39;).text)
                    if pd.to_datetime(review_date, infer_datetime_format=True) &lt; \
                            pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                        continue
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_date.&#39;)
                    review_date = &#39;&#39;

                try:
                    review_title = rev.find_element_by_class_name(
                        &#39;css-1jfmule&#39;).text
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_title.&#39;)
                    review_title = &#39;&#39;

                try:
                    product_variant = rev.find_element_by_class_name(
                        &#39;css-1op1cn7&#39;).text
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to extract product_variant.&#39;)
                    product_variant = &#39;&#39;

                try:
                    user_rating = rev.find_element_by_class_name(
                        &#39;css-3z5ot7&#39;).get_attribute(&#39;aria-label&#39;)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to extract user_rating.&#39;)
                    user_rating = &#39;&#39;

                try:
                    user_attribute = [{&#39;_&#39;.join(u.lower().split()[0:-1]): u.lower().split()[-1]}
                                      for u in rev.find_element_by_class_name(&#39;css-ecreye&#39;).text.split(&#39;\n&#39;)]
                    # user_attribute = []
                    # for u in rev.find_elements_by_class_name(&#39;css-j5yt83&#39;):
                    #     user_attribute.append(
                    #         {&#39;_&#39;.join(u.text.lower().split()[0:-1]): u.text.lower().split()[-1]})
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to extract user_attribute.&#39;)
                    user_attribute = []

                try:
                    recommend = rev.find_element_by_class_name(
                        &#39;css-1tf5yph&#39;).text
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to extract recommend.&#39;)
                    recommend = &#39;&#39;

                try:
                    helpful = rev.find_element_by_class_name(&#39;css-b7zg5r&#39;).text
                    # helpful = []
                    # for h in rev.find_elements_by_class_name(&#39;css-39esqn&#39;):
                    #     helpful.append(h.text)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to extract helpful.&#39;)
                    helpful = &#39;&#39;

                review_data.append({&#39;prod_id&#39;: prod_id, &#39;product_name&#39;: product_name,
                                    &#39;user_attribute&#39;: user_attribute, &#39;product_variant&#39;: product_variant,
                                    &#39;review_title&#39;: review_title, &#39;review_text&#39;: review_text,
                                    &#39;review_rating&#39;: user_rating, &#39;recommend&#39;: recommend,
                                    &#39;review_date&#39;: review_date,   &#39;helpful&#39;: helpful})
            drv.quit()
            self.meta.loc[prod, &#39;review_scraped&#39;] = &#39;Y&#39;
            review_data = store_data_refresh_mem(review_data)
            if not incremental:
                self.logger.info(str.encode(
                    f&#39;Product_name: {product_name} prod_id:{prod_id} reviews extracted successfully.(total_reviews: {no_of_reviews}, \
                    extracted_reviews: {len(product_reviews)}, page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            else:
                self.logger.info(str.encode(
                    f&#39;Product_name: {product_name} prod_id:{prod_id} new reviews extracted successfully.\
                        (no_of_new_extracted_reviews: {len(product_reviews)},\
                         page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
        # save the final review file
        review_data = store_data_refresh_mem(review_data)

    def extract(self, metadata: Union[pd.DataFrame, str, Path], download: bool = True, n_workers: int = 5,
                fresh_start: bool = False, auto_fresh_start: bool = False, incremental: bool = True,
                open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = True,
                start_idx: Optional[int] = None, end_idx: Optional[int] = None, list_of_index=None,
                compile_progress_files: bool = False, clean: bool = True, delete_progress: bool = False) -&gt; None:
        &#34;&#34;&#34;extract method controls all properties of the spiders and runs multi-threaded web crawling.

        Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
        This method has four major functionality:
        * 1. Run the spider
        * 2. Store data in regular intervals to free up ram
        * 3. Compile all crawled data into one file.
        * 4. Clean and push cleaned data to S3 storage for further algorithmic processing.

        Args:
            metadata (Union[pd.DataFrame, str, Path]): Dataframe containing product specific url, name and id of the products to be scraped.
            download (bool, optional): Whether to crawl data from or compile crawled data into one file. Defaults to True.
            n_workers (int, optional): No. of parallel threads to run. Defaults to 5.
            fresh_start (bool, optional): Whether to continue last crawl job or start new one. Defaults to False.
            auto_fresh_start (bool, optional): Whether to automatically start a new crawl job if last job was finished. Defaults to False.
            incremental (bool, optional): Whether to scrape reviews incrementally from last scraped review date. Defaults to True.
            open_headless (bool, optional):  Whether to open browser headless. Defaults to False.
            open_with_proxy_server (bool, optional): Whether to use ip rotation service. Defaults to True.
            randomize_proxy_usage (bool, optional): Whether to use both proxy and native network in tandem to decrease proxy requests.
            Defaults to False.
            start_idx (Optional[int], optional): Starting index of the links to crawl. Defaults to None.
            end_idx (Optional[int], optional): Ending index of the links to crawl. Defaults to None.
            list_of_index ([type], optional): List of indices or range of indices of product urls to scrape. Defaults to None.
            compile_progress_files (bool, optional): Whether to combine crawled data into one file. Defaults to False.
            clean (bool, optional): Whether to clean the compiled data. Defaults to True.
            delete_progress (bool, optional): Whether to delete intermediate data after compilation into one file. Defaults to False.
        &#34;&#34;&#34;
        def fresh():
            &#34;&#34;&#34; If fresh_start is True, this function sets initial parameters for a fresh data crawl.
            &#34;&#34;&#34;
            if not isinstance(metadata, pd.core.frame.DataFrame):
                list_of_files = self.metadata_clean_path.glob(
                    &#39;no_cat_cleaned_sph_product_metadata_all*&#39;)
                self.meta = pd.read_feather(max(list_of_files, key=os.path.getctime))[
                    [&#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;, &#39;last_scraped_review_date&#39;]]
            else:
                self.meta = metadata[[
                    &#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;, &#39;last_scraped_review_date&#39;]]
            self.meta.last_scraped_review_date.fillna(&#39;&#39;, inplace=True)
            self.meta[&#39;review_scraped&#39;] = &#39;N&#39;

        if download:
            if fresh_start:
                fresh()
            else:
                if Path(self.review_path/&#39;sph_review_progress_tracker.csv&#39;).exists():
                    self.meta = pd.read_csv(
                        self.review_path/&#39;sph_review_progress_tracker.csv&#39;)
                    if sum(self.meta.review_scraped == &#39;N&#39;) == 0:
                        if auto_fresh_start:
                            fresh()
                            self.logger.info(
                                &#39;Last Run was Completed. Starting Fresh Extraction.&#39;)
                        else:
                            self.logger.info(
                                f&#39;Review extraction for this cycle is complete. Please check files in path: {self.review_path}&#39;)
                            print(
                                f&#39;Review extraction for this cycle is complete. Please check files in path: {self.review_path}&#39;)
                    else:
                        self.logger.info(
                            &#39;Continuing Review Extraction From Last Run.&#39;)
                else:
                    fresh()
                    self.logger.info(
                        &#39;Review Progress Tracker not found. Starting Fresh Extraction.&#39;)

            # set list or range of product indices to crawl
            if list_of_index:
                indices = list_of_index
            elif start_idx and end_idx is None:
                indices = range(start_idx, len(self.meta))
            elif start_idx is None and end_idx:
                indices = range(0, end_idx)
            elif start_idx is not None and end_idx is not None:
                indices = range(start_idx, end_idx)
            else:
                indices = range(len(self.meta))
            # print(indices)

            if list_of_index:
                self.get_reviews(
                    indices=list_of_index, incremental=incremental, open_headless=open_headless,
                    open_with_proxy_server=open_with_proxy_server, randomize_proxy_usage=randomize_proxy_usage)
            else:  # By default the code will with 5 concurrent threads. you can change this behaviour by changing n_workers
                &#39;&#39;&#39;
                # review_Data and item_data are lists of empty lists so that each namepace of function call will
                # have its separate detail_data
                # list to strore scraped dictionaries. will save memory(ram/hard-disk) consumption. will stop data duplication
                &#39;&#39;&#39;
                if start_idx:
                    lst_of_lst = ranges(
                        indices[-1]+1, n_workers, start_idx=start_idx)
                else:
                    lst_of_lst = ranges(len(indices), n_workers)
                print(lst_of_lst)
                # lst_of_lst2 = list(
                #     chunks(indices, len(indices)//n_workers))  # type: list

                # print(lst_of_lst, &#39;\n&#39;, lst_of_lst2)

                headless = [open_headless for i in lst_of_lst]
                proxy = [open_with_proxy_server for i in lst_of_lst]
                rand_proxy = [randomize_proxy_usage for i in lst_of_lst]
                review_data = [[] for i in lst_of_lst]  # type: list
                inc_list = [incremental for i in lst_of_lst]  # type: list
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    &#39;&#39;&#39;
                    # but each of the function namespace will be modifying only one metadata tracing file so that progress saving
                    # is tracked correctly. else multiple progress tracker file will be created with difficulty to combine correct
                    # progress information
                    &#39;&#39;&#39;
                    executor.map(self.get_reviews, lst_of_lst, headless, proxy,
                                 rand_proxy, review_data, inc_list)
        try:
            if compile_progress_files:
                self.logger.info(&#39;Creating Combined Review File&#39;)
                if datetime.now().day &lt; 15:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
                else:
                    meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;
                rev_li = []
                self.bad_rev_li = []
                review_files = [f for f in self.current_progress_path.glob(
                    &#34;sph_prod_review_extract_progress_*&#34;)]
                for file in review_files:
                    try:
                        df = pd.read_csv(file)
                    except Exception:
                        self.bad_rev_li.append(file)
                    else:
                        rev_li.append(df)
                rev_df = pd.concat(rev_li, axis=0, ignore_index=True)
                rev_df.drop_duplicates(inplace=True)
                rev_df.reset_index(inplace=True, drop=True)
                rev_df[&#39;meta_date&#39;] = pd.to_datetime(meta_date).date()
                review_filename = f&#39;sph_product_review_all_{pd.to_datetime(meta_date).date()}&#39;
                # , index=None)
                rev_df.to_feather(self.review_path/review_filename)

                self.logger.info(
                    f&#39;Review file created. Please look for file sph_product_review_all in path {self.review_path}&#39;)
                print(
                    f&#39;Review file created. Please look for file sph_product_review_all in path {self.review_path}&#39;)

                if clean:
                    cleaner = Cleaner(path=self.path)
                    self.review_clean_df = cleaner.clean(
                        self.review_path/review_filename)
                    file_creation_status = True
            else:
                file_creation_status = False
        except Exception as ex:
            log_exception(
                self.logger, additional_information=f&#39;Review Combined File Creation Failed.&#39;)
            file_creation_status = False

        if delete_progress and file_creation_status:
            shutil.rmtree(
                f&#39;{self.review_path}\\current_progress&#39;, ignore_errors=True)
            self.logger.info(&#39;Progress files deleted&#39;)

    def terminate_logging(self):
        &#34;&#34;&#34;terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.
        &#34;&#34;&#34;
        self.logger.handlers.clear()
        self.prod_review_log.stop_log()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="meiyume.utils.Sephora" href="../utils.html#meiyume.utils.Sephora">Sephora</a></li>
<li><a title="meiyume.utils.Browser" href="../utils.html#meiyume.utils.Browser">Browser</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="meiyume.sph.crawler.Review.extract"><code class="name flex">
<span>def <span class="ident">extract</span></span>(<span>self, metadata: Union[pandas.core.frame.DataFrame, str, pathlib.Path], download: bool = True, n_workers: int = 5, fresh_start: bool = False, auto_fresh_start: bool = False, incremental: bool = True, open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = True, start_idx: Union[int, NoneType] = None, end_idx: Union[int, NoneType] = None, list_of_index=None, compile_progress_files: bool = False, clean: bool = True, delete_progress: bool = False) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>extract method controls all properties of the spiders and runs multi-threaded web crawling.</p>
<p>Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
This method has four major functionality:
* 1. Run the spider
* 2. Store data in regular intervals to free up ram
* 3. Compile all crawled data into one file.
* 4. Clean and push cleaned data to S3 storage for further algorithmic processing.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>metadata</code></strong> :&ensp;<code>Union[pd.DataFrame, str, Path]</code></dt>
<dd>Dataframe containing product specific url, name and id of the products to be scraped.</dd>
<dt><strong><code>download</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to crawl data from or compile crawled data into one file. Defaults to True.</dd>
<dt><strong><code>n_workers</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>No. of parallel threads to run. Defaults to 5.</dd>
<dt><strong><code>fresh_start</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to continue last crawl job or start new one. Defaults to False.</dd>
<dt><strong><code>auto_fresh_start</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to automatically start a new crawl job if last job was finished. Defaults to False.</dd>
<dt><strong><code>incremental</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to scrape reviews incrementally from last scraped review date. Defaults to True.</dd>
<dt><strong><code>open_headless</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to open browser headless. Defaults to False.</dd>
<dt><strong><code>open_with_proxy_server</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use ip rotation service. Defaults to True.</dd>
<dt><strong><code>randomize_proxy_usage</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use both proxy and native network in tandem to decrease proxy requests.</dd>
<dt>Defaults to False.</dt>
<dt><strong><code>start_idx</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>Starting index of the links to crawl. Defaults to None.</dd>
<dt><strong><code>end_idx</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>Ending index of the links to crawl. Defaults to None.</dd>
<dt><strong><code>list_of_index</code></strong> :&ensp;<code>[type]</code>, optional</dt>
<dd>List of indices or range of indices of product urls to scrape. Defaults to None.</dd>
<dt><strong><code>compile_progress_files</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to combine crawled data into one file. Defaults to False.</dd>
<dt><strong><code>clean</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to clean the compiled data. Defaults to True.</dd>
<dt><strong><code>delete_progress</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to delete intermediate data after compilation into one file. Defaults to False.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract(self, metadata: Union[pd.DataFrame, str, Path], download: bool = True, n_workers: int = 5,
            fresh_start: bool = False, auto_fresh_start: bool = False, incremental: bool = True,
            open_headless: bool = False, open_with_proxy_server: bool = True, randomize_proxy_usage: bool = True,
            start_idx: Optional[int] = None, end_idx: Optional[int] = None, list_of_index=None,
            compile_progress_files: bool = False, clean: bool = True, delete_progress: bool = False) -&gt; None:
    &#34;&#34;&#34;extract method controls all properties of the spiders and runs multi-threaded web crawling.

    Extract is exposes all functionality of the spiders and user needs to run this method to begin data crawling from web.
    This method has four major functionality:
    * 1. Run the spider
    * 2. Store data in regular intervals to free up ram
    * 3. Compile all crawled data into one file.
    * 4. Clean and push cleaned data to S3 storage for further algorithmic processing.

    Args:
        metadata (Union[pd.DataFrame, str, Path]): Dataframe containing product specific url, name and id of the products to be scraped.
        download (bool, optional): Whether to crawl data from or compile crawled data into one file. Defaults to True.
        n_workers (int, optional): No. of parallel threads to run. Defaults to 5.
        fresh_start (bool, optional): Whether to continue last crawl job or start new one. Defaults to False.
        auto_fresh_start (bool, optional): Whether to automatically start a new crawl job if last job was finished. Defaults to False.
        incremental (bool, optional): Whether to scrape reviews incrementally from last scraped review date. Defaults to True.
        open_headless (bool, optional):  Whether to open browser headless. Defaults to False.
        open_with_proxy_server (bool, optional): Whether to use ip rotation service. Defaults to True.
        randomize_proxy_usage (bool, optional): Whether to use both proxy and native network in tandem to decrease proxy requests.
        Defaults to False.
        start_idx (Optional[int], optional): Starting index of the links to crawl. Defaults to None.
        end_idx (Optional[int], optional): Ending index of the links to crawl. Defaults to None.
        list_of_index ([type], optional): List of indices or range of indices of product urls to scrape. Defaults to None.
        compile_progress_files (bool, optional): Whether to combine crawled data into one file. Defaults to False.
        clean (bool, optional): Whether to clean the compiled data. Defaults to True.
        delete_progress (bool, optional): Whether to delete intermediate data after compilation into one file. Defaults to False.
    &#34;&#34;&#34;
    def fresh():
        &#34;&#34;&#34; If fresh_start is True, this function sets initial parameters for a fresh data crawl.
        &#34;&#34;&#34;
        if not isinstance(metadata, pd.core.frame.DataFrame):
            list_of_files = self.metadata_clean_path.glob(
                &#39;no_cat_cleaned_sph_product_metadata_all*&#39;)
            self.meta = pd.read_feather(max(list_of_files, key=os.path.getctime))[
                [&#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;, &#39;last_scraped_review_date&#39;]]
        else:
            self.meta = metadata[[
                &#39;prod_id&#39;, &#39;product_name&#39;, &#39;product_page&#39;, &#39;meta_date&#39;, &#39;last_scraped_review_date&#39;]]
        self.meta.last_scraped_review_date.fillna(&#39;&#39;, inplace=True)
        self.meta[&#39;review_scraped&#39;] = &#39;N&#39;

    if download:
        if fresh_start:
            fresh()
        else:
            if Path(self.review_path/&#39;sph_review_progress_tracker.csv&#39;).exists():
                self.meta = pd.read_csv(
                    self.review_path/&#39;sph_review_progress_tracker.csv&#39;)
                if sum(self.meta.review_scraped == &#39;N&#39;) == 0:
                    if auto_fresh_start:
                        fresh()
                        self.logger.info(
                            &#39;Last Run was Completed. Starting Fresh Extraction.&#39;)
                    else:
                        self.logger.info(
                            f&#39;Review extraction for this cycle is complete. Please check files in path: {self.review_path}&#39;)
                        print(
                            f&#39;Review extraction for this cycle is complete. Please check files in path: {self.review_path}&#39;)
                else:
                    self.logger.info(
                        &#39;Continuing Review Extraction From Last Run.&#39;)
            else:
                fresh()
                self.logger.info(
                    &#39;Review Progress Tracker not found. Starting Fresh Extraction.&#39;)

        # set list or range of product indices to crawl
        if list_of_index:
            indices = list_of_index
        elif start_idx and end_idx is None:
            indices = range(start_idx, len(self.meta))
        elif start_idx is None and end_idx:
            indices = range(0, end_idx)
        elif start_idx is not None and end_idx is not None:
            indices = range(start_idx, end_idx)
        else:
            indices = range(len(self.meta))
        # print(indices)

        if list_of_index:
            self.get_reviews(
                indices=list_of_index, incremental=incremental, open_headless=open_headless,
                open_with_proxy_server=open_with_proxy_server, randomize_proxy_usage=randomize_proxy_usage)
        else:  # By default the code will with 5 concurrent threads. you can change this behaviour by changing n_workers
            &#39;&#39;&#39;
            # review_Data and item_data are lists of empty lists so that each namepace of function call will
            # have its separate detail_data
            # list to strore scraped dictionaries. will save memory(ram/hard-disk) consumption. will stop data duplication
            &#39;&#39;&#39;
            if start_idx:
                lst_of_lst = ranges(
                    indices[-1]+1, n_workers, start_idx=start_idx)
            else:
                lst_of_lst = ranges(len(indices), n_workers)
            print(lst_of_lst)
            # lst_of_lst2 = list(
            #     chunks(indices, len(indices)//n_workers))  # type: list

            # print(lst_of_lst, &#39;\n&#39;, lst_of_lst2)

            headless = [open_headless for i in lst_of_lst]
            proxy = [open_with_proxy_server for i in lst_of_lst]
            rand_proxy = [randomize_proxy_usage for i in lst_of_lst]
            review_data = [[] for i in lst_of_lst]  # type: list
            inc_list = [incremental for i in lst_of_lst]  # type: list
            with concurrent.futures.ThreadPoolExecutor() as executor:
                &#39;&#39;&#39;
                # but each of the function namespace will be modifying only one metadata tracing file so that progress saving
                # is tracked correctly. else multiple progress tracker file will be created with difficulty to combine correct
                # progress information
                &#39;&#39;&#39;
                executor.map(self.get_reviews, lst_of_lst, headless, proxy,
                             rand_proxy, review_data, inc_list)
    try:
        if compile_progress_files:
            self.logger.info(&#39;Creating Combined Review File&#39;)
            if datetime.now().day &lt; 15:
                meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-01&#39;
            else:
                meta_date = f&#39;{time.strftime(&#34;%Y-%m&#34;)}-15&#39;
            rev_li = []
            self.bad_rev_li = []
            review_files = [f for f in self.current_progress_path.glob(
                &#34;sph_prod_review_extract_progress_*&#34;)]
            for file in review_files:
                try:
                    df = pd.read_csv(file)
                except Exception:
                    self.bad_rev_li.append(file)
                else:
                    rev_li.append(df)
            rev_df = pd.concat(rev_li, axis=0, ignore_index=True)
            rev_df.drop_duplicates(inplace=True)
            rev_df.reset_index(inplace=True, drop=True)
            rev_df[&#39;meta_date&#39;] = pd.to_datetime(meta_date).date()
            review_filename = f&#39;sph_product_review_all_{pd.to_datetime(meta_date).date()}&#39;
            # , index=None)
            rev_df.to_feather(self.review_path/review_filename)

            self.logger.info(
                f&#39;Review file created. Please look for file sph_product_review_all in path {self.review_path}&#39;)
            print(
                f&#39;Review file created. Please look for file sph_product_review_all in path {self.review_path}&#39;)

            if clean:
                cleaner = Cleaner(path=self.path)
                self.review_clean_df = cleaner.clean(
                    self.review_path/review_filename)
                file_creation_status = True
        else:
            file_creation_status = False
    except Exception as ex:
        log_exception(
            self.logger, additional_information=f&#39;Review Combined File Creation Failed.&#39;)
        file_creation_status = False

    if delete_progress and file_creation_status:
        shutil.rmtree(
            f&#39;{self.review_path}\\current_progress&#39;, ignore_errors=True)
        self.logger.info(&#39;Progress files deleted&#39;)</code></pre>
</details>
</dd>
<dt id="meiyume.sph.crawler.Review.get_reviews"><code class="name flex">
<span>def <span class="ident">get_reviews</span></span>(<span>self, indices: list, open_headless: bool, open_with_proxy_server: bool, randomize_proxy_usage: bool, review_data: list = [], incremental: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>get_reviews Crawls individual product pages for review text, title, date user attributes etc.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>indices</code></strong> :&ensp;<code>list</code></dt>
<dd>list of indices or range of indices of product urls to scrape.</dd>
<dt><strong><code>open_headless</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to open browser headless.</dd>
<dt><strong><code>open_with_proxy_server</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use ip rotation service.</dd>
<dt><strong><code>randomize_proxy_usage</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use both proxy and native network in tandem to decrease proxy requests.</dd>
<dt><strong><code>review_data</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Empty intermediate list to store data during parallel crawl. Defaults to [].</dd>
<dt><strong><code>incremental</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to scrape reviews incrementally from last scraped review date. Defaults to True.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_reviews(self, indices: list, open_headless: bool, open_with_proxy_server: bool,
                randomize_proxy_usage: bool,
                review_data: list = [], incremental: bool = True):
    &#34;&#34;&#34;get_reviews Crawls individual product pages for review text, title, date user attributes etc.

    Args:
        indices (list): list of indices or range of indices of product urls to scrape.
        open_headless (bool): Whether to open browser headless.
        open_with_proxy_server (bool): Whether to use ip rotation service.
        randomize_proxy_usage (bool): Whether to use both proxy and native network in tandem to decrease proxy requests.
        review_data (list, optional): Empty intermediate list to store data during parallel crawl. Defaults to [].
        incremental (bool, optional): Whether to scrape reviews incrementally from last scraped review date. Defaults to True.
    &#34;&#34;&#34;
    def store_data_refresh_mem(review_data: list) -&gt; list:
        &#34;&#34;&#34;store_data_refresh_mem method stores crawled data in regular interval to free up system memory.

        Store data after each product&#39;s reviews are extracted to free up RAM.

        Args:
            review_data (list): List containing scraped Review data to store.

        Returns:
            list: Empty list to accumulate data from next product scraping.
        &#34;&#34;&#34;
        pd.DataFrame(review_data).to_csv(self.current_progress_path /
                                         f&#39;sph_prod_review_extract_progress_{time.strftime(&#34;%Y-%m-%d-%H%M%S&#34;)}.csv&#39;,
                                         index=None)

        self.meta.to_csv(
            self.review_path/&#39;sph_review_progress_tracker.csv&#39;, index=None)
        return []

    for prod in self.meta.index[self.meta.index.isin(indices)]:
        if self.meta.loc[prod, &#39;review_scraped&#39;] in [&#39;Y&#39;, &#39;NA&#39;] or self.meta.loc[prod, &#39;review_scraped&#39;] is np.nan:
            continue
        prod_id = self.meta.loc[prod, &#39;prod_id&#39;]
        product_name = self.meta.loc[prod, &#39;product_name&#39;]
        product_page = self.meta.loc[prod, &#39;product_page&#39;]

        last_scraped_review_date = self.meta.loc[prod,
                                                 &#39;last_scraped_review_date&#39;]
        # print(last_scraped_review_date)

        if randomize_proxy_usage:
            use_proxy = np.random.choice([True, False])
        else:
            use_proxy = True
        if open_with_proxy_server:
            # print(use_proxy)
            drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
                                    path=self.detail_path)
            # drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=use_proxy,
            #                                 path=self.detail_path)
        else:
            drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
                                    path=self.detail_path)
            # drv = self.open_browser(open_headless=open_headless, open_with_proxy_server=False,
            #                                 path=self.detail_path)

        drv.get(product_page)
        time.sleep(10)  # 30
        accept_alert(drv, 10)
        close_popups(drv)

        self.scroll_down_page(drv, speed=6, h2=0.6)
        time.sleep(5)

        try:
            close_popups(drv)
            accept_alert(drv, 1)
            no_of_reviews = int(drv.find_element_by_class_name(
                &#39;css-ils4e4&#39;).text.split()[0])
        except Exception as ex:
            log_exception(self.logger,
                          additional_information=f&#39;Prod ID: {prod_id}&#39;)
            self.logger.info(str.encode(f&#39;Product: {product_name} prod_id: {prod_id} reviews extraction failed.\
                                          Either product has no reviews or not\
                                          available for sell currently.(page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
            no_of_reviews = 0
            self.meta.loc[prod, &#39;review_scraped&#39;] = &#34;NA&#34;
            self.meta.to_csv(
                self.review_path/&#39;sph_review_progress_tracker.csv&#39;, index=None)
            drv.quit()
            # print(&#39;in except - continue&#39;)
            continue

        # print(no_of_reviews)
        # drv.find_element_by_class_name(&#39;css-2rg6q7&#39;).click()
        if incremental and last_scraped_review_date != &#39;&#39;:
            for n in range(no_of_reviews//6):
                if n &gt; 400:
                    break

                time.sleep(0.4)
                revs = drv.find_elements_by_class_name(
                    &#39;css-1kk8dps&#39;)[2:]

                try:
                    if pd.to_datetime(convert_ago_to_date(revs[-1].find_element_by_class_name(&#39;css-1t84k9w&#39;).text),
                                      infer_datetime_format=True)\
                            &lt; pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                        # print(&#39;breaking incremental&#39;)
                        break
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}&#39;)
                    try:
                        if pd.to_datetime(convert_ago_to_date(revs[-2].find_element_by_class_name(&#39;css-1t84k9w&#39;).text),
                                          infer_datetime_format=True)\
                                &lt; pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                            # print(&#39;breaking incremental&#39;)
                            break
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}&#39;)
                        self.logger.info(str.encode(f&#39;Product: {product_name} prod_id: {prod_id} \
                                                     last_scraped_review_date to current review date \
                                                     comparision failed.(page: {product_page})&#39;,
                                                    &#39;utf-8&#39;, &#39;ignore&#39;))
                        # print(&#39;in second except block&#39;)
                        continue
                try:
                    show_more_review_button = drv.find_element_by_class_name(
                        &#39;css-xswy5p&#39;)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to get show more review button.&#39;)
                else:
                    try:
                        self.scroll_to_element(
                            drv, show_more_review_button)
                        ActionChains(drv).move_to_element(
                            show_more_review_button).click(show_more_review_button).perform()
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)
                        accept_alert(drv, 1)
                        close_popups(drv)
                        try:
                            self.scroll_to_element(
                                drv, show_more_review_button)
                            ActionChains(drv).move_to_element(
                                show_more_review_button).click(show_more_review_button).perform()
                        except Exception as ex:
                            log_exception(self.logger,
                                          additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)

        else:
            # print(&#39;inside get all reviews&#39;)
            # 6 because for click sephora shows 6 reviews. additional 25 no. of clicks for buffer.
            for n in range(no_of_reviews//6+10):
                &#39;&#39;&#39;
                code will stop after getting 1800 reviews of one particular product
                when crawling all reviews. By default it will get latest 1800 reviews.
                then in subsequent incremental runs it will get al new reviews on weekly basis
                &#39;&#39;&#39;
                if n &gt;= 400:  # 200:
                    break
                time.sleep(1)
                # close any opened popups by escape
                accept_alert(drv, 1)
                close_popups(drv)
                try:
                    show_more_review_button = drv.find_element_by_class_name(
                        &#39;css-xswy5p&#39;)
                except Exception as ex:
                    log_exception(self.logger,
                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to get show more review button.&#39;)
                else:
                    try:
                        self.scroll_to_element(
                            drv, show_more_review_button)
                        ActionChains(drv).move_to_element(
                            show_more_review_button).click(show_more_review_button).perform()
                    except Exception as ex:
                        log_exception(self.logger,
                                      additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)
                        accept_alert(drv, 1)
                        close_popups(drv)
                        try:
                            self.scroll_to_element(
                                drv, show_more_review_button)
                            ActionChains(drv).move_to_element(
                                show_more_review_button).click(show_more_review_button).perform()
                        except Exception as ex:
                            log_exception(self.logger,
                                          additional_information=f&#39;Prod ID: {prod_id}.\
                                               Failed to click on show more review button.&#39;)
                            try:
                                self.scroll_to_element(
                                    drv, show_more_review_button)
                                ActionChains(drv).move_to_element(
                                    show_more_review_button).click(show_more_review_button).perform()
                            except Exception as ex:
                                log_exception(self.logger,
                                              additional_information=f&#39;Prod ID: {prod_id}.\
                                               Failed to click on show more review button.&#39;)
                                accept_alert(drv, 2)
                                close_popups(drv)
                                try:
                                    self.scroll_to_element(
                                        drv, show_more_review_button)
                                    ActionChains(drv).move_to_element(
                                        show_more_review_button).click(show_more_review_button).perform()
                                except Exception as ex:
                                    log_exception(self.logger,
                                                  additional_information=f&#39;Prod ID: {prod_id}. Failed to click on show more review button.&#39;)
                                    if n &lt; (no_of_reviews//6):
                                        self.logger.info(str.encode(f&#39;Product: {product_name} - prod_id \
                                            {prod_id} breaking click next review loop.\
                                                                    [total_reviews:{no_of_reviews} loaded_reviews:{n}]\
                                                                    (page link: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                                        self.logger.info(str.encode(f&#39;Product: {product_name} - prod_id {prod_id} cant load all reviews.\
                                                                      Check click next 6 reviews\
                                                                      code section(page link: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
                                    break

        accept_alert(drv, 2)
        close_popups(drv)

        product_reviews = drv.find_elements_by_class_name(
            &#39;css-1kk8dps&#39;)[2:]

        # print(&#39;starting extraction&#39;)
        r = 0
        for rev in product_reviews:
            accept_alert(drv, 0.5)
            close_popups(drv)
            self.scroll_to_element(drv, rev)
            ActionChains(drv).move_to_element(rev).perform()

            try:
                try:
                    review_text = rev.find_element_by_class_name(
                        &#39;css-1jg2pb9&#39;).text
                except NoSuchElementException:
                    review_text = rev.find_element_by_class_name(
                        &#39;css-429528&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_text. Skip review.&#39;)
                continue

            try:
                review_date = convert_ago_to_date(
                    rev.find_element_by_class_name(&#39;css-h2vfi1&#39;).text)
                if pd.to_datetime(review_date, infer_datetime_format=True) &lt; \
                        pd.to_datetime(last_scraped_review_date, infer_datetime_format=True):
                    continue
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_date.&#39;)
                review_date = &#39;&#39;

            try:
                review_title = rev.find_element_by_class_name(
                    &#39;css-1jfmule&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract review_title.&#39;)
                review_title = &#39;&#39;

            try:
                product_variant = rev.find_element_by_class_name(
                    &#39;css-1op1cn7&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract product_variant.&#39;)
                product_variant = &#39;&#39;

            try:
                user_rating = rev.find_element_by_class_name(
                    &#39;css-3z5ot7&#39;).get_attribute(&#39;aria-label&#39;)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract user_rating.&#39;)
                user_rating = &#39;&#39;

            try:
                user_attribute = [{&#39;_&#39;.join(u.lower().split()[0:-1]): u.lower().split()[-1]}
                                  for u in rev.find_element_by_class_name(&#39;css-ecreye&#39;).text.split(&#39;\n&#39;)]
                # user_attribute = []
                # for u in rev.find_elements_by_class_name(&#39;css-j5yt83&#39;):
                #     user_attribute.append(
                #         {&#39;_&#39;.join(u.text.lower().split()[0:-1]): u.text.lower().split()[-1]})
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract user_attribute.&#39;)
                user_attribute = []

            try:
                recommend = rev.find_element_by_class_name(
                    &#39;css-1tf5yph&#39;).text
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract recommend.&#39;)
                recommend = &#39;&#39;

            try:
                helpful = rev.find_element_by_class_name(&#39;css-b7zg5r&#39;).text
                # helpful = []
                # for h in rev.find_elements_by_class_name(&#39;css-39esqn&#39;):
                #     helpful.append(h.text)
            except Exception as ex:
                log_exception(self.logger,
                              additional_information=f&#39;Prod ID: {prod_id}. Failed to extract helpful.&#39;)
                helpful = &#39;&#39;

            review_data.append({&#39;prod_id&#39;: prod_id, &#39;product_name&#39;: product_name,
                                &#39;user_attribute&#39;: user_attribute, &#39;product_variant&#39;: product_variant,
                                &#39;review_title&#39;: review_title, &#39;review_text&#39;: review_text,
                                &#39;review_rating&#39;: user_rating, &#39;recommend&#39;: recommend,
                                &#39;review_date&#39;: review_date,   &#39;helpful&#39;: helpful})
        drv.quit()
        self.meta.loc[prod, &#39;review_scraped&#39;] = &#39;Y&#39;
        review_data = store_data_refresh_mem(review_data)
        if not incremental:
            self.logger.info(str.encode(
                f&#39;Product_name: {product_name} prod_id:{prod_id} reviews extracted successfully.(total_reviews: {no_of_reviews}, \
                extracted_reviews: {len(product_reviews)}, page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
        else:
            self.logger.info(str.encode(
                f&#39;Product_name: {product_name} prod_id:{prod_id} new reviews extracted successfully.\
                    (no_of_new_extracted_reviews: {len(product_reviews)},\
                     page: {product_page})&#39;, &#39;utf-8&#39;, &#39;ignore&#39;))
    # save the final review file
    review_data = store_data_refresh_mem(review_data)</code></pre>
</details>
</dd>
<dt id="meiyume.sph.crawler.Review.terminate_logging"><code class="name flex">
<span>def <span class="ident">terminate_logging</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def terminate_logging(self):
    &#34;&#34;&#34;terminate_logging ends log generation for the crawler and cleaner after the job finshes successfully.
    &#34;&#34;&#34;
    self.logger.handlers.clear()
    self.prod_review_log.stop_log()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="meiyume.utils.Sephora" href="../utils.html#meiyume.utils.Sephora">Sephora</a></b></code>:
<ul class="hlist">
<li><code><a title="meiyume.utils.Sephora.open_browser" href="../utils.html#meiyume.utils.Browser.open_browser">open_browser</a></code></li>
<li><code><a title="meiyume.utils.Sephora.open_browser_firefox" href="../utils.html#meiyume.utils.Browser.open_browser_firefox">open_browser_firefox</a></code></li>
<li><code><a title="meiyume.utils.Sephora.scroll_down_page" href="../utils.html#meiyume.utils.Browser.scroll_down_page">scroll_down_page</a></code></li>
<li><code><a title="meiyume.utils.Sephora.scroll_to_element" href="../utils.html#meiyume.utils.Browser.scroll_to_element">scroll_to_element</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="meiyume.sph" href="index.html">meiyume.sph</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="meiyume.sph.crawler.Detail" href="#meiyume.sph.crawler.Detail">Detail</a></code></h4>
<ul class="">
<li><code><a title="meiyume.sph.crawler.Detail.extract" href="#meiyume.sph.crawler.Detail.extract">extract</a></code></li>
<li><code><a title="meiyume.sph.crawler.Detail.get_detail" href="#meiyume.sph.crawler.Detail.get_detail">get_detail</a></code></li>
<li><code><a title="meiyume.sph.crawler.Detail.terminate_logging" href="#meiyume.sph.crawler.Detail.terminate_logging">terminate_logging</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="meiyume.sph.crawler.DetailReview" href="#meiyume.sph.crawler.DetailReview">DetailReview</a></code></h4>
<ul class="">
<li><code><a title="meiyume.sph.crawler.DetailReview.crawl_page" href="#meiyume.sph.crawler.DetailReview.crawl_page">crawl_page</a></code></li>
<li><code><a title="meiyume.sph.crawler.DetailReview.extract" href="#meiyume.sph.crawler.DetailReview.extract">extract</a></code></li>
<li><code><a title="meiyume.sph.crawler.DetailReview.get_details" href="#meiyume.sph.crawler.DetailReview.get_details">get_details</a></code></li>
<li><code><a title="meiyume.sph.crawler.DetailReview.get_reviews" href="#meiyume.sph.crawler.DetailReview.get_reviews">get_reviews</a></code></li>
<li><code><a title="meiyume.sph.crawler.DetailReview.terminate_logging" href="#meiyume.sph.crawler.DetailReview.terminate_logging">terminate_logging</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="meiyume.sph.crawler.Image" href="#meiyume.sph.crawler.Image">Image</a></code></h4>
<ul class="">
<li><code><a title="meiyume.sph.crawler.Image.extract" href="#meiyume.sph.crawler.Image.extract">extract</a></code></li>
<li><code><a title="meiyume.sph.crawler.Image.get_images" href="#meiyume.sph.crawler.Image.get_images">get_images</a></code></li>
<li><code><a title="meiyume.sph.crawler.Image.terminate_logging" href="#meiyume.sph.crawler.Image.terminate_logging">terminate_logging</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="meiyume.sph.crawler.Metadata" href="#meiyume.sph.crawler.Metadata">Metadata</a></code></h4>
<ul class="">
<li><code><a title="meiyume.sph.crawler.Metadata.base_url" href="#meiyume.sph.crawler.Metadata.base_url">base_url</a></code></li>
<li><code><a title="meiyume.sph.crawler.Metadata.extract" href="#meiyume.sph.crawler.Metadata.extract">extract</a></code></li>
<li><code><a title="meiyume.sph.crawler.Metadata.get_metadata" href="#meiyume.sph.crawler.Metadata.get_metadata">get_metadata</a></code></li>
<li><code><a title="meiyume.sph.crawler.Metadata.get_product_type_urls" href="#meiyume.sph.crawler.Metadata.get_product_type_urls">get_product_type_urls</a></code></li>
<li><code><a title="meiyume.sph.crawler.Metadata.info" href="#meiyume.sph.crawler.Metadata.info">info</a></code></li>
<li><code><a title="meiyume.sph.crawler.Metadata.source" href="#meiyume.sph.crawler.Metadata.source">source</a></code></li>
<li><code><a title="meiyume.sph.crawler.Metadata.terminate_logging" href="#meiyume.sph.crawler.Metadata.terminate_logging">terminate_logging</a></code></li>
<li><code><a title="meiyume.sph.crawler.Metadata.update_base_url" href="#meiyume.sph.crawler.Metadata.update_base_url">update_base_url</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="meiyume.sph.crawler.Review" href="#meiyume.sph.crawler.Review">Review</a></code></h4>
<ul class="">
<li><code><a title="meiyume.sph.crawler.Review.extract" href="#meiyume.sph.crawler.Review.extract">extract</a></code></li>
<li><code><a title="meiyume.sph.crawler.Review.get_reviews" href="#meiyume.sph.crawler.Review.get_reviews">get_reviews</a></code></li>
<li><code><a title="meiyume.sph.crawler.Review.terminate_logging" href="#meiyume.sph.crawler.Review.terminate_logging">terminate_logging</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>