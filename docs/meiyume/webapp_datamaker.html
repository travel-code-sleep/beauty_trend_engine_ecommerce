<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>meiyume.webapp_datamaker API documentation</title>
<meta name="description" content="[summary] …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>meiyume.webapp_datamaker</code></h1>
</header>
<section id="section-intro">
<p>[summary]</p>
<p>[extended_summary]</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>[type]</code></dt>
<dd>[description]</dd>
</dl>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34; [summary]

[extended_summary]

Returns:
    [type]: [description]
&#34;&#34;&#34;
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)

import concurrent.futures
import gc
import os
import time
import re
import warnings
from ast import literal_eval
from datetime import datetime, timedelta
from functools import reduce
from pathlib import Path
from ast import literal_eval
from typing import *

import numpy as np
import pandas as pd

from meiyume.utils import (Boots, Logger, MeiyumeException, ModelsAlgorithms,
                           RedShiftReader, S3FileManager, Sephora)

db = RedShiftReader()
file_manager = S3FileManager()

warnings.simplefilter(action=&#39;ignore&#39;)
np.random.seed(1337)


class RefreshData():
    &#34;&#34;&#34;RefreshData [summary]

    [extended_summary]
    &#34;&#34;&#34;

    def __init__(self, path=&#39;.&#39;):
        &#34;&#34;&#34;__init__ [summary]

        [extended_summary]

        Args:
            path (str, optional): [description]. Defaults to &#39;.&#39;.
        &#34;&#34;&#34;
        self.path = Path(path)
        self.sph = Sephora(path=self.path)
        self.bts = Boots(path=self.path)
        self.out = ModelsAlgorithms(path=self.path)
        self.dash_data_path = Path(r&#39;D:\Amit\Meiyume\meiyume_data\dash_data&#39;)
        self.landing_page_data = {}

    def refresh_market_trend_data(self):
        &#34;&#34;&#34;refresh_market_trend_data [summary]

        [extended_summary]
        &#34;&#34;&#34;
        # Get Metadata
        metadata = db.query_database(
            &#34;select prod_id, category, product_type, new_flag, meta_date from r_bte_meta_detail_f&#34;)
        metadata[&#39;source&#39;] = metadata.prod_id.apply(
            lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
        metadata.drop_duplicates(inplace=True)
        metadata.reset_index(inplace=True, drop=True)
        metadata.meta_date = metadata.meta_date.astype(&#39;datetime64[M]&#39;)

        meta_df = metadata[[&#39;prod_id&#39;, &#39;category&#39;,
                            &#39;product_type&#39;]].drop_duplicates(subset=&#39;prod_id&#39;)
        # Get Review Data
        reviews = db.query_database(
            &#34;select prod_id, review_date, is_influenced, review_text from r_bte_product_review_f&#34;)
        reviews.drop_duplicates(
            subset=[&#39;prod_id&#39;, &#39;review_text&#39;, &#39;review_date&#39;], inplace=True)
        reviews.drop(columns=&#39;review_text&#39;, inplace=True)

        meta_df.set_index(&#39;prod_id&#39;, inplace=True)
        reviews.set_index(&#39;prod_id&#39;, inplace=True)

        reviews = reviews.join(meta_df, how=&#39;left&#39;)
        reviews = reviews[~reviews.category.isna()]
        reviews = reviews[~reviews.review_date.isna()]

        del meta_df
        gc.collect()

        reviews.reset_index(inplace=True)
        reviews[&#39;month&#39;] = reviews[&#39;review_date&#39;].astype(&#39;datetime64[M]&#39;)
        reviews[&#39;source&#39;] = reviews.prod_id.apply(
            lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
        reviews[reviews.columns.difference([&#39;product_name&#39;])] \
            = reviews[reviews.columns.difference([&#39;product_name&#39;])]\
            .apply(lambda x: x.astype(str).str.lower() if(x.dtype == &#39;object&#39;) else x)
        reviews.drop(columns=[&#39;review_date&#39;], inplace=True)

        # Influenced Review Trend
        rev_by_marketing_cate_month = reviews[reviews.is_influenced == &#39;yes&#39;].groupby(
            by=[&#39;source&#39;, &#39;category&#39;, &#39;month&#39;]).prod_id.count().reset_index()
        # for col in [&#39;source&#39;, &#39;category&#39;]:
        #     rev_by_marketing_cate_month[col] = rev_by_marketing_cate_month[col].astype(
        #         &#39;category&#39;)
        rev_by_marketing_cate_month.rename(
            columns={&#39;prod_id&#39;: &#39;review_text&#39;}, inplace=True)
        rev_by_marketing_cate_month.to_feather(
            self.dash_data_path/&#39;review_trend_by_marketing_category_month&#39;)

        del rev_by_marketing_cate_month
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;review_trend_by_marketing_category_month&#39;,
            job_name=&#39;webapp&#39;)

        rev_by_marketing_ptype_month = reviews[reviews.is_influenced == &#39;yes&#39;].groupby(
            by=[&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;, &#39;month&#39;]).prod_id.count().reset_index()
        # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
        #     rev_by_marketing_ptype_month[col] = rev_by_marketing_ptype_month[col].astype(
        #         &#39;category&#39;)
        rev_by_marketing_ptype_month.rename(
            columns={&#39;prod_id&#39;: &#39;review_text&#39;}, inplace=True)
        rev_by_marketing_ptype_month.to_feather(
            self.dash_data_path/&#39;review_trend_by_marketing_product_type_month&#39;)

        del rev_by_marketing_ptype_month
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;review_trend_by_marketing_product_type_month&#39;,
            job_name=&#39;webapp&#39;)

        # Review Trend
        rev_by_cate_month = reviews.groupby(
            by=[&#39;source&#39;, &#39;category&#39;, &#39;month&#39;]).prod_id.count().reset_index()
        # for col in [&#39;source&#39;, &#39;category&#39;]:
        #     rev_by_cate_month[col] = rev_by_cate_month[col].astype(
        #         &#39;category&#39;)
        rev_by_cate_month.rename(
            columns={&#39;prod_id&#39;: &#39;review_text&#39;}, inplace=True)
        rev_by_cate_month.to_feather(
            self.dash_data_path/&#39;review_trend_category_month&#39;)

        del rev_by_cate_month
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;review_trend_category_month&#39;,
            job_name=&#39;webapp&#39;)

        rev_by_ptype_month = reviews.groupby(
            by=[&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;, &#39;month&#39;]).prod_id.count().reset_index()
        # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
        #     rev_by_ptype_month[col] = rev_by_ptype_month[col].astype(
        #         &#39;category&#39;)
        rev_by_ptype_month.rename(
            columns={&#39;prod_id&#39;: &#39;review_text&#39;}, inplace=True)
        rev_by_ptype_month.to_feather(
            self.dash_data_path/&#39;review_trend_product_type_month&#39;)

        del rev_by_ptype_month
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;review_trend_product_type_month&#39;,
            job_name=&#39;webapp&#39;)

        # Product Launch Trend
        meta_product_launces_trend_category_month = metadata[metadata.new_flag == &#39;new&#39;].groupby(
            by=[&#39;source&#39;, &#39;category&#39;, &#39;meta_date&#39;]).prod_id.count().reset_index()
        meta_product_launces_trend_category_month.rename(
            columns={&#39;prod_id&#39;: &#39;new_product_count&#39;}, inplace=True)
        # for col in [&#39;source&#39;, &#39;category&#39;]:
        #     meta_product_launces_trend_category_month[col] = meta_product_launces_trend_category_month[col].astype(
        #         &#39;category&#39;)
        meta_product_launces_trend_category_month.to_feather(
            self.dash_data_path/&#39;meta_product_launch_trend_category_month&#39;)

        del meta_product_launces_trend_category_month
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;meta_product_launch_trend_category_month&#39;,
            job_name=&#39;webapp&#39;)

        meta_product_launch_trend_product_type_df = metadata[metadata.new_flag ==
                                                             &#39;new&#39;].groupby(by=[&#39;source&#39;, &#39;category&#39;,
                                                                                &#39;product_type&#39;, &#39;meta_date&#39;]).prod_id.count().reset_index()
        meta_product_launch_trend_product_type_df.rename(
            columns={&#39;prod_id&#39;: &#39;new_product_count&#39;}, inplace=True)
        # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
        #     meta_product_launch_trend_product_type_df[col] = meta_product_launch_trend_product_type_df[col].astype(
        #         &#39;category&#39;)
        meta_product_launch_trend_product_type_df.to_feather(
            self.dash_data_path/&#39;meta_product_launch_trend_product_type_month&#39;)

        del meta_product_launch_trend_product_type_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;meta_product_launch_trend_product_type_month&#39;,
            job_name=&#39;webapp&#39;)

        metadata.new_flag[metadata.new_flag == &#39;&#39;] = &#39;old&#39;
        # this is awesome way to get in-class counts. reuse this everywhere
        product_launch_intensity_category_df = metadata.pivot_table(index=[&#39;source&#39;, &#39;category&#39;, &#39;meta_date&#39;],
                                                                    columns=&#39;new_flag&#39;, aggfunc=&#39;size&#39;, fill_value=0).reset_index()
        product_launch_intensity_category_df[&#39;product_count&#39;] = product_launch_intensity_category_df.new + \
            product_launch_intensity_category_df.old
        product_launch_intensity_category_df[&#39;launch_intensity&#39;] = round(
            product_launch_intensity_category_df.new/product_launch_intensity_category_df.product_count, 3)
        # for col in [&#39;source&#39;, &#39;category&#39;]:
        #     product_launch_intensity_category_df[col] = product_launch_intensity_category_df[col].astype(
        #         &#39;category&#39;)
        product_launch_intensity_category_df.to_feather(
            self.dash_data_path/&#39;meta_product_launch_intensity_category_month&#39;)

        del product_launch_intensity_category_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;meta_product_launch_intensity_category_month&#39;,
            job_name=&#39;webapp&#39;)

        # Ingredient Launch Trend
        ingredients = db.query_database(
            &#34;select prod_id, new_flag, meta_date, category, product_type from r_bte_product_ingredients_f&#34;)
        ingredients[&#39;source&#39;] = ingredients.prod_id.apply(
            lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
        new_ingredient_trend_category_df = ingredients[ingredients.new_flag == &#39;new_ingredient&#39;].groupby(
            by=[&#39;source&#39;, &#39;category&#39;, &#39;meta_date&#39;]).new_flag.count().reset_index()
        new_ingredient_trend_category_df.rename(
            columns={&#39;new_flag&#39;: &#39;new_ingredient_count&#39;}, inplace=True)
        # for col in [&#39;source&#39;, &#39;category&#39;]:
        #     new_ingredient_trend_category_df[col] = new_ingredient_trend_category_df[col].astype(
        #         &#39;category&#39;)
        new_ingredient_trend_category_df.to_feather(
            self.dash_data_path/&#39;new_ingredient_trend_category_month&#39;)

        del new_ingredient_trend_category_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;new_ingredient_trend_category_month&#39;,
            job_name=&#39;webapp&#39;)

        new_ingredient_trend_product_type_df = ingredients[ingredients.new_flag == &#39;new_ingredient&#39;].groupby(
            by=[&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;, &#39;meta_date&#39;]).new_flag.count().reset_index()
        new_ingredient_trend_product_type_df.rename(
            columns={&#39;new_flag&#39;: &#39;new_ingredient_count&#39;}, inplace=True)
        # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
        #     new_ingredient_trend_product_type_df[col] = new_ingredient_trend_product_type_df[col].astype(
        #         &#39;category&#39;)
        new_ingredient_trend_product_type_df.to_feather(
            self.dash_data_path/&#39;new_ingredient_trend_product_type_month&#39;)

        del new_ingredient_trend_product_type_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;new_ingredient_trend_product_type_month&#39;,
            job_name=&#39;webapp&#39;)

        del metadata, reviews, ingredients
        gc.collect()

    def refresh_category_page_data(self):
        &#34;&#34;&#34;refresh_category_page_data [summary]

        [extended_summary]
        &#34;&#34;&#34;
        metadata = db.query_database(
            &#34;select prod_id, product_name, brand, category, product_type, new_flag, meta_date, low_p, high_p,\
             mrp, reviews, bayesian_estimate, first_review_date from r_bte_meta_detail_f&#34;)
        numeric_cols = [&#39;low_p&#39;, &#39;high_p&#39;, &#39;mrp&#39;,
                        &#39;reviews&#39;, &#39;bayesian_estimate&#39;]
        metadata[numeric_cols] = metadata[numeric_cols].apply(
            pd.to_numeric, errors=&#39;coerce&#39;)
        metadata[&#39;source&#39;] = metadata.prod_id.apply(
            lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
        grouped = metadata.groupby([&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;])
        # Pricing Analysis Data
        pricing_analytics_df = grouped.agg(min_price=pd.NamedAgg(column=&#39;low_p&#39;, aggfunc=&#39;min&#39;),
                                           max_price=pd.NamedAgg(
                                               column=&#39;high_p&#39;, aggfunc=&#39;max&#39;),
                                           avg_low_price=pd.NamedAgg(
                                               column=&#39;low_p&#39;, aggfunc=&#39;mean&#39;),
                                           avg_high_price=pd.NamedAgg(column=&#39;high_p&#39;, aggfunc=&#39;mean&#39;)).reset_index()
        pricing_analytics_df[[&#39;min_price&#39;, &#39;max_price&#39;, &#39;avg_low_price&#39;, &#39;avg_high_price&#39;]] = pricing_analytics_df[[
            &#39;min_price&#39;, &#39;max_price&#39;, &#39;avg_low_price&#39;, &#39;avg_high_price&#39;]].apply(lambda x: round(x, 2), axis=1)

        # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
        #     pricing_analytics_df[col] = pricing_analytics_df[col].astype(
        #         &#39;category&#39;)
        pricing_analytics_df.to_feather(
            self.dash_data_path/&#39;category_page_pricing_data&#39;)

        del pricing_analytics_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;category_page_pricing_data&#39;,
            job_name=&#39;webapp&#39;)

        # Product Analysis Data
        cat_page_distinct_brands_products_df = grouped.agg(distinct_brands=pd.NamedAgg(column=&#39;brand&#39;, aggfunc=&#39;nunique&#39;),
                                                           distinct_products=pd.NamedAgg(column=&#39;prod_id&#39;, aggfunc=&#39;nunique&#39;)).reset_index()
        # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
        #     cat_page_distinct_brands_products_df[col] = cat_page_distinct_brands_products_df[col].astype(
        #         &#39;category&#39;)
        cat_page_distinct_brands_products_df.to_feather(
            self.dash_data_path/&#39;category_page_distinct_brands_products&#39;)

        del cat_page_distinct_brands_products_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;category_page_distinct_brands_products&#39;,
            job_name=&#39;webapp&#39;)

        # New Products Data
        cat_page_new_products_df = metadata[metadata.new_flag == &#39;new&#39;].groupby(
            [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]).new_flag.count().reset_index()
        cat_page_new_products_df = cat_page_new_products_df.rename(
            columns={&#39;new_flag&#39;: &#39;new_product_count&#39;})
        # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
        #     cat_page_new_products_df[col] = cat_page_new_products_df[col].astype(
        #         &#39;category&#39;)
        cat_page_new_products_df.to_feather(
            self.dash_data_path/&#39;category_page_new_products_count&#39;)

        del cat_page_new_products_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;category_page_new_products_count&#39;,
            job_name=&#39;webapp&#39;)

        # Product Varieties and Price Data
        items = db.query_database(
            &#34;select prod_id, item_price, size_oz from r_bte_product_item_f&#34;)
        metadata.set_index(&#39;prod_id&#39;, inplace=True)
        items.set_index(&#39;prod_id&#39;, inplace=True)
        items = items.join(metadata[[&#39;category&#39;, &#39;product_type&#39;]], how=&#39;left&#39;)
        items.reset_index(inplace=True)
        metadata.reset_index(inplace=True)
        items[&#39;source&#39;] = items.prod_id.apply(
            lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
        df1 = items.groupby([&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]
                            ).agg(product_variations=pd.NamedAgg(column=&#39;prod_id&#39;, aggfunc=&#39;count&#39;)).reset_index()
        df2 = items.drop_duplicates(subset=[&#39;item_price&#39;, &#39;prod_id&#39;]
                                    ).groupby([&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]
                                              ).agg(avg_item_price=pd.NamedAgg(column=&#39;item_price&#39;, aggfunc=&#39;mean&#39;)).reset_index()
        cat_page_item_variations_price_df = pd.concat(
            [df1, df2[[&#39;avg_item_price&#39;]]], axis=1)
        del df1, df2
        gc.collect()
        cat_page_item_variations_price_df.avg_item_price = cat_page_item_variations_price_df.avg_item_price.apply(
            lambda x: round(x, 2))
        # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
        #     cat_page_item_variations_price_df[col] = cat_page_item_variations_price_df[col].astype(
        #         &#39;category&#39;)
        cat_page_item_variations_price_df.to_feather(
            self.dash_data_path/&#39;category_page_item_variations_price&#39;)

        del cat_page_item_variations_price_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;category_page_item_variations_price&#39;,
            job_name=&#39;webapp&#39;)

        # Packaging Analysis Data
        cat_page_item_package_oz_df = pd.DataFrame(items.drop_duplicates(subset=[&#39;size_oz&#39;, &#39;prod_id&#39;]
                                                                         ).groupby([&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]
                                                                                   ).size_oz.value_counts()
                                                   ).rename(columns={&#39;size_oz&#39;: &#39;product_count&#39;}).reset_index()
        cat_page_item_package_oz_df = cat_page_item_package_oz_df[
            cat_page_item_package_oz_df.size_oz != &#39;&#39;]
        cat_page_item_package_oz_df.rename(
            columns={&#39;size_oz&#39;: &#39;item_size&#39;}, inplace=True)

        def hasNumbers(inputString):
            return bool(re.search(r&#39;\d&#39;, inputString))

        cat_page_item_package_oz_df = cat_page_item_package_oz_df[cat_page_item_package_oz_df.item_size.apply(
            hasNumbers)]
        cat_page_item_package_oz_df = cat_page_item_package_oz_df[cat_page_item_package_oz_df.item_size.str.len(
        ) &lt; 30]
        cat_page_item_package_oz_df.item_size = cat_page_item_package_oz_df.item_size.str.replace(
            &#39;out of stock:&#39;, &#39;&#39;)
        cat_page_item_package_oz_df.reset_index(inplace=True, drop=True)
        # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
        #     cat_page_item_package_oz_df[col] = cat_page_item_package_oz_df[col].astype(
        #         &#39;category&#39;)
        cat_page_item_package_oz_df.to_feather(
            self.dash_data_path/&#39;category_page_item_package_oz&#39;)

        del cat_page_item_package_oz_df, items
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;category_page_item_package_oz&#39;,
            job_name=&#39;webapp&#39;)

        # Top Product Data
        cat_page_top_products_df = metadata.groupby([&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]
                                                    )[&#39;prod_id&#39;, &#39;brand&#39;, &#39;product_name&#39;,
                                                      &#39;bayesian_estimate&#39;, &#39;low_p&#39;, &#39;high_p&#39;, &#39;first_review_date&#39;].apply(
            lambda x: x.nlargest(20, columns=[&#39;bayesian_estimate&#39;])).reset_index()
        cat_page_top_products_df.drop(columns=[&#39;level_3&#39;], inplace=True)
        cat_page_top_products_df.rename(columns={&#39;bayesian_estimate&#39;: &#39;adjusted_rating&#39;,
                                                 &#34;low_p&#34;: &#39;small_size_price&#39;,
                                                 &#39;high_p&#39;: &#39;big_size_price&#39;}, inplace=True)
        cat_page_top_products_df.adjusted_rating = cat_page_top_products_df.adjusted_rating.apply(
            lambda x: round(x, 3))
        cat_page_top_products_df.small_size_price = cat_page_top_products_df.apply(lambda x: f&#39;${x.small_size_price}&#39;
                                                                                   if x.source == &#39;us&#39; else f&#39;£{x.small_size_price}&#39;, axis=1)
        cat_page_top_products_df.big_size_price = cat_page_top_products_df.apply(lambda x: f&#39;${x.big_size_price}&#39;
                                                                                 if x.source == &#39;us&#39; else f&#39;£{x.big_size_price}&#39;, axis=1)
        # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
        #     cat_page_top_products_df[col] = cat_page_top_products_df[col].astype(
        #         &#39;category&#39;)
        cat_page_top_products_df.to_feather(
            self.dash_data_path/&#39;category_page_top_products&#39;)

        del cat_page_top_products_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;category_page_top_products&#39;,
            job_name=&#39;webapp&#39;)

        # New Product Data
        cat_page_new_products_details_df = metadata[metadata.new_flag == &#39;new&#39;][[&#39;source&#39;, &#39;prod_id&#39;, &#39;product_name&#39;,
                                                                                 &#39;brand&#39;, &#39;category&#39;, &#39;product_type&#39;,
                                                                                 &#39;low_p&#39;, &#39;high_p&#39;, &#39;bayesian_estimate&#39;, &#39;reviews&#39;,
                                                                                 &#39;first_review_date&#39;]]
        cat_page_new_products_details_df.rename(columns={&#39;bayesian_estimate&#39;: &#39;adjusted_rating&#39;,
                                                         &#34;low_p&#34;: &#39;small_size_price&#39;,
                                                         &#39;high_p&#39;: &#39;big_size_price&#39;}, inplace=True)
        cat_page_new_products_details_df.small_size_price = cat_page_new_products_details_df.apply(
            lambda x: f&#39;${x.small_size_price}&#39;
            if x.source == &#39;us&#39; else f&#39;£{x.small_size_price}&#39;, axis=1)
        cat_page_new_products_details_df.big_size_price = cat_page_new_products_details_df.apply(
            lambda x: f&#39;${x.big_size_price}&#39;
            if x.source == &#39;us&#39; else f&#39;£{x.big_size_price}&#39;, axis=1)
        cat_page_new_products_details_df.adjusted_rating = cat_page_new_products_details_df.adjusted_rating.apply(
            lambda x: round(x, 3))
        cat_page_new_products_details_df.sort_values(
            by=&#39;adjusted_rating&#39;, inplace=True, ascending=False)
        cat_page_new_products_details_df.reset_index(drop=True, inplace=True)
        # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;, &#39;brand&#39;]:
        #     cat_page_new_products_details_df[col] = cat_page_new_products_details_df[col].astype(
        #         &#39;category&#39;)
        cat_page_new_products_details_df.to_feather(
            self.dash_data_path/&#39;category_page_new_products_details&#39;)

        del cat_page_new_products_details_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;category_page_new_products_details&#39;,
            job_name=&#39;webapp&#39;)

        # New Ingredients Data
        ingredients = db.query_database(&#34;select prod_id, ingredient, new_flag, ingredient_type, brand, meta_date, category,\
            product_name, product_type, ban_flag, bayesian_estimate from r_bte_product_ingredients_f&#34;)
        ingredients[&#39;source&#39;] = ingredients.prod_id.apply(
            lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
        cat_page_new_ingredients_df = ingredients[ingredients.new_flag == &#39;new_ingredient&#39;][
            [&#39;prod_id&#39;, &#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;, &#39;brand&#39;, &#39;product_name&#39;, &#39;ingredient&#39;,
             &#39;ingredient_type&#39;, &#39;bayesian_estimate&#39;, &#39;ban_flag&#39;]
        ]
        cat_page_new_ingredients_df.rename(
            columns={&#39;bayesian_estimate&#39;: &#39;adjusted_rating&#39;}, inplace=True)
        cat_page_new_ingredients_df.reset_index(inplace=True, drop=True)
        # for col in cat_page_new_ingredients_df.columns:
        #     if col not in [&#39;ingredient&#39;, &#39;adjusted_rating&#39;, &#39;ingredient&#39;]:
        #         cat_page_new_ingredients_df[col] = cat_page_new_ingredients_df[col].astype(
        #             &#39;category&#39;)
        cat_page_new_ingredients_df.to_feather(
            self.dash_data_path/&#39;category_page_new_ingredients&#39;)

        del cat_page_new_ingredients_df, ingredients
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;category_page_new_ingredients&#39;,
            job_name=&#39;webapp&#39;)

        # User Attribute Data
        reviews = db.query_database(
            &#34;select prod_id, age, eye_color, hair_color, skin_tone, review_text, skin_type, review_date from  r_bte_product_review_f&#34;)
        reviews.drop_duplicates(
            subset=[&#39;prod_id&#39;, &#39;review_text&#39;, &#39;review_date&#39;], inplace=True)
        reviews.drop(columns=&#39;review_text&#39;, inplace=True)
        metadata.set_index(&#39;prod_id&#39;, inplace=True)
        reviews.set_index(&#39;prod_id&#39;, inplace=True)
        reviews = reviews.join(
            metadata[[&#39;category&#39;, &#39;product_type&#39;]], how=&#39;left&#39;)
        reviews.reset_index(inplace=True)
        metadata.reset_index(inplace=True)
        reviews = reviews[~reviews.category.isna()]
        reviews = reviews[~reviews.review_date.isna()]
        reviews.drop_duplicates(inplace=True)
        reviews[&#39;month&#39;] = reviews[&#39;review_date&#39;].astype(&#39;datetime64[M]&#39;)
        reviews[&#39;source&#39;] = reviews.prod_id.apply(
            lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
        reviews.replace(&#39;none&#39;, &#39;&#39;, regex=True, inplace=True)
        cat_page_reviews_by_user_attributes = reviews[[&#39;prod_id&#39;, &#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;, &#39;age&#39;, &#39;eye_color&#39;,
                                                       &#39;hair_color&#39;, &#39;skin_tone&#39;, &#39;skin_type&#39;]
                                                      ][(reviews.age != &#39;&#39;) |
                                                        (reviews.eye_color != &#39;&#39;) |
                                                        (reviews.hair_color != &#39;&#39;) |
                                                        (reviews.skin_tone != &#39;&#39;) |
                                                        (reviews.skin_type != &#39;&#39;)
                                                        ].reset_index(drop=True)
        cat_page_reviews_by_user_attributes.drop(
            columns=&#39;prod_id&#39;, inplace=True)
        for col in cat_page_reviews_by_user_attributes.columns:
            cat_page_reviews_by_user_attributes[col] = cat_page_reviews_by_user_attributes[col].astype(
                &#39;category&#39;)
        cat_page_reviews_by_user_attributes.to_feather(
            self.dash_data_path/&#39;category_page_reviews_by_user_attributes&#39;)

        del cat_page_reviews_by_user_attributes, reviews, metadata
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;category_page_reviews_by_user_attributes&#39;,
            job_name=&#39;webapp&#39;)

    def refresh_product_page_data(self):
        &#34;&#34;&#34;refresh_product_page_data [summary]

        [extended_summary]
        &#34;&#34;&#34;
        # get metadata
        metadata = db.query_database(&#34;select prod_id, product_name, brand, category, product_type, new_flag, meta_date, low_p, high_p, \
                              mrp, reviews, bayesian_estimate, first_review_date from r_bte_meta_detail_f&#34;)
        metadata[&#39;source&#39;] = metadata.prod_id.apply(
            lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
        metadata[&#39;meta_date&#39;] = metadata[&#39;meta_date&#39;].astype(&#39;datetime64&#39;)

        # initialize landing page data dict
        self.landing_page_data[&#39;products&#39;] = metadata.prod_id.nunique()
        self.landing_page_data[&#39;brands&#39;] = metadata.brand.nunique()

        dt = metadata.groupby(&#39;source&#39;).meta_date.max(
        ).reset_index().to_dict(&#39;records&#39;)
        meta = []
        for i in dt:
            meta.append(metadata[(metadata.source == i[&#39;source&#39;]) &amp;
                                 (metadata.meta_date == i[&#39;meta_date&#39;])])
        metadata = pd.concat(meta, axis=0)
        metadata.fillna(&#39;&#39;, inplace=True)
        metadata.drop_duplicates(subset=&#39;prod_id&#39;, inplace=True)

        self.landing_page_data[&#39;latest_scraped_date&#39;] = metadata.meta_date.max().strftime(
            &#34;%d/%m/%Y&#34;)

        metadata[metadata.columns.difference([&#39;product_name&#39;, &#39;brand&#39;])] \
            = metadata[metadata.columns.difference([&#39;product_name&#39;, &#39;brand&#39;])]\
            .apply(lambda x: x.astype(str).str.lower() if(x.dtype == &#39;object&#39;) else x)

        # get review data
        reviews = db.query_database(
            &#34;select prod_id, review_date, sentiment, is_influenced, meta_date, age, eye_color,\
                 review_rating, hair_color, skin_tone, skin_type, review_text from r_bte_product_review_f&#34;)
        reviews.drop_duplicates(
            subset=[&#39;prod_id&#39;, &#39;review_text&#39;, &#39;review_date&#39;], inplace=True)
        reviews.drop(columns=&#39;review_text&#39;, inplace=True)

        self.landing_page_data[&#39;reviews&#39;] = reviews.shape[0]

        reviews.fillna(&#39;&#39;, inplace=True)

        first_review_date_df = reviews.groupby(
            &#39;prod_id&#39;).review_date.min().reset_index()

        metadata.set_index(&#39;prod_id&#39;, inplace=True)
        reviews.set_index(&#39;prod_id&#39;, inplace=True)
        first_review_date_df.set_index(&#39;prod_id&#39;, inplace=True)

        metadata = metadata.join(first_review_date_df, how=&#39;left&#39;)
        metadata.first_review_date = metadata.review_date
        metadata.drop(columns=&#39;review_date&#39;, inplace=True)

        del first_review_date_df
        gc.collect()

        reviews = reviews.join(
            metadata[[&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]], how=&#39;left&#39;)

        metadata.reset_index(inplace=True)
        reviews.reset_index(inplace=True)

        reviews = reviews[reviews.prod_id.isin(metadata.prod_id.tolist())]
        reviews[&#39;review_date&#39;] = reviews[&#39;review_date&#39;].astype(&#39;datetime64[M]&#39;)
        reviews[reviews.columns.difference([&#39;product_name&#39;])] \
            = reviews[reviews.columns.difference([&#39;product_name&#39;])]\
            .apply(lambda x: x.astype(str).str.lower() if(x.dtype == &#39;object&#39;) else x)
        reviews.reset_index(inplace=True, drop=True)

        # get review summary data
        review_sum = db.query_database(
            &#34;select prod_id, pos_review_summary, neg_review_summary, \
                pos_talking_points, neg_talking_points \
                    from r_bte_product_review_summary_f&#34;)
        review_sum = review_sum[review_sum.prod_id.isin(
            metadata.prod_id.tolist())]
        review_sum.reset_index(inplace=True, drop=True)

        # get ingredient data
        ingredients = db.query_database(&#34;select prod_id, product_name, brand, category, product_type, \
            ingredient, ingredient_type, new_flag, ban_flag \
                from r_bte_product_ingredients_f&#34;)
        ingredients = ingredients[ingredients.prod_id.isin(
            metadata.prod_id.tolist())]
        ingredients.reset_index(inplace=True, drop=True)
        ingredients.fillna(&#39;&#39;, inplace=True)
        ingredients[&#39;source&#39;] = ingredients.prod_id.apply(
            lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
        ingredients[ingredients.columns.difference([&#39;product_name&#39;, &#39;brand&#39;])] \
            = ingredients[ingredients.columns.difference([&#39;product_name&#39;, &#39;brand&#39;])]\
            .apply(lambda x: x.astype(str).str.lower() if(x.dtype == &#39;object&#39;) else x)

        # get item data
        items = db.query_database(&#34;select * from r_bte_product_item_f&#34;)
        items = items[items.prod_id.isin(metadata.prod_id.tolist())]
        items[&#39;meta_date&#39;] = items[&#39;meta_date&#39;].astype(&#39;datetime64[M]&#39;)
        items.fillna(&#39;&#39;, inplace=True)
        items.reset_index(inplace=True, drop=True)

        # Dropdown and Metadata Data
        prod_page_metadetail_data_df = metadata
        del metadata
        gc.collect()

        prod_page_metadetail_data_df.rename(columns={&#39;low_p&#39;: &#39;small_size_price&#39;, &#39;high_p&#39;: &#39;big_size_price&#39;,
                                                     &#39;bayesian_estimate&#39;: &#39;adjusted_rating&#39;}, inplace=True)
        prod_page_metadetail_data_df.adjusted_rating = prod_page_metadetail_data_df.adjusted_rating.apply(
            lambda x: round(float(x), 2) if x != &#39;&#39; else &#39;&#39;)
        prod_page_metadetail_data_df = prod_page_metadetail_data_df[
            prod_page_metadetail_data_df.small_size_price != &#39;&#39;]
        prod_page_metadetail_data_df.big_size_price = prod_page_metadetail_data_df.apply(
            lambda x: x.small_size_price if x.big_size_price == &#39;&#39; else x.big_size_price, axis=1)
        prod_page_metadetail_data_df.mrp = prod_page_metadetail_data_df.apply(
            lambda x: x.big_size_price if x.mrp == &#39;&#39; else x.mrp, axis=1)
        prod_page_metadetail_data_df.reset_index(inplace=True, drop=True)
        prod_page_metadetail_data_df.small_size_price = prod_page_metadetail_data_df.small_size_price.astype(
            float)
        prod_page_metadetail_data_df.big_size_price = prod_page_metadetail_data_df.big_size_price.astype(
            float)
        prod_page_metadetail_data_df.mrp = prod_page_metadetail_data_df.mrp.astype(
            str)
        prod_page_metadetail_data_df.adjusted_rating = prod_page_metadetail_data_df.adjusted_rating.astype(
            str)
        for col in [&#39;brand&#39;, &#39;category&#39;, &#39;product_type&#39;, &#39;new_flag&#39;, &#39;source&#39;]:
            prod_page_metadetail_data_df[col] = prod_page_metadetail_data_df[col].astype(
                &#39;category&#39;)
        prod_page_metadetail_data_df.drop_duplicates(inplace=True)
        prod_page_metadetail_data_df.reset_index(drop=True, inplace=True)
        prod_page_metadetail_data_df.to_feather(
            self.dash_data_path/&#39;product_page_metadetail_data&#39;)

        del prod_page_metadetail_data_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;product_page_metadetail_data&#39;,
            job_name=&#39;webapp&#39;)

        # Review Tab Data
        review_sum.fillna(&#34;{}&#34;, inplace=True)
        review_sum.pos_talking_points[review_sum.pos_talking_points == &#34;&#34;] = &#34;{}&#34;
        review_sum.neg_talking_points[review_sum.neg_talking_points == &#34;&#34;] = &#34;{}&#34;
        indices = []
        for i in review_sum.iterrows():
            try:
                d = literal_eval(i[1].pos_talking_points)
                d = literal_eval(i[1].neg_talking_points)
            except Exception as ex:
                indices.append(i[0])
        review_sum = review_sum[~review_sum.index.isin(
            indices)].reset_index(drop=True)
        review_sum.pos_talking_points = review_sum.pos_talking_points.apply(
            lambda x: literal_eval(x) if x != &#34;{}&#34; else {})
        review_sum.neg_talking_points = review_sum.neg_talking_points.apply(
            lambda x: literal_eval(x) if x != &#34;{}&#34; else {})
        # Review Summary Data
        prod_page_review_sum_df = review_sum[[
            &#39;prod_id&#39;, &#39;pos_review_summary&#39;, &#39;neg_review_summary&#39;]]

        prod_page_review_sum_df.drop_duplicates(inplace=True)
        prod_page_review_sum_df.reset_index(drop=True, inplace=True)
        prod_page_review_sum_df.to_feather(
            self.dash_data_path/&#39;prod_page_product_review_summary&#39;)

        del prod_page_review_sum_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;prod_page_product_review_summary&#39;,
            job_name=&#39;webapp&#39;)

        # Talking Points Data
        prod_page_review_talking_points_df = review_sum[[
            &#39;prod_id&#39;, &#39;pos_talking_points&#39;, &#39;neg_talking_points&#39;]]

        prod_page_review_talking_points_df.drop_duplicates(
            inplace=True, subset=&#39;prod_id&#39;)
        prod_page_review_talking_points_df.reset_index(drop=True, inplace=True)
        prod_page_review_talking_points_df.to_pickle(
            self.dash_data_path/&#39;prod_page_review_talking_points&#39;)

        del prod_page_review_talking_points_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;prod_page_review_talking_points&#39;,
            job_name=&#39;webapp&#39;)

        # Review Sentiment and Time Series Data
        reviews.is_influenced = reviews.is_influenced.str.lower()
        prod_page_review_sentiment_influence_df = reviews[[
            &#39;prod_id&#39;, &#39;sentiment&#39;, &#39;is_influenced&#39;, &#39;review_date&#39;, &#39;review_rating&#39;]]
        for col in [&#39;prod_id&#39;, &#39;sentiment&#39;, &#39;is_influenced&#39;, &#39;review_rating&#39;]:
            prod_page_review_sentiment_influence_df[col] = prod_page_review_sentiment_influence_df[col].astype(
                &#39;category&#39;)
        prod_page_review_sentiment_influence_df.review_date = prod_page_review_sentiment_influence_df.review_date.astype(
            &#39;datetime64[M]&#39;)

        prod_page_review_sentiment_influence_df.reset_index(
            drop=True, inplace=True)
        prod_page_review_sentiment_influence_df.to_feather(
            self.dash_data_path/&#39;prod_page_review_sentiment_influence&#39;)

        del prod_page_review_sentiment_influence_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;prod_page_review_sentiment_influence&#39;,
            job_name=&#39;webapp&#39;)

        # Review User Attribute Data
        prod_page_reviews_attribute_df = reviews[[&#39;prod_id&#39;, &#39;age&#39;, &#39;eye_color&#39;, &#39;hair_color&#39;, &#39;skin_tone&#39;, &#39;skin_type&#39;]][
            (reviews.age != &#39;&#39;) | (reviews.eye_color != &#39;&#39;) | (reviews.hair_color != &#39;&#39;) | (reviews.skin_tone != &#39;&#39;) | (reviews.skin_type != &#39;&#39;)]
        for col in prod_page_reviews_attribute_df.columns:
            prod_page_reviews_attribute_df[col] = prod_page_reviews_attribute_df[col].astype(
                &#39;category&#39;)
        prod_page_reviews_attribute_df.reset_index(inplace=True, drop=True)
        prod_page_reviews_attribute_df.to_feather(
            self.dash_data_path/&#39;prod_page_reviews_attribute&#39;)

        del prod_page_reviews_attribute_df, reviews
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;prod_page_reviews_attribute&#39;,
            job_name=&#39;webapp&#39;)

        # Pricing and Ingredients Tab Data
        prod_page_item_df = items[[&#39;prod_id&#39;, &#39;product_name&#39;,
                                   &#39;meta_date&#39;, &#39;item_name&#39;, &#39;item_price&#39;, &#39;size_oz&#39;]]
        prod_page_item_df.rename(
            columns={&#39;size_oz&#39;: &#39;item_size&#39;}, inplace=True)

        def hasNumbers(inputString):
            return bool(re.search(r&#39;\d&#39;, inputString))
        prod_page_item_df.item_size = prod_page_item_df.item_size.apply(
            lambda x: x if hasNumbers(x) else &#39;&#39;).str.replace(&#39;out of stock:&#39;, &#39;&#39;).str.replace(&#39;nan&#39;, &#39;&#39;)
        prod_page_item_df.item_size = prod_page_item_df.item_size.apply(
            lambda x: x if len(x) &lt; 30 else &#39;&#39;)
        prod_page_item_df.reset_index(inplace=True, drop=True)
        for col in [&#39;item_price&#39;, &#39;item_size&#39;]:
            prod_page_item_df[col] = prod_page_item_df[col].astype(&#39;category&#39;)
        prod_page_item_df.drop_duplicates(
            inplace=True, subset=[&#39;prod_id&#39;, &#39;item_size&#39;, &#39;item_name&#39;])
        prod_page_item_df.reset_index(inplace=True, drop=True)
        prod_page_item_df.to_feather(self.dash_data_path/&#39;prod_page_item_data&#39;)

        del prod_page_item_df, items
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;prod_page_item_data&#39;,
            job_name=&#39;webapp&#39;)

        # Ingredient Data
        prod_page_ing_df = ingredients[[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;ingredient&#39;, &#39;new_flag&#39;,
                                        &#39;ingredient_type&#39;, &#39;ban_flag&#39;,
                                        &#39;category&#39;, &#39;product_type&#39;, &#39;source&#39;]]
        for col in [&#39;prod_id&#39;, &#39;product_name&#39;, &#39;ingredient&#39;, &#39;new_flag&#39;,
                    &#39;ingredient_type&#39;, &#39;ban_flag&#39;,
                    &#39;category&#39;, &#39;product_type&#39;, &#39;source&#39;]:
            prod_page_ing_df[col] = prod_page_ing_df[col].astype(&#39;category&#39;)
        prod_page_ing_df.to_feather(self.dash_data_path/&#39;prod_page_ing_data&#39;)

        del prod_page_ing_df, ingredients
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;prod_page_ing_data&#39;,
            job_name=&#39;webapp&#39;)

    def refresh_ingredient_page_data(self):
        &#34;&#34;&#34;refresh_ingredient_page_data [summary]

        [extended_summary]
        &#34;&#34;&#34;
        # get ingredients
        ing_page_ing_df = db.query_database(
            &#34;select prod_id, ingredient, new_flag, ingredient_type, \
                category, product_name, product_type, vegan_flag, ban_flag \
                from r_bte_product_ingredients_f&#34;)
        ing_page_ing_df.fillna(&#39;&#39;, inplace=True)

        vegan_prods = ing_page_ing_df.prod_id[ing_page_ing_df.vegan_flag == &#39;vegan&#39;].unique(
        ).tolist()
        ing_page_ing_df.ingredient_type = ing_page_ing_df.apply(
            lambda x: &#39;vegan&#39; if x.ingredient_type == &#39;&#39; and x.prod_id in vegan_prods else x.ingredient_type, axis=1)
        ing_page_ing_df.drop(columns=[&#39;vegan_flag&#39;], inplace=True)

        ing_page_ing_df.drop_duplicates(
            subset=[&#39;ingredient&#39;, &#39;prod_id&#39;], inplace=True)
        ing_page_ing_df[&#39;source&#39;] = ing_page_ing_df.prod_id.apply(
            lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
        ing_page_ing_df[ing_page_ing_df.columns.difference([&#39;product_name&#39;, ])] \
            = ing_page_ing_df[ing_page_ing_df.columns.difference([&#39;product_name&#39;, ])]\
            .apply(lambda x: x.astype(str).str.lower() if(x.dtype == &#39;object&#39;) else x)

        self.landing_page_data[&#39;ingredients&#39;] = ing_page_ing_df.ingredient.nunique(
        )

        for col in [&#39;prod_id&#39;, &#39;ingredient&#39;, &#39;new_flag&#39;, &#39;ingredient_type&#39;,
                    &#39;category&#39;, &#39;product_name&#39;, &#39;product_type&#39;, &#39;source&#39;,
                    &#39;ban_flag&#39;]:
            ing_page_ing_df[col] = ing_page_ing_df[col].astype(&#39;category&#39;)

        ing_page_ing_df.reset_index(inplace=True, drop=True)
        ing_page_ing_df.to_feather(self.dash_data_path/&#39;ing_page_ing_data&#39;)

        del ing_page_ing_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;ing_page_ing_data&#39;,
            job_name=&#39;webapp&#39;)

    def refresh_landing_page_data(self):
        &#34;&#34;&#34;refresh_landing_page_data [summary]

        [extended_summary]
        &#34;&#34;&#34;
        image_keys = [i[&#39;Key&#39;] for i in file_manager.get_matching_s3_keys(prefix=&#39;Feeds/BeautyTrendEngine/Image/Staging/&#39;, suffix=&#39;jpg&#39;)
                      if any(job in i[&#39;Key&#39;].lower() for job in [&#39;image&#39;])]

        self.landing_page_data[&#39;images&#39;] = len(image_keys)

        pd.DataFrame(self.landing_page_data, index=range(0, 1)).to_feather(
            self.dash_data_path/&#39;landing_page_data&#39;)

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;landing_page_data&#39;,
            job_name=&#39;webapp&#39;)

    def make(self):
        &#34;&#34;&#34;make [summary]

        [extended_summary]
        &#34;&#34;&#34;
        try:
            self.refresh_market_trend_data()
            self.refresh_category_page_data()
            self.refresh_product_page_data()
            self.refresh_ingredient_page_data()
            self.refresh_landing_page_data()
        except Exception as ex:
            print(&#39;refresh job failed.&#39;)
        else:
            print(&#39;refresh job completed.&#39;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="meiyume.webapp_datamaker.RefreshData"><code class="flex name class">
<span>class <span class="ident">RefreshData</span></span>
<span>(</span><span>path='.')</span>
</code></dt>
<dd>
<div class="desc"><p>RefreshData [summary]</p>
<p>[extended_summary]</p>
<p><strong>init</strong> [summary]</p>
<p>[extended_summary]</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>[description]. Defaults to '.'.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RefreshData():
    &#34;&#34;&#34;RefreshData [summary]

    [extended_summary]
    &#34;&#34;&#34;

    def __init__(self, path=&#39;.&#39;):
        &#34;&#34;&#34;__init__ [summary]

        [extended_summary]

        Args:
            path (str, optional): [description]. Defaults to &#39;.&#39;.
        &#34;&#34;&#34;
        self.path = Path(path)
        self.sph = Sephora(path=self.path)
        self.bts = Boots(path=self.path)
        self.out = ModelsAlgorithms(path=self.path)
        self.dash_data_path = Path(r&#39;D:\Amit\Meiyume\meiyume_data\dash_data&#39;)
        self.landing_page_data = {}

    def refresh_market_trend_data(self):
        &#34;&#34;&#34;refresh_market_trend_data [summary]

        [extended_summary]
        &#34;&#34;&#34;
        # Get Metadata
        metadata = db.query_database(
            &#34;select prod_id, category, product_type, new_flag, meta_date from r_bte_meta_detail_f&#34;)
        metadata[&#39;source&#39;] = metadata.prod_id.apply(
            lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
        metadata.drop_duplicates(inplace=True)
        metadata.reset_index(inplace=True, drop=True)
        metadata.meta_date = metadata.meta_date.astype(&#39;datetime64[M]&#39;)

        meta_df = metadata[[&#39;prod_id&#39;, &#39;category&#39;,
                            &#39;product_type&#39;]].drop_duplicates(subset=&#39;prod_id&#39;)
        # Get Review Data
        reviews = db.query_database(
            &#34;select prod_id, review_date, is_influenced, review_text from r_bte_product_review_f&#34;)
        reviews.drop_duplicates(
            subset=[&#39;prod_id&#39;, &#39;review_text&#39;, &#39;review_date&#39;], inplace=True)
        reviews.drop(columns=&#39;review_text&#39;, inplace=True)

        meta_df.set_index(&#39;prod_id&#39;, inplace=True)
        reviews.set_index(&#39;prod_id&#39;, inplace=True)

        reviews = reviews.join(meta_df, how=&#39;left&#39;)
        reviews = reviews[~reviews.category.isna()]
        reviews = reviews[~reviews.review_date.isna()]

        del meta_df
        gc.collect()

        reviews.reset_index(inplace=True)
        reviews[&#39;month&#39;] = reviews[&#39;review_date&#39;].astype(&#39;datetime64[M]&#39;)
        reviews[&#39;source&#39;] = reviews.prod_id.apply(
            lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
        reviews[reviews.columns.difference([&#39;product_name&#39;])] \
            = reviews[reviews.columns.difference([&#39;product_name&#39;])]\
            .apply(lambda x: x.astype(str).str.lower() if(x.dtype == &#39;object&#39;) else x)
        reviews.drop(columns=[&#39;review_date&#39;], inplace=True)

        # Influenced Review Trend
        rev_by_marketing_cate_month = reviews[reviews.is_influenced == &#39;yes&#39;].groupby(
            by=[&#39;source&#39;, &#39;category&#39;, &#39;month&#39;]).prod_id.count().reset_index()
        # for col in [&#39;source&#39;, &#39;category&#39;]:
        #     rev_by_marketing_cate_month[col] = rev_by_marketing_cate_month[col].astype(
        #         &#39;category&#39;)
        rev_by_marketing_cate_month.rename(
            columns={&#39;prod_id&#39;: &#39;review_text&#39;}, inplace=True)
        rev_by_marketing_cate_month.to_feather(
            self.dash_data_path/&#39;review_trend_by_marketing_category_month&#39;)

        del rev_by_marketing_cate_month
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;review_trend_by_marketing_category_month&#39;,
            job_name=&#39;webapp&#39;)

        rev_by_marketing_ptype_month = reviews[reviews.is_influenced == &#39;yes&#39;].groupby(
            by=[&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;, &#39;month&#39;]).prod_id.count().reset_index()
        # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
        #     rev_by_marketing_ptype_month[col] = rev_by_marketing_ptype_month[col].astype(
        #         &#39;category&#39;)
        rev_by_marketing_ptype_month.rename(
            columns={&#39;prod_id&#39;: &#39;review_text&#39;}, inplace=True)
        rev_by_marketing_ptype_month.to_feather(
            self.dash_data_path/&#39;review_trend_by_marketing_product_type_month&#39;)

        del rev_by_marketing_ptype_month
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;review_trend_by_marketing_product_type_month&#39;,
            job_name=&#39;webapp&#39;)

        # Review Trend
        rev_by_cate_month = reviews.groupby(
            by=[&#39;source&#39;, &#39;category&#39;, &#39;month&#39;]).prod_id.count().reset_index()
        # for col in [&#39;source&#39;, &#39;category&#39;]:
        #     rev_by_cate_month[col] = rev_by_cate_month[col].astype(
        #         &#39;category&#39;)
        rev_by_cate_month.rename(
            columns={&#39;prod_id&#39;: &#39;review_text&#39;}, inplace=True)
        rev_by_cate_month.to_feather(
            self.dash_data_path/&#39;review_trend_category_month&#39;)

        del rev_by_cate_month
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;review_trend_category_month&#39;,
            job_name=&#39;webapp&#39;)

        rev_by_ptype_month = reviews.groupby(
            by=[&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;, &#39;month&#39;]).prod_id.count().reset_index()
        # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
        #     rev_by_ptype_month[col] = rev_by_ptype_month[col].astype(
        #         &#39;category&#39;)
        rev_by_ptype_month.rename(
            columns={&#39;prod_id&#39;: &#39;review_text&#39;}, inplace=True)
        rev_by_ptype_month.to_feather(
            self.dash_data_path/&#39;review_trend_product_type_month&#39;)

        del rev_by_ptype_month
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;review_trend_product_type_month&#39;,
            job_name=&#39;webapp&#39;)

        # Product Launch Trend
        meta_product_launces_trend_category_month = metadata[metadata.new_flag == &#39;new&#39;].groupby(
            by=[&#39;source&#39;, &#39;category&#39;, &#39;meta_date&#39;]).prod_id.count().reset_index()
        meta_product_launces_trend_category_month.rename(
            columns={&#39;prod_id&#39;: &#39;new_product_count&#39;}, inplace=True)
        # for col in [&#39;source&#39;, &#39;category&#39;]:
        #     meta_product_launces_trend_category_month[col] = meta_product_launces_trend_category_month[col].astype(
        #         &#39;category&#39;)
        meta_product_launces_trend_category_month.to_feather(
            self.dash_data_path/&#39;meta_product_launch_trend_category_month&#39;)

        del meta_product_launces_trend_category_month
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;meta_product_launch_trend_category_month&#39;,
            job_name=&#39;webapp&#39;)

        meta_product_launch_trend_product_type_df = metadata[metadata.new_flag ==
                                                             &#39;new&#39;].groupby(by=[&#39;source&#39;, &#39;category&#39;,
                                                                                &#39;product_type&#39;, &#39;meta_date&#39;]).prod_id.count().reset_index()
        meta_product_launch_trend_product_type_df.rename(
            columns={&#39;prod_id&#39;: &#39;new_product_count&#39;}, inplace=True)
        # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
        #     meta_product_launch_trend_product_type_df[col] = meta_product_launch_trend_product_type_df[col].astype(
        #         &#39;category&#39;)
        meta_product_launch_trend_product_type_df.to_feather(
            self.dash_data_path/&#39;meta_product_launch_trend_product_type_month&#39;)

        del meta_product_launch_trend_product_type_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;meta_product_launch_trend_product_type_month&#39;,
            job_name=&#39;webapp&#39;)

        metadata.new_flag[metadata.new_flag == &#39;&#39;] = &#39;old&#39;
        # this is awesome way to get in-class counts. reuse this everywhere
        product_launch_intensity_category_df = metadata.pivot_table(index=[&#39;source&#39;, &#39;category&#39;, &#39;meta_date&#39;],
                                                                    columns=&#39;new_flag&#39;, aggfunc=&#39;size&#39;, fill_value=0).reset_index()
        product_launch_intensity_category_df[&#39;product_count&#39;] = product_launch_intensity_category_df.new + \
            product_launch_intensity_category_df.old
        product_launch_intensity_category_df[&#39;launch_intensity&#39;] = round(
            product_launch_intensity_category_df.new/product_launch_intensity_category_df.product_count, 3)
        # for col in [&#39;source&#39;, &#39;category&#39;]:
        #     product_launch_intensity_category_df[col] = product_launch_intensity_category_df[col].astype(
        #         &#39;category&#39;)
        product_launch_intensity_category_df.to_feather(
            self.dash_data_path/&#39;meta_product_launch_intensity_category_month&#39;)

        del product_launch_intensity_category_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;meta_product_launch_intensity_category_month&#39;,
            job_name=&#39;webapp&#39;)

        # Ingredient Launch Trend
        ingredients = db.query_database(
            &#34;select prod_id, new_flag, meta_date, category, product_type from r_bte_product_ingredients_f&#34;)
        ingredients[&#39;source&#39;] = ingredients.prod_id.apply(
            lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
        new_ingredient_trend_category_df = ingredients[ingredients.new_flag == &#39;new_ingredient&#39;].groupby(
            by=[&#39;source&#39;, &#39;category&#39;, &#39;meta_date&#39;]).new_flag.count().reset_index()
        new_ingredient_trend_category_df.rename(
            columns={&#39;new_flag&#39;: &#39;new_ingredient_count&#39;}, inplace=True)
        # for col in [&#39;source&#39;, &#39;category&#39;]:
        #     new_ingredient_trend_category_df[col] = new_ingredient_trend_category_df[col].astype(
        #         &#39;category&#39;)
        new_ingredient_trend_category_df.to_feather(
            self.dash_data_path/&#39;new_ingredient_trend_category_month&#39;)

        del new_ingredient_trend_category_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;new_ingredient_trend_category_month&#39;,
            job_name=&#39;webapp&#39;)

        new_ingredient_trend_product_type_df = ingredients[ingredients.new_flag == &#39;new_ingredient&#39;].groupby(
            by=[&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;, &#39;meta_date&#39;]).new_flag.count().reset_index()
        new_ingredient_trend_product_type_df.rename(
            columns={&#39;new_flag&#39;: &#39;new_ingredient_count&#39;}, inplace=True)
        # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
        #     new_ingredient_trend_product_type_df[col] = new_ingredient_trend_product_type_df[col].astype(
        #         &#39;category&#39;)
        new_ingredient_trend_product_type_df.to_feather(
            self.dash_data_path/&#39;new_ingredient_trend_product_type_month&#39;)

        del new_ingredient_trend_product_type_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;new_ingredient_trend_product_type_month&#39;,
            job_name=&#39;webapp&#39;)

        del metadata, reviews, ingredients
        gc.collect()

    def refresh_category_page_data(self):
        &#34;&#34;&#34;refresh_category_page_data [summary]

        [extended_summary]
        &#34;&#34;&#34;
        metadata = db.query_database(
            &#34;select prod_id, product_name, brand, category, product_type, new_flag, meta_date, low_p, high_p,\
             mrp, reviews, bayesian_estimate, first_review_date from r_bte_meta_detail_f&#34;)
        numeric_cols = [&#39;low_p&#39;, &#39;high_p&#39;, &#39;mrp&#39;,
                        &#39;reviews&#39;, &#39;bayesian_estimate&#39;]
        metadata[numeric_cols] = metadata[numeric_cols].apply(
            pd.to_numeric, errors=&#39;coerce&#39;)
        metadata[&#39;source&#39;] = metadata.prod_id.apply(
            lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
        grouped = metadata.groupby([&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;])
        # Pricing Analysis Data
        pricing_analytics_df = grouped.agg(min_price=pd.NamedAgg(column=&#39;low_p&#39;, aggfunc=&#39;min&#39;),
                                           max_price=pd.NamedAgg(
                                               column=&#39;high_p&#39;, aggfunc=&#39;max&#39;),
                                           avg_low_price=pd.NamedAgg(
                                               column=&#39;low_p&#39;, aggfunc=&#39;mean&#39;),
                                           avg_high_price=pd.NamedAgg(column=&#39;high_p&#39;, aggfunc=&#39;mean&#39;)).reset_index()
        pricing_analytics_df[[&#39;min_price&#39;, &#39;max_price&#39;, &#39;avg_low_price&#39;, &#39;avg_high_price&#39;]] = pricing_analytics_df[[
            &#39;min_price&#39;, &#39;max_price&#39;, &#39;avg_low_price&#39;, &#39;avg_high_price&#39;]].apply(lambda x: round(x, 2), axis=1)

        # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
        #     pricing_analytics_df[col] = pricing_analytics_df[col].astype(
        #         &#39;category&#39;)
        pricing_analytics_df.to_feather(
            self.dash_data_path/&#39;category_page_pricing_data&#39;)

        del pricing_analytics_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;category_page_pricing_data&#39;,
            job_name=&#39;webapp&#39;)

        # Product Analysis Data
        cat_page_distinct_brands_products_df = grouped.agg(distinct_brands=pd.NamedAgg(column=&#39;brand&#39;, aggfunc=&#39;nunique&#39;),
                                                           distinct_products=pd.NamedAgg(column=&#39;prod_id&#39;, aggfunc=&#39;nunique&#39;)).reset_index()
        # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
        #     cat_page_distinct_brands_products_df[col] = cat_page_distinct_brands_products_df[col].astype(
        #         &#39;category&#39;)
        cat_page_distinct_brands_products_df.to_feather(
            self.dash_data_path/&#39;category_page_distinct_brands_products&#39;)

        del cat_page_distinct_brands_products_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;category_page_distinct_brands_products&#39;,
            job_name=&#39;webapp&#39;)

        # New Products Data
        cat_page_new_products_df = metadata[metadata.new_flag == &#39;new&#39;].groupby(
            [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]).new_flag.count().reset_index()
        cat_page_new_products_df = cat_page_new_products_df.rename(
            columns={&#39;new_flag&#39;: &#39;new_product_count&#39;})
        # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
        #     cat_page_new_products_df[col] = cat_page_new_products_df[col].astype(
        #         &#39;category&#39;)
        cat_page_new_products_df.to_feather(
            self.dash_data_path/&#39;category_page_new_products_count&#39;)

        del cat_page_new_products_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;category_page_new_products_count&#39;,
            job_name=&#39;webapp&#39;)

        # Product Varieties and Price Data
        items = db.query_database(
            &#34;select prod_id, item_price, size_oz from r_bte_product_item_f&#34;)
        metadata.set_index(&#39;prod_id&#39;, inplace=True)
        items.set_index(&#39;prod_id&#39;, inplace=True)
        items = items.join(metadata[[&#39;category&#39;, &#39;product_type&#39;]], how=&#39;left&#39;)
        items.reset_index(inplace=True)
        metadata.reset_index(inplace=True)
        items[&#39;source&#39;] = items.prod_id.apply(
            lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
        df1 = items.groupby([&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]
                            ).agg(product_variations=pd.NamedAgg(column=&#39;prod_id&#39;, aggfunc=&#39;count&#39;)).reset_index()
        df2 = items.drop_duplicates(subset=[&#39;item_price&#39;, &#39;prod_id&#39;]
                                    ).groupby([&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]
                                              ).agg(avg_item_price=pd.NamedAgg(column=&#39;item_price&#39;, aggfunc=&#39;mean&#39;)).reset_index()
        cat_page_item_variations_price_df = pd.concat(
            [df1, df2[[&#39;avg_item_price&#39;]]], axis=1)
        del df1, df2
        gc.collect()
        cat_page_item_variations_price_df.avg_item_price = cat_page_item_variations_price_df.avg_item_price.apply(
            lambda x: round(x, 2))
        # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
        #     cat_page_item_variations_price_df[col] = cat_page_item_variations_price_df[col].astype(
        #         &#39;category&#39;)
        cat_page_item_variations_price_df.to_feather(
            self.dash_data_path/&#39;category_page_item_variations_price&#39;)

        del cat_page_item_variations_price_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;category_page_item_variations_price&#39;,
            job_name=&#39;webapp&#39;)

        # Packaging Analysis Data
        cat_page_item_package_oz_df = pd.DataFrame(items.drop_duplicates(subset=[&#39;size_oz&#39;, &#39;prod_id&#39;]
                                                                         ).groupby([&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]
                                                                                   ).size_oz.value_counts()
                                                   ).rename(columns={&#39;size_oz&#39;: &#39;product_count&#39;}).reset_index()
        cat_page_item_package_oz_df = cat_page_item_package_oz_df[
            cat_page_item_package_oz_df.size_oz != &#39;&#39;]
        cat_page_item_package_oz_df.rename(
            columns={&#39;size_oz&#39;: &#39;item_size&#39;}, inplace=True)

        def hasNumbers(inputString):
            return bool(re.search(r&#39;\d&#39;, inputString))

        cat_page_item_package_oz_df = cat_page_item_package_oz_df[cat_page_item_package_oz_df.item_size.apply(
            hasNumbers)]
        cat_page_item_package_oz_df = cat_page_item_package_oz_df[cat_page_item_package_oz_df.item_size.str.len(
        ) &lt; 30]
        cat_page_item_package_oz_df.item_size = cat_page_item_package_oz_df.item_size.str.replace(
            &#39;out of stock:&#39;, &#39;&#39;)
        cat_page_item_package_oz_df.reset_index(inplace=True, drop=True)
        # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
        #     cat_page_item_package_oz_df[col] = cat_page_item_package_oz_df[col].astype(
        #         &#39;category&#39;)
        cat_page_item_package_oz_df.to_feather(
            self.dash_data_path/&#39;category_page_item_package_oz&#39;)

        del cat_page_item_package_oz_df, items
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;category_page_item_package_oz&#39;,
            job_name=&#39;webapp&#39;)

        # Top Product Data
        cat_page_top_products_df = metadata.groupby([&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]
                                                    )[&#39;prod_id&#39;, &#39;brand&#39;, &#39;product_name&#39;,
                                                      &#39;bayesian_estimate&#39;, &#39;low_p&#39;, &#39;high_p&#39;, &#39;first_review_date&#39;].apply(
            lambda x: x.nlargest(20, columns=[&#39;bayesian_estimate&#39;])).reset_index()
        cat_page_top_products_df.drop(columns=[&#39;level_3&#39;], inplace=True)
        cat_page_top_products_df.rename(columns={&#39;bayesian_estimate&#39;: &#39;adjusted_rating&#39;,
                                                 &#34;low_p&#34;: &#39;small_size_price&#39;,
                                                 &#39;high_p&#39;: &#39;big_size_price&#39;}, inplace=True)
        cat_page_top_products_df.adjusted_rating = cat_page_top_products_df.adjusted_rating.apply(
            lambda x: round(x, 3))
        cat_page_top_products_df.small_size_price = cat_page_top_products_df.apply(lambda x: f&#39;${x.small_size_price}&#39;
                                                                                   if x.source == &#39;us&#39; else f&#39;£{x.small_size_price}&#39;, axis=1)
        cat_page_top_products_df.big_size_price = cat_page_top_products_df.apply(lambda x: f&#39;${x.big_size_price}&#39;
                                                                                 if x.source == &#39;us&#39; else f&#39;£{x.big_size_price}&#39;, axis=1)
        # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
        #     cat_page_top_products_df[col] = cat_page_top_products_df[col].astype(
        #         &#39;category&#39;)
        cat_page_top_products_df.to_feather(
            self.dash_data_path/&#39;category_page_top_products&#39;)

        del cat_page_top_products_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;category_page_top_products&#39;,
            job_name=&#39;webapp&#39;)

        # New Product Data
        cat_page_new_products_details_df = metadata[metadata.new_flag == &#39;new&#39;][[&#39;source&#39;, &#39;prod_id&#39;, &#39;product_name&#39;,
                                                                                 &#39;brand&#39;, &#39;category&#39;, &#39;product_type&#39;,
                                                                                 &#39;low_p&#39;, &#39;high_p&#39;, &#39;bayesian_estimate&#39;, &#39;reviews&#39;,
                                                                                 &#39;first_review_date&#39;]]
        cat_page_new_products_details_df.rename(columns={&#39;bayesian_estimate&#39;: &#39;adjusted_rating&#39;,
                                                         &#34;low_p&#34;: &#39;small_size_price&#39;,
                                                         &#39;high_p&#39;: &#39;big_size_price&#39;}, inplace=True)
        cat_page_new_products_details_df.small_size_price = cat_page_new_products_details_df.apply(
            lambda x: f&#39;${x.small_size_price}&#39;
            if x.source == &#39;us&#39; else f&#39;£{x.small_size_price}&#39;, axis=1)
        cat_page_new_products_details_df.big_size_price = cat_page_new_products_details_df.apply(
            lambda x: f&#39;${x.big_size_price}&#39;
            if x.source == &#39;us&#39; else f&#39;£{x.big_size_price}&#39;, axis=1)
        cat_page_new_products_details_df.adjusted_rating = cat_page_new_products_details_df.adjusted_rating.apply(
            lambda x: round(x, 3))
        cat_page_new_products_details_df.sort_values(
            by=&#39;adjusted_rating&#39;, inplace=True, ascending=False)
        cat_page_new_products_details_df.reset_index(drop=True, inplace=True)
        # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;, &#39;brand&#39;]:
        #     cat_page_new_products_details_df[col] = cat_page_new_products_details_df[col].astype(
        #         &#39;category&#39;)
        cat_page_new_products_details_df.to_feather(
            self.dash_data_path/&#39;category_page_new_products_details&#39;)

        del cat_page_new_products_details_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;category_page_new_products_details&#39;,
            job_name=&#39;webapp&#39;)

        # New Ingredients Data
        ingredients = db.query_database(&#34;select prod_id, ingredient, new_flag, ingredient_type, brand, meta_date, category,\
            product_name, product_type, ban_flag, bayesian_estimate from r_bte_product_ingredients_f&#34;)
        ingredients[&#39;source&#39;] = ingredients.prod_id.apply(
            lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
        cat_page_new_ingredients_df = ingredients[ingredients.new_flag == &#39;new_ingredient&#39;][
            [&#39;prod_id&#39;, &#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;, &#39;brand&#39;, &#39;product_name&#39;, &#39;ingredient&#39;,
             &#39;ingredient_type&#39;, &#39;bayesian_estimate&#39;, &#39;ban_flag&#39;]
        ]
        cat_page_new_ingredients_df.rename(
            columns={&#39;bayesian_estimate&#39;: &#39;adjusted_rating&#39;}, inplace=True)
        cat_page_new_ingredients_df.reset_index(inplace=True, drop=True)
        # for col in cat_page_new_ingredients_df.columns:
        #     if col not in [&#39;ingredient&#39;, &#39;adjusted_rating&#39;, &#39;ingredient&#39;]:
        #         cat_page_new_ingredients_df[col] = cat_page_new_ingredients_df[col].astype(
        #             &#39;category&#39;)
        cat_page_new_ingredients_df.to_feather(
            self.dash_data_path/&#39;category_page_new_ingredients&#39;)

        del cat_page_new_ingredients_df, ingredients
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;category_page_new_ingredients&#39;,
            job_name=&#39;webapp&#39;)

        # User Attribute Data
        reviews = db.query_database(
            &#34;select prod_id, age, eye_color, hair_color, skin_tone, review_text, skin_type, review_date from  r_bte_product_review_f&#34;)
        reviews.drop_duplicates(
            subset=[&#39;prod_id&#39;, &#39;review_text&#39;, &#39;review_date&#39;], inplace=True)
        reviews.drop(columns=&#39;review_text&#39;, inplace=True)
        metadata.set_index(&#39;prod_id&#39;, inplace=True)
        reviews.set_index(&#39;prod_id&#39;, inplace=True)
        reviews = reviews.join(
            metadata[[&#39;category&#39;, &#39;product_type&#39;]], how=&#39;left&#39;)
        reviews.reset_index(inplace=True)
        metadata.reset_index(inplace=True)
        reviews = reviews[~reviews.category.isna()]
        reviews = reviews[~reviews.review_date.isna()]
        reviews.drop_duplicates(inplace=True)
        reviews[&#39;month&#39;] = reviews[&#39;review_date&#39;].astype(&#39;datetime64[M]&#39;)
        reviews[&#39;source&#39;] = reviews.prod_id.apply(
            lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
        reviews.replace(&#39;none&#39;, &#39;&#39;, regex=True, inplace=True)
        cat_page_reviews_by_user_attributes = reviews[[&#39;prod_id&#39;, &#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;, &#39;age&#39;, &#39;eye_color&#39;,
                                                       &#39;hair_color&#39;, &#39;skin_tone&#39;, &#39;skin_type&#39;]
                                                      ][(reviews.age != &#39;&#39;) |
                                                        (reviews.eye_color != &#39;&#39;) |
                                                        (reviews.hair_color != &#39;&#39;) |
                                                        (reviews.skin_tone != &#39;&#39;) |
                                                        (reviews.skin_type != &#39;&#39;)
                                                        ].reset_index(drop=True)
        cat_page_reviews_by_user_attributes.drop(
            columns=&#39;prod_id&#39;, inplace=True)
        for col in cat_page_reviews_by_user_attributes.columns:
            cat_page_reviews_by_user_attributes[col] = cat_page_reviews_by_user_attributes[col].astype(
                &#39;category&#39;)
        cat_page_reviews_by_user_attributes.to_feather(
            self.dash_data_path/&#39;category_page_reviews_by_user_attributes&#39;)

        del cat_page_reviews_by_user_attributes, reviews, metadata
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;category_page_reviews_by_user_attributes&#39;,
            job_name=&#39;webapp&#39;)

    def refresh_product_page_data(self):
        &#34;&#34;&#34;refresh_product_page_data [summary]

        [extended_summary]
        &#34;&#34;&#34;
        # get metadata
        metadata = db.query_database(&#34;select prod_id, product_name, brand, category, product_type, new_flag, meta_date, low_p, high_p, \
                              mrp, reviews, bayesian_estimate, first_review_date from r_bte_meta_detail_f&#34;)
        metadata[&#39;source&#39;] = metadata.prod_id.apply(
            lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
        metadata[&#39;meta_date&#39;] = metadata[&#39;meta_date&#39;].astype(&#39;datetime64&#39;)

        # initialize landing page data dict
        self.landing_page_data[&#39;products&#39;] = metadata.prod_id.nunique()
        self.landing_page_data[&#39;brands&#39;] = metadata.brand.nunique()

        dt = metadata.groupby(&#39;source&#39;).meta_date.max(
        ).reset_index().to_dict(&#39;records&#39;)
        meta = []
        for i in dt:
            meta.append(metadata[(metadata.source == i[&#39;source&#39;]) &amp;
                                 (metadata.meta_date == i[&#39;meta_date&#39;])])
        metadata = pd.concat(meta, axis=0)
        metadata.fillna(&#39;&#39;, inplace=True)
        metadata.drop_duplicates(subset=&#39;prod_id&#39;, inplace=True)

        self.landing_page_data[&#39;latest_scraped_date&#39;] = metadata.meta_date.max().strftime(
            &#34;%d/%m/%Y&#34;)

        metadata[metadata.columns.difference([&#39;product_name&#39;, &#39;brand&#39;])] \
            = metadata[metadata.columns.difference([&#39;product_name&#39;, &#39;brand&#39;])]\
            .apply(lambda x: x.astype(str).str.lower() if(x.dtype == &#39;object&#39;) else x)

        # get review data
        reviews = db.query_database(
            &#34;select prod_id, review_date, sentiment, is_influenced, meta_date, age, eye_color,\
                 review_rating, hair_color, skin_tone, skin_type, review_text from r_bte_product_review_f&#34;)
        reviews.drop_duplicates(
            subset=[&#39;prod_id&#39;, &#39;review_text&#39;, &#39;review_date&#39;], inplace=True)
        reviews.drop(columns=&#39;review_text&#39;, inplace=True)

        self.landing_page_data[&#39;reviews&#39;] = reviews.shape[0]

        reviews.fillna(&#39;&#39;, inplace=True)

        first_review_date_df = reviews.groupby(
            &#39;prod_id&#39;).review_date.min().reset_index()

        metadata.set_index(&#39;prod_id&#39;, inplace=True)
        reviews.set_index(&#39;prod_id&#39;, inplace=True)
        first_review_date_df.set_index(&#39;prod_id&#39;, inplace=True)

        metadata = metadata.join(first_review_date_df, how=&#39;left&#39;)
        metadata.first_review_date = metadata.review_date
        metadata.drop(columns=&#39;review_date&#39;, inplace=True)

        del first_review_date_df
        gc.collect()

        reviews = reviews.join(
            metadata[[&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]], how=&#39;left&#39;)

        metadata.reset_index(inplace=True)
        reviews.reset_index(inplace=True)

        reviews = reviews[reviews.prod_id.isin(metadata.prod_id.tolist())]
        reviews[&#39;review_date&#39;] = reviews[&#39;review_date&#39;].astype(&#39;datetime64[M]&#39;)
        reviews[reviews.columns.difference([&#39;product_name&#39;])] \
            = reviews[reviews.columns.difference([&#39;product_name&#39;])]\
            .apply(lambda x: x.astype(str).str.lower() if(x.dtype == &#39;object&#39;) else x)
        reviews.reset_index(inplace=True, drop=True)

        # get review summary data
        review_sum = db.query_database(
            &#34;select prod_id, pos_review_summary, neg_review_summary, \
                pos_talking_points, neg_talking_points \
                    from r_bte_product_review_summary_f&#34;)
        review_sum = review_sum[review_sum.prod_id.isin(
            metadata.prod_id.tolist())]
        review_sum.reset_index(inplace=True, drop=True)

        # get ingredient data
        ingredients = db.query_database(&#34;select prod_id, product_name, brand, category, product_type, \
            ingredient, ingredient_type, new_flag, ban_flag \
                from r_bte_product_ingredients_f&#34;)
        ingredients = ingredients[ingredients.prod_id.isin(
            metadata.prod_id.tolist())]
        ingredients.reset_index(inplace=True, drop=True)
        ingredients.fillna(&#39;&#39;, inplace=True)
        ingredients[&#39;source&#39;] = ingredients.prod_id.apply(
            lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
        ingredients[ingredients.columns.difference([&#39;product_name&#39;, &#39;brand&#39;])] \
            = ingredients[ingredients.columns.difference([&#39;product_name&#39;, &#39;brand&#39;])]\
            .apply(lambda x: x.astype(str).str.lower() if(x.dtype == &#39;object&#39;) else x)

        # get item data
        items = db.query_database(&#34;select * from r_bte_product_item_f&#34;)
        items = items[items.prod_id.isin(metadata.prod_id.tolist())]
        items[&#39;meta_date&#39;] = items[&#39;meta_date&#39;].astype(&#39;datetime64[M]&#39;)
        items.fillna(&#39;&#39;, inplace=True)
        items.reset_index(inplace=True, drop=True)

        # Dropdown and Metadata Data
        prod_page_metadetail_data_df = metadata
        del metadata
        gc.collect()

        prod_page_metadetail_data_df.rename(columns={&#39;low_p&#39;: &#39;small_size_price&#39;, &#39;high_p&#39;: &#39;big_size_price&#39;,
                                                     &#39;bayesian_estimate&#39;: &#39;adjusted_rating&#39;}, inplace=True)
        prod_page_metadetail_data_df.adjusted_rating = prod_page_metadetail_data_df.adjusted_rating.apply(
            lambda x: round(float(x), 2) if x != &#39;&#39; else &#39;&#39;)
        prod_page_metadetail_data_df = prod_page_metadetail_data_df[
            prod_page_metadetail_data_df.small_size_price != &#39;&#39;]
        prod_page_metadetail_data_df.big_size_price = prod_page_metadetail_data_df.apply(
            lambda x: x.small_size_price if x.big_size_price == &#39;&#39; else x.big_size_price, axis=1)
        prod_page_metadetail_data_df.mrp = prod_page_metadetail_data_df.apply(
            lambda x: x.big_size_price if x.mrp == &#39;&#39; else x.mrp, axis=1)
        prod_page_metadetail_data_df.reset_index(inplace=True, drop=True)
        prod_page_metadetail_data_df.small_size_price = prod_page_metadetail_data_df.small_size_price.astype(
            float)
        prod_page_metadetail_data_df.big_size_price = prod_page_metadetail_data_df.big_size_price.astype(
            float)
        prod_page_metadetail_data_df.mrp = prod_page_metadetail_data_df.mrp.astype(
            str)
        prod_page_metadetail_data_df.adjusted_rating = prod_page_metadetail_data_df.adjusted_rating.astype(
            str)
        for col in [&#39;brand&#39;, &#39;category&#39;, &#39;product_type&#39;, &#39;new_flag&#39;, &#39;source&#39;]:
            prod_page_metadetail_data_df[col] = prod_page_metadetail_data_df[col].astype(
                &#39;category&#39;)
        prod_page_metadetail_data_df.drop_duplicates(inplace=True)
        prod_page_metadetail_data_df.reset_index(drop=True, inplace=True)
        prod_page_metadetail_data_df.to_feather(
            self.dash_data_path/&#39;product_page_metadetail_data&#39;)

        del prod_page_metadetail_data_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;product_page_metadetail_data&#39;,
            job_name=&#39;webapp&#39;)

        # Review Tab Data
        review_sum.fillna(&#34;{}&#34;, inplace=True)
        review_sum.pos_talking_points[review_sum.pos_talking_points == &#34;&#34;] = &#34;{}&#34;
        review_sum.neg_talking_points[review_sum.neg_talking_points == &#34;&#34;] = &#34;{}&#34;
        indices = []
        for i in review_sum.iterrows():
            try:
                d = literal_eval(i[1].pos_talking_points)
                d = literal_eval(i[1].neg_talking_points)
            except Exception as ex:
                indices.append(i[0])
        review_sum = review_sum[~review_sum.index.isin(
            indices)].reset_index(drop=True)
        review_sum.pos_talking_points = review_sum.pos_talking_points.apply(
            lambda x: literal_eval(x) if x != &#34;{}&#34; else {})
        review_sum.neg_talking_points = review_sum.neg_talking_points.apply(
            lambda x: literal_eval(x) if x != &#34;{}&#34; else {})
        # Review Summary Data
        prod_page_review_sum_df = review_sum[[
            &#39;prod_id&#39;, &#39;pos_review_summary&#39;, &#39;neg_review_summary&#39;]]

        prod_page_review_sum_df.drop_duplicates(inplace=True)
        prod_page_review_sum_df.reset_index(drop=True, inplace=True)
        prod_page_review_sum_df.to_feather(
            self.dash_data_path/&#39;prod_page_product_review_summary&#39;)

        del prod_page_review_sum_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;prod_page_product_review_summary&#39;,
            job_name=&#39;webapp&#39;)

        # Talking Points Data
        prod_page_review_talking_points_df = review_sum[[
            &#39;prod_id&#39;, &#39;pos_talking_points&#39;, &#39;neg_talking_points&#39;]]

        prod_page_review_talking_points_df.drop_duplicates(
            inplace=True, subset=&#39;prod_id&#39;)
        prod_page_review_talking_points_df.reset_index(drop=True, inplace=True)
        prod_page_review_talking_points_df.to_pickle(
            self.dash_data_path/&#39;prod_page_review_talking_points&#39;)

        del prod_page_review_talking_points_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;prod_page_review_talking_points&#39;,
            job_name=&#39;webapp&#39;)

        # Review Sentiment and Time Series Data
        reviews.is_influenced = reviews.is_influenced.str.lower()
        prod_page_review_sentiment_influence_df = reviews[[
            &#39;prod_id&#39;, &#39;sentiment&#39;, &#39;is_influenced&#39;, &#39;review_date&#39;, &#39;review_rating&#39;]]
        for col in [&#39;prod_id&#39;, &#39;sentiment&#39;, &#39;is_influenced&#39;, &#39;review_rating&#39;]:
            prod_page_review_sentiment_influence_df[col] = prod_page_review_sentiment_influence_df[col].astype(
                &#39;category&#39;)
        prod_page_review_sentiment_influence_df.review_date = prod_page_review_sentiment_influence_df.review_date.astype(
            &#39;datetime64[M]&#39;)

        prod_page_review_sentiment_influence_df.reset_index(
            drop=True, inplace=True)
        prod_page_review_sentiment_influence_df.to_feather(
            self.dash_data_path/&#39;prod_page_review_sentiment_influence&#39;)

        del prod_page_review_sentiment_influence_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;prod_page_review_sentiment_influence&#39;,
            job_name=&#39;webapp&#39;)

        # Review User Attribute Data
        prod_page_reviews_attribute_df = reviews[[&#39;prod_id&#39;, &#39;age&#39;, &#39;eye_color&#39;, &#39;hair_color&#39;, &#39;skin_tone&#39;, &#39;skin_type&#39;]][
            (reviews.age != &#39;&#39;) | (reviews.eye_color != &#39;&#39;) | (reviews.hair_color != &#39;&#39;) | (reviews.skin_tone != &#39;&#39;) | (reviews.skin_type != &#39;&#39;)]
        for col in prod_page_reviews_attribute_df.columns:
            prod_page_reviews_attribute_df[col] = prod_page_reviews_attribute_df[col].astype(
                &#39;category&#39;)
        prod_page_reviews_attribute_df.reset_index(inplace=True, drop=True)
        prod_page_reviews_attribute_df.to_feather(
            self.dash_data_path/&#39;prod_page_reviews_attribute&#39;)

        del prod_page_reviews_attribute_df, reviews
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;prod_page_reviews_attribute&#39;,
            job_name=&#39;webapp&#39;)

        # Pricing and Ingredients Tab Data
        prod_page_item_df = items[[&#39;prod_id&#39;, &#39;product_name&#39;,
                                   &#39;meta_date&#39;, &#39;item_name&#39;, &#39;item_price&#39;, &#39;size_oz&#39;]]
        prod_page_item_df.rename(
            columns={&#39;size_oz&#39;: &#39;item_size&#39;}, inplace=True)

        def hasNumbers(inputString):
            return bool(re.search(r&#39;\d&#39;, inputString))
        prod_page_item_df.item_size = prod_page_item_df.item_size.apply(
            lambda x: x if hasNumbers(x) else &#39;&#39;).str.replace(&#39;out of stock:&#39;, &#39;&#39;).str.replace(&#39;nan&#39;, &#39;&#39;)
        prod_page_item_df.item_size = prod_page_item_df.item_size.apply(
            lambda x: x if len(x) &lt; 30 else &#39;&#39;)
        prod_page_item_df.reset_index(inplace=True, drop=True)
        for col in [&#39;item_price&#39;, &#39;item_size&#39;]:
            prod_page_item_df[col] = prod_page_item_df[col].astype(&#39;category&#39;)
        prod_page_item_df.drop_duplicates(
            inplace=True, subset=[&#39;prod_id&#39;, &#39;item_size&#39;, &#39;item_name&#39;])
        prod_page_item_df.reset_index(inplace=True, drop=True)
        prod_page_item_df.to_feather(self.dash_data_path/&#39;prod_page_item_data&#39;)

        del prod_page_item_df, items
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;prod_page_item_data&#39;,
            job_name=&#39;webapp&#39;)

        # Ingredient Data
        prod_page_ing_df = ingredients[[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;ingredient&#39;, &#39;new_flag&#39;,
                                        &#39;ingredient_type&#39;, &#39;ban_flag&#39;,
                                        &#39;category&#39;, &#39;product_type&#39;, &#39;source&#39;]]
        for col in [&#39;prod_id&#39;, &#39;product_name&#39;, &#39;ingredient&#39;, &#39;new_flag&#39;,
                    &#39;ingredient_type&#39;, &#39;ban_flag&#39;,
                    &#39;category&#39;, &#39;product_type&#39;, &#39;source&#39;]:
            prod_page_ing_df[col] = prod_page_ing_df[col].astype(&#39;category&#39;)
        prod_page_ing_df.to_feather(self.dash_data_path/&#39;prod_page_ing_data&#39;)

        del prod_page_ing_df, ingredients
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;prod_page_ing_data&#39;,
            job_name=&#39;webapp&#39;)

    def refresh_ingredient_page_data(self):
        &#34;&#34;&#34;refresh_ingredient_page_data [summary]

        [extended_summary]
        &#34;&#34;&#34;
        # get ingredients
        ing_page_ing_df = db.query_database(
            &#34;select prod_id, ingredient, new_flag, ingredient_type, \
                category, product_name, product_type, vegan_flag, ban_flag \
                from r_bte_product_ingredients_f&#34;)
        ing_page_ing_df.fillna(&#39;&#39;, inplace=True)

        vegan_prods = ing_page_ing_df.prod_id[ing_page_ing_df.vegan_flag == &#39;vegan&#39;].unique(
        ).tolist()
        ing_page_ing_df.ingredient_type = ing_page_ing_df.apply(
            lambda x: &#39;vegan&#39; if x.ingredient_type == &#39;&#39; and x.prod_id in vegan_prods else x.ingredient_type, axis=1)
        ing_page_ing_df.drop(columns=[&#39;vegan_flag&#39;], inplace=True)

        ing_page_ing_df.drop_duplicates(
            subset=[&#39;ingredient&#39;, &#39;prod_id&#39;], inplace=True)
        ing_page_ing_df[&#39;source&#39;] = ing_page_ing_df.prod_id.apply(
            lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
        ing_page_ing_df[ing_page_ing_df.columns.difference([&#39;product_name&#39;, ])] \
            = ing_page_ing_df[ing_page_ing_df.columns.difference([&#39;product_name&#39;, ])]\
            .apply(lambda x: x.astype(str).str.lower() if(x.dtype == &#39;object&#39;) else x)

        self.landing_page_data[&#39;ingredients&#39;] = ing_page_ing_df.ingredient.nunique(
        )

        for col in [&#39;prod_id&#39;, &#39;ingredient&#39;, &#39;new_flag&#39;, &#39;ingredient_type&#39;,
                    &#39;category&#39;, &#39;product_name&#39;, &#39;product_type&#39;, &#39;source&#39;,
                    &#39;ban_flag&#39;]:
            ing_page_ing_df[col] = ing_page_ing_df[col].astype(&#39;category&#39;)

        ing_page_ing_df.reset_index(inplace=True, drop=True)
        ing_page_ing_df.to_feather(self.dash_data_path/&#39;ing_page_ing_data&#39;)

        del ing_page_ing_df
        gc.collect()

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;ing_page_ing_data&#39;,
            job_name=&#39;webapp&#39;)

    def refresh_landing_page_data(self):
        &#34;&#34;&#34;refresh_landing_page_data [summary]

        [extended_summary]
        &#34;&#34;&#34;
        image_keys = [i[&#39;Key&#39;] for i in file_manager.get_matching_s3_keys(prefix=&#39;Feeds/BeautyTrendEngine/Image/Staging/&#39;, suffix=&#39;jpg&#39;)
                      if any(job in i[&#39;Key&#39;].lower() for job in [&#39;image&#39;])]

        self.landing_page_data[&#39;images&#39;] = len(image_keys)

        pd.DataFrame(self.landing_page_data, index=range(0, 1)).to_feather(
            self.dash_data_path/&#39;landing_page_data&#39;)

        file_manager.push_file_s3(
            file_path=self.dash_data_path/&#39;landing_page_data&#39;,
            job_name=&#39;webapp&#39;)

    def make(self):
        &#34;&#34;&#34;make [summary]

        [extended_summary]
        &#34;&#34;&#34;
        try:
            self.refresh_market_trend_data()
            self.refresh_category_page_data()
            self.refresh_product_page_data()
            self.refresh_ingredient_page_data()
            self.refresh_landing_page_data()
        except Exception as ex:
            print(&#39;refresh job failed.&#39;)
        else:
            print(&#39;refresh job completed.&#39;)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="meiyume.webapp_datamaker.RefreshData.make"><code class="name flex">
<span>def <span class="ident">make</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>make [summary]</p>
<p>[extended_summary]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make(self):
    &#34;&#34;&#34;make [summary]

    [extended_summary]
    &#34;&#34;&#34;
    try:
        self.refresh_market_trend_data()
        self.refresh_category_page_data()
        self.refresh_product_page_data()
        self.refresh_ingredient_page_data()
        self.refresh_landing_page_data()
    except Exception as ex:
        print(&#39;refresh job failed.&#39;)
    else:
        print(&#39;refresh job completed.&#39;)</code></pre>
</details>
</dd>
<dt id="meiyume.webapp_datamaker.RefreshData.refresh_category_page_data"><code class="name flex">
<span>def <span class="ident">refresh_category_page_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>refresh_category_page_data [summary]</p>
<p>[extended_summary]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def refresh_category_page_data(self):
    &#34;&#34;&#34;refresh_category_page_data [summary]

    [extended_summary]
    &#34;&#34;&#34;
    metadata = db.query_database(
        &#34;select prod_id, product_name, brand, category, product_type, new_flag, meta_date, low_p, high_p,\
         mrp, reviews, bayesian_estimate, first_review_date from r_bte_meta_detail_f&#34;)
    numeric_cols = [&#39;low_p&#39;, &#39;high_p&#39;, &#39;mrp&#39;,
                    &#39;reviews&#39;, &#39;bayesian_estimate&#39;]
    metadata[numeric_cols] = metadata[numeric_cols].apply(
        pd.to_numeric, errors=&#39;coerce&#39;)
    metadata[&#39;source&#39;] = metadata.prod_id.apply(
        lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
    grouped = metadata.groupby([&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;])
    # Pricing Analysis Data
    pricing_analytics_df = grouped.agg(min_price=pd.NamedAgg(column=&#39;low_p&#39;, aggfunc=&#39;min&#39;),
                                       max_price=pd.NamedAgg(
                                           column=&#39;high_p&#39;, aggfunc=&#39;max&#39;),
                                       avg_low_price=pd.NamedAgg(
                                           column=&#39;low_p&#39;, aggfunc=&#39;mean&#39;),
                                       avg_high_price=pd.NamedAgg(column=&#39;high_p&#39;, aggfunc=&#39;mean&#39;)).reset_index()
    pricing_analytics_df[[&#39;min_price&#39;, &#39;max_price&#39;, &#39;avg_low_price&#39;, &#39;avg_high_price&#39;]] = pricing_analytics_df[[
        &#39;min_price&#39;, &#39;max_price&#39;, &#39;avg_low_price&#39;, &#39;avg_high_price&#39;]].apply(lambda x: round(x, 2), axis=1)

    # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
    #     pricing_analytics_df[col] = pricing_analytics_df[col].astype(
    #         &#39;category&#39;)
    pricing_analytics_df.to_feather(
        self.dash_data_path/&#39;category_page_pricing_data&#39;)

    del pricing_analytics_df
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;category_page_pricing_data&#39;,
        job_name=&#39;webapp&#39;)

    # Product Analysis Data
    cat_page_distinct_brands_products_df = grouped.agg(distinct_brands=pd.NamedAgg(column=&#39;brand&#39;, aggfunc=&#39;nunique&#39;),
                                                       distinct_products=pd.NamedAgg(column=&#39;prod_id&#39;, aggfunc=&#39;nunique&#39;)).reset_index()
    # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
    #     cat_page_distinct_brands_products_df[col] = cat_page_distinct_brands_products_df[col].astype(
    #         &#39;category&#39;)
    cat_page_distinct_brands_products_df.to_feather(
        self.dash_data_path/&#39;category_page_distinct_brands_products&#39;)

    del cat_page_distinct_brands_products_df
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;category_page_distinct_brands_products&#39;,
        job_name=&#39;webapp&#39;)

    # New Products Data
    cat_page_new_products_df = metadata[metadata.new_flag == &#39;new&#39;].groupby(
        [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]).new_flag.count().reset_index()
    cat_page_new_products_df = cat_page_new_products_df.rename(
        columns={&#39;new_flag&#39;: &#39;new_product_count&#39;})
    # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
    #     cat_page_new_products_df[col] = cat_page_new_products_df[col].astype(
    #         &#39;category&#39;)
    cat_page_new_products_df.to_feather(
        self.dash_data_path/&#39;category_page_new_products_count&#39;)

    del cat_page_new_products_df
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;category_page_new_products_count&#39;,
        job_name=&#39;webapp&#39;)

    # Product Varieties and Price Data
    items = db.query_database(
        &#34;select prod_id, item_price, size_oz from r_bte_product_item_f&#34;)
    metadata.set_index(&#39;prod_id&#39;, inplace=True)
    items.set_index(&#39;prod_id&#39;, inplace=True)
    items = items.join(metadata[[&#39;category&#39;, &#39;product_type&#39;]], how=&#39;left&#39;)
    items.reset_index(inplace=True)
    metadata.reset_index(inplace=True)
    items[&#39;source&#39;] = items.prod_id.apply(
        lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
    df1 = items.groupby([&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]
                        ).agg(product_variations=pd.NamedAgg(column=&#39;prod_id&#39;, aggfunc=&#39;count&#39;)).reset_index()
    df2 = items.drop_duplicates(subset=[&#39;item_price&#39;, &#39;prod_id&#39;]
                                ).groupby([&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]
                                          ).agg(avg_item_price=pd.NamedAgg(column=&#39;item_price&#39;, aggfunc=&#39;mean&#39;)).reset_index()
    cat_page_item_variations_price_df = pd.concat(
        [df1, df2[[&#39;avg_item_price&#39;]]], axis=1)
    del df1, df2
    gc.collect()
    cat_page_item_variations_price_df.avg_item_price = cat_page_item_variations_price_df.avg_item_price.apply(
        lambda x: round(x, 2))
    # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
    #     cat_page_item_variations_price_df[col] = cat_page_item_variations_price_df[col].astype(
    #         &#39;category&#39;)
    cat_page_item_variations_price_df.to_feather(
        self.dash_data_path/&#39;category_page_item_variations_price&#39;)

    del cat_page_item_variations_price_df
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;category_page_item_variations_price&#39;,
        job_name=&#39;webapp&#39;)

    # Packaging Analysis Data
    cat_page_item_package_oz_df = pd.DataFrame(items.drop_duplicates(subset=[&#39;size_oz&#39;, &#39;prod_id&#39;]
                                                                     ).groupby([&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]
                                                                               ).size_oz.value_counts()
                                               ).rename(columns={&#39;size_oz&#39;: &#39;product_count&#39;}).reset_index()
    cat_page_item_package_oz_df = cat_page_item_package_oz_df[
        cat_page_item_package_oz_df.size_oz != &#39;&#39;]
    cat_page_item_package_oz_df.rename(
        columns={&#39;size_oz&#39;: &#39;item_size&#39;}, inplace=True)

    def hasNumbers(inputString):
        return bool(re.search(r&#39;\d&#39;, inputString))

    cat_page_item_package_oz_df = cat_page_item_package_oz_df[cat_page_item_package_oz_df.item_size.apply(
        hasNumbers)]
    cat_page_item_package_oz_df = cat_page_item_package_oz_df[cat_page_item_package_oz_df.item_size.str.len(
    ) &lt; 30]
    cat_page_item_package_oz_df.item_size = cat_page_item_package_oz_df.item_size.str.replace(
        &#39;out of stock:&#39;, &#39;&#39;)
    cat_page_item_package_oz_df.reset_index(inplace=True, drop=True)
    # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
    #     cat_page_item_package_oz_df[col] = cat_page_item_package_oz_df[col].astype(
    #         &#39;category&#39;)
    cat_page_item_package_oz_df.to_feather(
        self.dash_data_path/&#39;category_page_item_package_oz&#39;)

    del cat_page_item_package_oz_df, items
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;category_page_item_package_oz&#39;,
        job_name=&#39;webapp&#39;)

    # Top Product Data
    cat_page_top_products_df = metadata.groupby([&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]
                                                )[&#39;prod_id&#39;, &#39;brand&#39;, &#39;product_name&#39;,
                                                  &#39;bayesian_estimate&#39;, &#39;low_p&#39;, &#39;high_p&#39;, &#39;first_review_date&#39;].apply(
        lambda x: x.nlargest(20, columns=[&#39;bayesian_estimate&#39;])).reset_index()
    cat_page_top_products_df.drop(columns=[&#39;level_3&#39;], inplace=True)
    cat_page_top_products_df.rename(columns={&#39;bayesian_estimate&#39;: &#39;adjusted_rating&#39;,
                                             &#34;low_p&#34;: &#39;small_size_price&#39;,
                                             &#39;high_p&#39;: &#39;big_size_price&#39;}, inplace=True)
    cat_page_top_products_df.adjusted_rating = cat_page_top_products_df.adjusted_rating.apply(
        lambda x: round(x, 3))
    cat_page_top_products_df.small_size_price = cat_page_top_products_df.apply(lambda x: f&#39;${x.small_size_price}&#39;
                                                                               if x.source == &#39;us&#39; else f&#39;£{x.small_size_price}&#39;, axis=1)
    cat_page_top_products_df.big_size_price = cat_page_top_products_df.apply(lambda x: f&#39;${x.big_size_price}&#39;
                                                                             if x.source == &#39;us&#39; else f&#39;£{x.big_size_price}&#39;, axis=1)
    # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
    #     cat_page_top_products_df[col] = cat_page_top_products_df[col].astype(
    #         &#39;category&#39;)
    cat_page_top_products_df.to_feather(
        self.dash_data_path/&#39;category_page_top_products&#39;)

    del cat_page_top_products_df
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;category_page_top_products&#39;,
        job_name=&#39;webapp&#39;)

    # New Product Data
    cat_page_new_products_details_df = metadata[metadata.new_flag == &#39;new&#39;][[&#39;source&#39;, &#39;prod_id&#39;, &#39;product_name&#39;,
                                                                             &#39;brand&#39;, &#39;category&#39;, &#39;product_type&#39;,
                                                                             &#39;low_p&#39;, &#39;high_p&#39;, &#39;bayesian_estimate&#39;, &#39;reviews&#39;,
                                                                             &#39;first_review_date&#39;]]
    cat_page_new_products_details_df.rename(columns={&#39;bayesian_estimate&#39;: &#39;adjusted_rating&#39;,
                                                     &#34;low_p&#34;: &#39;small_size_price&#39;,
                                                     &#39;high_p&#39;: &#39;big_size_price&#39;}, inplace=True)
    cat_page_new_products_details_df.small_size_price = cat_page_new_products_details_df.apply(
        lambda x: f&#39;${x.small_size_price}&#39;
        if x.source == &#39;us&#39; else f&#39;£{x.small_size_price}&#39;, axis=1)
    cat_page_new_products_details_df.big_size_price = cat_page_new_products_details_df.apply(
        lambda x: f&#39;${x.big_size_price}&#39;
        if x.source == &#39;us&#39; else f&#39;£{x.big_size_price}&#39;, axis=1)
    cat_page_new_products_details_df.adjusted_rating = cat_page_new_products_details_df.adjusted_rating.apply(
        lambda x: round(x, 3))
    cat_page_new_products_details_df.sort_values(
        by=&#39;adjusted_rating&#39;, inplace=True, ascending=False)
    cat_page_new_products_details_df.reset_index(drop=True, inplace=True)
    # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;, &#39;brand&#39;]:
    #     cat_page_new_products_details_df[col] = cat_page_new_products_details_df[col].astype(
    #         &#39;category&#39;)
    cat_page_new_products_details_df.to_feather(
        self.dash_data_path/&#39;category_page_new_products_details&#39;)

    del cat_page_new_products_details_df
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;category_page_new_products_details&#39;,
        job_name=&#39;webapp&#39;)

    # New Ingredients Data
    ingredients = db.query_database(&#34;select prod_id, ingredient, new_flag, ingredient_type, brand, meta_date, category,\
        product_name, product_type, ban_flag, bayesian_estimate from r_bte_product_ingredients_f&#34;)
    ingredients[&#39;source&#39;] = ingredients.prod_id.apply(
        lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
    cat_page_new_ingredients_df = ingredients[ingredients.new_flag == &#39;new_ingredient&#39;][
        [&#39;prod_id&#39;, &#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;, &#39;brand&#39;, &#39;product_name&#39;, &#39;ingredient&#39;,
         &#39;ingredient_type&#39;, &#39;bayesian_estimate&#39;, &#39;ban_flag&#39;]
    ]
    cat_page_new_ingredients_df.rename(
        columns={&#39;bayesian_estimate&#39;: &#39;adjusted_rating&#39;}, inplace=True)
    cat_page_new_ingredients_df.reset_index(inplace=True, drop=True)
    # for col in cat_page_new_ingredients_df.columns:
    #     if col not in [&#39;ingredient&#39;, &#39;adjusted_rating&#39;, &#39;ingredient&#39;]:
    #         cat_page_new_ingredients_df[col] = cat_page_new_ingredients_df[col].astype(
    #             &#39;category&#39;)
    cat_page_new_ingredients_df.to_feather(
        self.dash_data_path/&#39;category_page_new_ingredients&#39;)

    del cat_page_new_ingredients_df, ingredients
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;category_page_new_ingredients&#39;,
        job_name=&#39;webapp&#39;)

    # User Attribute Data
    reviews = db.query_database(
        &#34;select prod_id, age, eye_color, hair_color, skin_tone, review_text, skin_type, review_date from  r_bte_product_review_f&#34;)
    reviews.drop_duplicates(
        subset=[&#39;prod_id&#39;, &#39;review_text&#39;, &#39;review_date&#39;], inplace=True)
    reviews.drop(columns=&#39;review_text&#39;, inplace=True)
    metadata.set_index(&#39;prod_id&#39;, inplace=True)
    reviews.set_index(&#39;prod_id&#39;, inplace=True)
    reviews = reviews.join(
        metadata[[&#39;category&#39;, &#39;product_type&#39;]], how=&#39;left&#39;)
    reviews.reset_index(inplace=True)
    metadata.reset_index(inplace=True)
    reviews = reviews[~reviews.category.isna()]
    reviews = reviews[~reviews.review_date.isna()]
    reviews.drop_duplicates(inplace=True)
    reviews[&#39;month&#39;] = reviews[&#39;review_date&#39;].astype(&#39;datetime64[M]&#39;)
    reviews[&#39;source&#39;] = reviews.prod_id.apply(
        lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
    reviews.replace(&#39;none&#39;, &#39;&#39;, regex=True, inplace=True)
    cat_page_reviews_by_user_attributes = reviews[[&#39;prod_id&#39;, &#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;, &#39;age&#39;, &#39;eye_color&#39;,
                                                   &#39;hair_color&#39;, &#39;skin_tone&#39;, &#39;skin_type&#39;]
                                                  ][(reviews.age != &#39;&#39;) |
                                                    (reviews.eye_color != &#39;&#39;) |
                                                    (reviews.hair_color != &#39;&#39;) |
                                                    (reviews.skin_tone != &#39;&#39;) |
                                                    (reviews.skin_type != &#39;&#39;)
                                                    ].reset_index(drop=True)
    cat_page_reviews_by_user_attributes.drop(
        columns=&#39;prod_id&#39;, inplace=True)
    for col in cat_page_reviews_by_user_attributes.columns:
        cat_page_reviews_by_user_attributes[col] = cat_page_reviews_by_user_attributes[col].astype(
            &#39;category&#39;)
    cat_page_reviews_by_user_attributes.to_feather(
        self.dash_data_path/&#39;category_page_reviews_by_user_attributes&#39;)

    del cat_page_reviews_by_user_attributes, reviews, metadata
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;category_page_reviews_by_user_attributes&#39;,
        job_name=&#39;webapp&#39;)</code></pre>
</details>
</dd>
<dt id="meiyume.webapp_datamaker.RefreshData.refresh_ingredient_page_data"><code class="name flex">
<span>def <span class="ident">refresh_ingredient_page_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>refresh_ingredient_page_data [summary]</p>
<p>[extended_summary]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def refresh_ingredient_page_data(self):
    &#34;&#34;&#34;refresh_ingredient_page_data [summary]

    [extended_summary]
    &#34;&#34;&#34;
    # get ingredients
    ing_page_ing_df = db.query_database(
        &#34;select prod_id, ingredient, new_flag, ingredient_type, \
            category, product_name, product_type, vegan_flag, ban_flag \
            from r_bte_product_ingredients_f&#34;)
    ing_page_ing_df.fillna(&#39;&#39;, inplace=True)

    vegan_prods = ing_page_ing_df.prod_id[ing_page_ing_df.vegan_flag == &#39;vegan&#39;].unique(
    ).tolist()
    ing_page_ing_df.ingredient_type = ing_page_ing_df.apply(
        lambda x: &#39;vegan&#39; if x.ingredient_type == &#39;&#39; and x.prod_id in vegan_prods else x.ingredient_type, axis=1)
    ing_page_ing_df.drop(columns=[&#39;vegan_flag&#39;], inplace=True)

    ing_page_ing_df.drop_duplicates(
        subset=[&#39;ingredient&#39;, &#39;prod_id&#39;], inplace=True)
    ing_page_ing_df[&#39;source&#39;] = ing_page_ing_df.prod_id.apply(
        lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
    ing_page_ing_df[ing_page_ing_df.columns.difference([&#39;product_name&#39;, ])] \
        = ing_page_ing_df[ing_page_ing_df.columns.difference([&#39;product_name&#39;, ])]\
        .apply(lambda x: x.astype(str).str.lower() if(x.dtype == &#39;object&#39;) else x)

    self.landing_page_data[&#39;ingredients&#39;] = ing_page_ing_df.ingredient.nunique(
    )

    for col in [&#39;prod_id&#39;, &#39;ingredient&#39;, &#39;new_flag&#39;, &#39;ingredient_type&#39;,
                &#39;category&#39;, &#39;product_name&#39;, &#39;product_type&#39;, &#39;source&#39;,
                &#39;ban_flag&#39;]:
        ing_page_ing_df[col] = ing_page_ing_df[col].astype(&#39;category&#39;)

    ing_page_ing_df.reset_index(inplace=True, drop=True)
    ing_page_ing_df.to_feather(self.dash_data_path/&#39;ing_page_ing_data&#39;)

    del ing_page_ing_df
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;ing_page_ing_data&#39;,
        job_name=&#39;webapp&#39;)</code></pre>
</details>
</dd>
<dt id="meiyume.webapp_datamaker.RefreshData.refresh_landing_page_data"><code class="name flex">
<span>def <span class="ident">refresh_landing_page_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>refresh_landing_page_data [summary]</p>
<p>[extended_summary]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def refresh_landing_page_data(self):
    &#34;&#34;&#34;refresh_landing_page_data [summary]

    [extended_summary]
    &#34;&#34;&#34;
    image_keys = [i[&#39;Key&#39;] for i in file_manager.get_matching_s3_keys(prefix=&#39;Feeds/BeautyTrendEngine/Image/Staging/&#39;, suffix=&#39;jpg&#39;)
                  if any(job in i[&#39;Key&#39;].lower() for job in [&#39;image&#39;])]

    self.landing_page_data[&#39;images&#39;] = len(image_keys)

    pd.DataFrame(self.landing_page_data, index=range(0, 1)).to_feather(
        self.dash_data_path/&#39;landing_page_data&#39;)

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;landing_page_data&#39;,
        job_name=&#39;webapp&#39;)</code></pre>
</details>
</dd>
<dt id="meiyume.webapp_datamaker.RefreshData.refresh_market_trend_data"><code class="name flex">
<span>def <span class="ident">refresh_market_trend_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>refresh_market_trend_data [summary]</p>
<p>[extended_summary]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def refresh_market_trend_data(self):
    &#34;&#34;&#34;refresh_market_trend_data [summary]

    [extended_summary]
    &#34;&#34;&#34;
    # Get Metadata
    metadata = db.query_database(
        &#34;select prod_id, category, product_type, new_flag, meta_date from r_bte_meta_detail_f&#34;)
    metadata[&#39;source&#39;] = metadata.prod_id.apply(
        lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
    metadata.drop_duplicates(inplace=True)
    metadata.reset_index(inplace=True, drop=True)
    metadata.meta_date = metadata.meta_date.astype(&#39;datetime64[M]&#39;)

    meta_df = metadata[[&#39;prod_id&#39;, &#39;category&#39;,
                        &#39;product_type&#39;]].drop_duplicates(subset=&#39;prod_id&#39;)
    # Get Review Data
    reviews = db.query_database(
        &#34;select prod_id, review_date, is_influenced, review_text from r_bte_product_review_f&#34;)
    reviews.drop_duplicates(
        subset=[&#39;prod_id&#39;, &#39;review_text&#39;, &#39;review_date&#39;], inplace=True)
    reviews.drop(columns=&#39;review_text&#39;, inplace=True)

    meta_df.set_index(&#39;prod_id&#39;, inplace=True)
    reviews.set_index(&#39;prod_id&#39;, inplace=True)

    reviews = reviews.join(meta_df, how=&#39;left&#39;)
    reviews = reviews[~reviews.category.isna()]
    reviews = reviews[~reviews.review_date.isna()]

    del meta_df
    gc.collect()

    reviews.reset_index(inplace=True)
    reviews[&#39;month&#39;] = reviews[&#39;review_date&#39;].astype(&#39;datetime64[M]&#39;)
    reviews[&#39;source&#39;] = reviews.prod_id.apply(
        lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
    reviews[reviews.columns.difference([&#39;product_name&#39;])] \
        = reviews[reviews.columns.difference([&#39;product_name&#39;])]\
        .apply(lambda x: x.astype(str).str.lower() if(x.dtype == &#39;object&#39;) else x)
    reviews.drop(columns=[&#39;review_date&#39;], inplace=True)

    # Influenced Review Trend
    rev_by_marketing_cate_month = reviews[reviews.is_influenced == &#39;yes&#39;].groupby(
        by=[&#39;source&#39;, &#39;category&#39;, &#39;month&#39;]).prod_id.count().reset_index()
    # for col in [&#39;source&#39;, &#39;category&#39;]:
    #     rev_by_marketing_cate_month[col] = rev_by_marketing_cate_month[col].astype(
    #         &#39;category&#39;)
    rev_by_marketing_cate_month.rename(
        columns={&#39;prod_id&#39;: &#39;review_text&#39;}, inplace=True)
    rev_by_marketing_cate_month.to_feather(
        self.dash_data_path/&#39;review_trend_by_marketing_category_month&#39;)

    del rev_by_marketing_cate_month
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;review_trend_by_marketing_category_month&#39;,
        job_name=&#39;webapp&#39;)

    rev_by_marketing_ptype_month = reviews[reviews.is_influenced == &#39;yes&#39;].groupby(
        by=[&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;, &#39;month&#39;]).prod_id.count().reset_index()
    # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
    #     rev_by_marketing_ptype_month[col] = rev_by_marketing_ptype_month[col].astype(
    #         &#39;category&#39;)
    rev_by_marketing_ptype_month.rename(
        columns={&#39;prod_id&#39;: &#39;review_text&#39;}, inplace=True)
    rev_by_marketing_ptype_month.to_feather(
        self.dash_data_path/&#39;review_trend_by_marketing_product_type_month&#39;)

    del rev_by_marketing_ptype_month
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;review_trend_by_marketing_product_type_month&#39;,
        job_name=&#39;webapp&#39;)

    # Review Trend
    rev_by_cate_month = reviews.groupby(
        by=[&#39;source&#39;, &#39;category&#39;, &#39;month&#39;]).prod_id.count().reset_index()
    # for col in [&#39;source&#39;, &#39;category&#39;]:
    #     rev_by_cate_month[col] = rev_by_cate_month[col].astype(
    #         &#39;category&#39;)
    rev_by_cate_month.rename(
        columns={&#39;prod_id&#39;: &#39;review_text&#39;}, inplace=True)
    rev_by_cate_month.to_feather(
        self.dash_data_path/&#39;review_trend_category_month&#39;)

    del rev_by_cate_month
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;review_trend_category_month&#39;,
        job_name=&#39;webapp&#39;)

    rev_by_ptype_month = reviews.groupby(
        by=[&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;, &#39;month&#39;]).prod_id.count().reset_index()
    # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
    #     rev_by_ptype_month[col] = rev_by_ptype_month[col].astype(
    #         &#39;category&#39;)
    rev_by_ptype_month.rename(
        columns={&#39;prod_id&#39;: &#39;review_text&#39;}, inplace=True)
    rev_by_ptype_month.to_feather(
        self.dash_data_path/&#39;review_trend_product_type_month&#39;)

    del rev_by_ptype_month
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;review_trend_product_type_month&#39;,
        job_name=&#39;webapp&#39;)

    # Product Launch Trend
    meta_product_launces_trend_category_month = metadata[metadata.new_flag == &#39;new&#39;].groupby(
        by=[&#39;source&#39;, &#39;category&#39;, &#39;meta_date&#39;]).prod_id.count().reset_index()
    meta_product_launces_trend_category_month.rename(
        columns={&#39;prod_id&#39;: &#39;new_product_count&#39;}, inplace=True)
    # for col in [&#39;source&#39;, &#39;category&#39;]:
    #     meta_product_launces_trend_category_month[col] = meta_product_launces_trend_category_month[col].astype(
    #         &#39;category&#39;)
    meta_product_launces_trend_category_month.to_feather(
        self.dash_data_path/&#39;meta_product_launch_trend_category_month&#39;)

    del meta_product_launces_trend_category_month
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;meta_product_launch_trend_category_month&#39;,
        job_name=&#39;webapp&#39;)

    meta_product_launch_trend_product_type_df = metadata[metadata.new_flag ==
                                                         &#39;new&#39;].groupby(by=[&#39;source&#39;, &#39;category&#39;,
                                                                            &#39;product_type&#39;, &#39;meta_date&#39;]).prod_id.count().reset_index()
    meta_product_launch_trend_product_type_df.rename(
        columns={&#39;prod_id&#39;: &#39;new_product_count&#39;}, inplace=True)
    # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
    #     meta_product_launch_trend_product_type_df[col] = meta_product_launch_trend_product_type_df[col].astype(
    #         &#39;category&#39;)
    meta_product_launch_trend_product_type_df.to_feather(
        self.dash_data_path/&#39;meta_product_launch_trend_product_type_month&#39;)

    del meta_product_launch_trend_product_type_df
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;meta_product_launch_trend_product_type_month&#39;,
        job_name=&#39;webapp&#39;)

    metadata.new_flag[metadata.new_flag == &#39;&#39;] = &#39;old&#39;
    # this is awesome way to get in-class counts. reuse this everywhere
    product_launch_intensity_category_df = metadata.pivot_table(index=[&#39;source&#39;, &#39;category&#39;, &#39;meta_date&#39;],
                                                                columns=&#39;new_flag&#39;, aggfunc=&#39;size&#39;, fill_value=0).reset_index()
    product_launch_intensity_category_df[&#39;product_count&#39;] = product_launch_intensity_category_df.new + \
        product_launch_intensity_category_df.old
    product_launch_intensity_category_df[&#39;launch_intensity&#39;] = round(
        product_launch_intensity_category_df.new/product_launch_intensity_category_df.product_count, 3)
    # for col in [&#39;source&#39;, &#39;category&#39;]:
    #     product_launch_intensity_category_df[col] = product_launch_intensity_category_df[col].astype(
    #         &#39;category&#39;)
    product_launch_intensity_category_df.to_feather(
        self.dash_data_path/&#39;meta_product_launch_intensity_category_month&#39;)

    del product_launch_intensity_category_df
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;meta_product_launch_intensity_category_month&#39;,
        job_name=&#39;webapp&#39;)

    # Ingredient Launch Trend
    ingredients = db.query_database(
        &#34;select prod_id, new_flag, meta_date, category, product_type from r_bte_product_ingredients_f&#34;)
    ingredients[&#39;source&#39;] = ingredients.prod_id.apply(
        lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
    new_ingredient_trend_category_df = ingredients[ingredients.new_flag == &#39;new_ingredient&#39;].groupby(
        by=[&#39;source&#39;, &#39;category&#39;, &#39;meta_date&#39;]).new_flag.count().reset_index()
    new_ingredient_trend_category_df.rename(
        columns={&#39;new_flag&#39;: &#39;new_ingredient_count&#39;}, inplace=True)
    # for col in [&#39;source&#39;, &#39;category&#39;]:
    #     new_ingredient_trend_category_df[col] = new_ingredient_trend_category_df[col].astype(
    #         &#39;category&#39;)
    new_ingredient_trend_category_df.to_feather(
        self.dash_data_path/&#39;new_ingredient_trend_category_month&#39;)

    del new_ingredient_trend_category_df
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;new_ingredient_trend_category_month&#39;,
        job_name=&#39;webapp&#39;)

    new_ingredient_trend_product_type_df = ingredients[ingredients.new_flag == &#39;new_ingredient&#39;].groupby(
        by=[&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;, &#39;meta_date&#39;]).new_flag.count().reset_index()
    new_ingredient_trend_product_type_df.rename(
        columns={&#39;new_flag&#39;: &#39;new_ingredient_count&#39;}, inplace=True)
    # for col in [&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]:
    #     new_ingredient_trend_product_type_df[col] = new_ingredient_trend_product_type_df[col].astype(
    #         &#39;category&#39;)
    new_ingredient_trend_product_type_df.to_feather(
        self.dash_data_path/&#39;new_ingredient_trend_product_type_month&#39;)

    del new_ingredient_trend_product_type_df
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;new_ingredient_trend_product_type_month&#39;,
        job_name=&#39;webapp&#39;)

    del metadata, reviews, ingredients
    gc.collect()</code></pre>
</details>
</dd>
<dt id="meiyume.webapp_datamaker.RefreshData.refresh_product_page_data"><code class="name flex">
<span>def <span class="ident">refresh_product_page_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>refresh_product_page_data [summary]</p>
<p>[extended_summary]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def refresh_product_page_data(self):
    &#34;&#34;&#34;refresh_product_page_data [summary]

    [extended_summary]
    &#34;&#34;&#34;
    # get metadata
    metadata = db.query_database(&#34;select prod_id, product_name, brand, category, product_type, new_flag, meta_date, low_p, high_p, \
                          mrp, reviews, bayesian_estimate, first_review_date from r_bte_meta_detail_f&#34;)
    metadata[&#39;source&#39;] = metadata.prod_id.apply(
        lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
    metadata[&#39;meta_date&#39;] = metadata[&#39;meta_date&#39;].astype(&#39;datetime64&#39;)

    # initialize landing page data dict
    self.landing_page_data[&#39;products&#39;] = metadata.prod_id.nunique()
    self.landing_page_data[&#39;brands&#39;] = metadata.brand.nunique()

    dt = metadata.groupby(&#39;source&#39;).meta_date.max(
    ).reset_index().to_dict(&#39;records&#39;)
    meta = []
    for i in dt:
        meta.append(metadata[(metadata.source == i[&#39;source&#39;]) &amp;
                             (metadata.meta_date == i[&#39;meta_date&#39;])])
    metadata = pd.concat(meta, axis=0)
    metadata.fillna(&#39;&#39;, inplace=True)
    metadata.drop_duplicates(subset=&#39;prod_id&#39;, inplace=True)

    self.landing_page_data[&#39;latest_scraped_date&#39;] = metadata.meta_date.max().strftime(
        &#34;%d/%m/%Y&#34;)

    metadata[metadata.columns.difference([&#39;product_name&#39;, &#39;brand&#39;])] \
        = metadata[metadata.columns.difference([&#39;product_name&#39;, &#39;brand&#39;])]\
        .apply(lambda x: x.astype(str).str.lower() if(x.dtype == &#39;object&#39;) else x)

    # get review data
    reviews = db.query_database(
        &#34;select prod_id, review_date, sentiment, is_influenced, meta_date, age, eye_color,\
             review_rating, hair_color, skin_tone, skin_type, review_text from r_bte_product_review_f&#34;)
    reviews.drop_duplicates(
        subset=[&#39;prod_id&#39;, &#39;review_text&#39;, &#39;review_date&#39;], inplace=True)
    reviews.drop(columns=&#39;review_text&#39;, inplace=True)

    self.landing_page_data[&#39;reviews&#39;] = reviews.shape[0]

    reviews.fillna(&#39;&#39;, inplace=True)

    first_review_date_df = reviews.groupby(
        &#39;prod_id&#39;).review_date.min().reset_index()

    metadata.set_index(&#39;prod_id&#39;, inplace=True)
    reviews.set_index(&#39;prod_id&#39;, inplace=True)
    first_review_date_df.set_index(&#39;prod_id&#39;, inplace=True)

    metadata = metadata.join(first_review_date_df, how=&#39;left&#39;)
    metadata.first_review_date = metadata.review_date
    metadata.drop(columns=&#39;review_date&#39;, inplace=True)

    del first_review_date_df
    gc.collect()

    reviews = reviews.join(
        metadata[[&#39;source&#39;, &#39;category&#39;, &#39;product_type&#39;]], how=&#39;left&#39;)

    metadata.reset_index(inplace=True)
    reviews.reset_index(inplace=True)

    reviews = reviews[reviews.prod_id.isin(metadata.prod_id.tolist())]
    reviews[&#39;review_date&#39;] = reviews[&#39;review_date&#39;].astype(&#39;datetime64[M]&#39;)
    reviews[reviews.columns.difference([&#39;product_name&#39;])] \
        = reviews[reviews.columns.difference([&#39;product_name&#39;])]\
        .apply(lambda x: x.astype(str).str.lower() if(x.dtype == &#39;object&#39;) else x)
    reviews.reset_index(inplace=True, drop=True)

    # get review summary data
    review_sum = db.query_database(
        &#34;select prod_id, pos_review_summary, neg_review_summary, \
            pos_talking_points, neg_talking_points \
                from r_bte_product_review_summary_f&#34;)
    review_sum = review_sum[review_sum.prod_id.isin(
        metadata.prod_id.tolist())]
    review_sum.reset_index(inplace=True, drop=True)

    # get ingredient data
    ingredients = db.query_database(&#34;select prod_id, product_name, brand, category, product_type, \
        ingredient, ingredient_type, new_flag, ban_flag \
            from r_bte_product_ingredients_f&#34;)
    ingredients = ingredients[ingredients.prod_id.isin(
        metadata.prod_id.tolist())]
    ingredients.reset_index(inplace=True, drop=True)
    ingredients.fillna(&#39;&#39;, inplace=True)
    ingredients[&#39;source&#39;] = ingredients.prod_id.apply(
        lambda x: &#39;us&#39; if &#39;sph&#39; in x else &#39;uk&#39;)
    ingredients[ingredients.columns.difference([&#39;product_name&#39;, &#39;brand&#39;])] \
        = ingredients[ingredients.columns.difference([&#39;product_name&#39;, &#39;brand&#39;])]\
        .apply(lambda x: x.astype(str).str.lower() if(x.dtype == &#39;object&#39;) else x)

    # get item data
    items = db.query_database(&#34;select * from r_bte_product_item_f&#34;)
    items = items[items.prod_id.isin(metadata.prod_id.tolist())]
    items[&#39;meta_date&#39;] = items[&#39;meta_date&#39;].astype(&#39;datetime64[M]&#39;)
    items.fillna(&#39;&#39;, inplace=True)
    items.reset_index(inplace=True, drop=True)

    # Dropdown and Metadata Data
    prod_page_metadetail_data_df = metadata
    del metadata
    gc.collect()

    prod_page_metadetail_data_df.rename(columns={&#39;low_p&#39;: &#39;small_size_price&#39;, &#39;high_p&#39;: &#39;big_size_price&#39;,
                                                 &#39;bayesian_estimate&#39;: &#39;adjusted_rating&#39;}, inplace=True)
    prod_page_metadetail_data_df.adjusted_rating = prod_page_metadetail_data_df.adjusted_rating.apply(
        lambda x: round(float(x), 2) if x != &#39;&#39; else &#39;&#39;)
    prod_page_metadetail_data_df = prod_page_metadetail_data_df[
        prod_page_metadetail_data_df.small_size_price != &#39;&#39;]
    prod_page_metadetail_data_df.big_size_price = prod_page_metadetail_data_df.apply(
        lambda x: x.small_size_price if x.big_size_price == &#39;&#39; else x.big_size_price, axis=1)
    prod_page_metadetail_data_df.mrp = prod_page_metadetail_data_df.apply(
        lambda x: x.big_size_price if x.mrp == &#39;&#39; else x.mrp, axis=1)
    prod_page_metadetail_data_df.reset_index(inplace=True, drop=True)
    prod_page_metadetail_data_df.small_size_price = prod_page_metadetail_data_df.small_size_price.astype(
        float)
    prod_page_metadetail_data_df.big_size_price = prod_page_metadetail_data_df.big_size_price.astype(
        float)
    prod_page_metadetail_data_df.mrp = prod_page_metadetail_data_df.mrp.astype(
        str)
    prod_page_metadetail_data_df.adjusted_rating = prod_page_metadetail_data_df.adjusted_rating.astype(
        str)
    for col in [&#39;brand&#39;, &#39;category&#39;, &#39;product_type&#39;, &#39;new_flag&#39;, &#39;source&#39;]:
        prod_page_metadetail_data_df[col] = prod_page_metadetail_data_df[col].astype(
            &#39;category&#39;)
    prod_page_metadetail_data_df.drop_duplicates(inplace=True)
    prod_page_metadetail_data_df.reset_index(drop=True, inplace=True)
    prod_page_metadetail_data_df.to_feather(
        self.dash_data_path/&#39;product_page_metadetail_data&#39;)

    del prod_page_metadetail_data_df
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;product_page_metadetail_data&#39;,
        job_name=&#39;webapp&#39;)

    # Review Tab Data
    review_sum.fillna(&#34;{}&#34;, inplace=True)
    review_sum.pos_talking_points[review_sum.pos_talking_points == &#34;&#34;] = &#34;{}&#34;
    review_sum.neg_talking_points[review_sum.neg_talking_points == &#34;&#34;] = &#34;{}&#34;
    indices = []
    for i in review_sum.iterrows():
        try:
            d = literal_eval(i[1].pos_talking_points)
            d = literal_eval(i[1].neg_talking_points)
        except Exception as ex:
            indices.append(i[0])
    review_sum = review_sum[~review_sum.index.isin(
        indices)].reset_index(drop=True)
    review_sum.pos_talking_points = review_sum.pos_talking_points.apply(
        lambda x: literal_eval(x) if x != &#34;{}&#34; else {})
    review_sum.neg_talking_points = review_sum.neg_talking_points.apply(
        lambda x: literal_eval(x) if x != &#34;{}&#34; else {})
    # Review Summary Data
    prod_page_review_sum_df = review_sum[[
        &#39;prod_id&#39;, &#39;pos_review_summary&#39;, &#39;neg_review_summary&#39;]]

    prod_page_review_sum_df.drop_duplicates(inplace=True)
    prod_page_review_sum_df.reset_index(drop=True, inplace=True)
    prod_page_review_sum_df.to_feather(
        self.dash_data_path/&#39;prod_page_product_review_summary&#39;)

    del prod_page_review_sum_df
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;prod_page_product_review_summary&#39;,
        job_name=&#39;webapp&#39;)

    # Talking Points Data
    prod_page_review_talking_points_df = review_sum[[
        &#39;prod_id&#39;, &#39;pos_talking_points&#39;, &#39;neg_talking_points&#39;]]

    prod_page_review_talking_points_df.drop_duplicates(
        inplace=True, subset=&#39;prod_id&#39;)
    prod_page_review_talking_points_df.reset_index(drop=True, inplace=True)
    prod_page_review_talking_points_df.to_pickle(
        self.dash_data_path/&#39;prod_page_review_talking_points&#39;)

    del prod_page_review_talking_points_df
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;prod_page_review_talking_points&#39;,
        job_name=&#39;webapp&#39;)

    # Review Sentiment and Time Series Data
    reviews.is_influenced = reviews.is_influenced.str.lower()
    prod_page_review_sentiment_influence_df = reviews[[
        &#39;prod_id&#39;, &#39;sentiment&#39;, &#39;is_influenced&#39;, &#39;review_date&#39;, &#39;review_rating&#39;]]
    for col in [&#39;prod_id&#39;, &#39;sentiment&#39;, &#39;is_influenced&#39;, &#39;review_rating&#39;]:
        prod_page_review_sentiment_influence_df[col] = prod_page_review_sentiment_influence_df[col].astype(
            &#39;category&#39;)
    prod_page_review_sentiment_influence_df.review_date = prod_page_review_sentiment_influence_df.review_date.astype(
        &#39;datetime64[M]&#39;)

    prod_page_review_sentiment_influence_df.reset_index(
        drop=True, inplace=True)
    prod_page_review_sentiment_influence_df.to_feather(
        self.dash_data_path/&#39;prod_page_review_sentiment_influence&#39;)

    del prod_page_review_sentiment_influence_df
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;prod_page_review_sentiment_influence&#39;,
        job_name=&#39;webapp&#39;)

    # Review User Attribute Data
    prod_page_reviews_attribute_df = reviews[[&#39;prod_id&#39;, &#39;age&#39;, &#39;eye_color&#39;, &#39;hair_color&#39;, &#39;skin_tone&#39;, &#39;skin_type&#39;]][
        (reviews.age != &#39;&#39;) | (reviews.eye_color != &#39;&#39;) | (reviews.hair_color != &#39;&#39;) | (reviews.skin_tone != &#39;&#39;) | (reviews.skin_type != &#39;&#39;)]
    for col in prod_page_reviews_attribute_df.columns:
        prod_page_reviews_attribute_df[col] = prod_page_reviews_attribute_df[col].astype(
            &#39;category&#39;)
    prod_page_reviews_attribute_df.reset_index(inplace=True, drop=True)
    prod_page_reviews_attribute_df.to_feather(
        self.dash_data_path/&#39;prod_page_reviews_attribute&#39;)

    del prod_page_reviews_attribute_df, reviews
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;prod_page_reviews_attribute&#39;,
        job_name=&#39;webapp&#39;)

    # Pricing and Ingredients Tab Data
    prod_page_item_df = items[[&#39;prod_id&#39;, &#39;product_name&#39;,
                               &#39;meta_date&#39;, &#39;item_name&#39;, &#39;item_price&#39;, &#39;size_oz&#39;]]
    prod_page_item_df.rename(
        columns={&#39;size_oz&#39;: &#39;item_size&#39;}, inplace=True)

    def hasNumbers(inputString):
        return bool(re.search(r&#39;\d&#39;, inputString))
    prod_page_item_df.item_size = prod_page_item_df.item_size.apply(
        lambda x: x if hasNumbers(x) else &#39;&#39;).str.replace(&#39;out of stock:&#39;, &#39;&#39;).str.replace(&#39;nan&#39;, &#39;&#39;)
    prod_page_item_df.item_size = prod_page_item_df.item_size.apply(
        lambda x: x if len(x) &lt; 30 else &#39;&#39;)
    prod_page_item_df.reset_index(inplace=True, drop=True)
    for col in [&#39;item_price&#39;, &#39;item_size&#39;]:
        prod_page_item_df[col] = prod_page_item_df[col].astype(&#39;category&#39;)
    prod_page_item_df.drop_duplicates(
        inplace=True, subset=[&#39;prod_id&#39;, &#39;item_size&#39;, &#39;item_name&#39;])
    prod_page_item_df.reset_index(inplace=True, drop=True)
    prod_page_item_df.to_feather(self.dash_data_path/&#39;prod_page_item_data&#39;)

    del prod_page_item_df, items
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;prod_page_item_data&#39;,
        job_name=&#39;webapp&#39;)

    # Ingredient Data
    prod_page_ing_df = ingredients[[&#39;prod_id&#39;, &#39;product_name&#39;, &#39;ingredient&#39;, &#39;new_flag&#39;,
                                    &#39;ingredient_type&#39;, &#39;ban_flag&#39;,
                                    &#39;category&#39;, &#39;product_type&#39;, &#39;source&#39;]]
    for col in [&#39;prod_id&#39;, &#39;product_name&#39;, &#39;ingredient&#39;, &#39;new_flag&#39;,
                &#39;ingredient_type&#39;, &#39;ban_flag&#39;,
                &#39;category&#39;, &#39;product_type&#39;, &#39;source&#39;]:
        prod_page_ing_df[col] = prod_page_ing_df[col].astype(&#39;category&#39;)
    prod_page_ing_df.to_feather(self.dash_data_path/&#39;prod_page_ing_data&#39;)

    del prod_page_ing_df, ingredients
    gc.collect()

    file_manager.push_file_s3(
        file_path=self.dash_data_path/&#39;prod_page_ing_data&#39;,
        job_name=&#39;webapp&#39;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="meiyume" href="index.html">meiyume</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="meiyume.webapp_datamaker.RefreshData" href="#meiyume.webapp_datamaker.RefreshData">RefreshData</a></code></h4>
<ul class="">
<li><code><a title="meiyume.webapp_datamaker.RefreshData.make" href="#meiyume.webapp_datamaker.RefreshData.make">make</a></code></li>
<li><code><a title="meiyume.webapp_datamaker.RefreshData.refresh_category_page_data" href="#meiyume.webapp_datamaker.RefreshData.refresh_category_page_data">refresh_category_page_data</a></code></li>
<li><code><a title="meiyume.webapp_datamaker.RefreshData.refresh_ingredient_page_data" href="#meiyume.webapp_datamaker.RefreshData.refresh_ingredient_page_data">refresh_ingredient_page_data</a></code></li>
<li><code><a title="meiyume.webapp_datamaker.RefreshData.refresh_landing_page_data" href="#meiyume.webapp_datamaker.RefreshData.refresh_landing_page_data">refresh_landing_page_data</a></code></li>
<li><code><a title="meiyume.webapp_datamaker.RefreshData.refresh_market_trend_data" href="#meiyume.webapp_datamaker.RefreshData.refresh_market_trend_data">refresh_market_trend_data</a></code></li>
<li><code><a title="meiyume.webapp_datamaker.RefreshData.refresh_product_page_data" href="#meiyume.webapp_datamaker.RefreshData.refresh_product_page_data">refresh_product_page_data</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>